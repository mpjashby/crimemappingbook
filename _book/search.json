[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Learn crime mapping with R",
    "section": "",
    "text": "Welcome!\nThis book will give you the knowledge and skills to effectively communicate information about crime and related topics using maps. We will cover the principles of analysing geographic information and the strengths and weaknesses of different maps for communicating it."
  },
  {
    "objectID": "index.html#who-is-this-book-for",
    "href": "index.html#who-is-this-book-for",
    "title": "Learn crime mapping with R",
    "section": "Who is this book for?",
    "text": "Who is this book for?\nThis book is for you if you are:\n\nSomeone who wants to learn how to understand patterns of crime using maps and related forms of data analysis.\nSomeone who already knows how to map crimes and would like to learn how to programme in a familiar context."
  },
  {
    "objectID": "index.html#why-use-this-book",
    "href": "index.html#why-use-this-book",
    "title": "Learn crime mapping with R",
    "section": "Why use this book?",
    "text": "Why use this book?\nThere are several books available for learning crime mapping. The advantages of using this book are that:\n\nIt teaches up-to-date crime mapping techniques. Some of the most-popular books on crime mapping were written over a decade ago and do not reflect substantial developments in the field since then.\nIt teaches crime mapping using exclusively free software (R and RStudio). Some other books teach crime mapping in expensive proprietary software that most people who need to make crime maps do not have access to.\nIt is an online book, so includes videos to introduce theoretical concepts and walk you through the process of creating different maps.\nIt uses examples from across the world, so it is not only useful to readers from one country.\nIt’s completely free to read and redistribute!"
  },
  {
    "objectID": "index.html#why-learn-crime-mapping-in-r",
    "href": "index.html#why-learn-crime-mapping-in-r",
    "title": "Learn crime mapping with R",
    "section": "Why learn crime mapping in R?",
    "text": "Why learn crime mapping in R?\nWe could make crime maps in several different apps. This includes commercial geographic information systems such as ArcGIS or MapInfo, free software such as QGIS, and data analysis programmes such as Tableau. So why learn crime mapping in a programming language like R?\nThere are several reasons:\n\nMaking maps using a programming language makes your work much more efficient, especially if (as is common in crime analysis) you need to produce similar maps periodically using updated data, or need to produce multiple similar maps for different areas or crime types.\nR is free. This can be important for people working in agencies with very limited budgets for software for data analysis.\nR has extensive mapping capabilities, supported by a large team of friendly experts who provide online support.\nR is good for other types of data analysis, so everything you learn here can be used for analysing crime data using techniques that are unrelated to maps.\n\nLearning a programming language like R involves a little extra work at the start, but this book is written to make this as easy as possibly by gently introducing you to programming ideas a little at a time. Once you’ve got started, you’ll find mapping crime in this way makes it much easier to advance further in the future."
  },
  {
    "objectID": "index.html#how-to-use-this-book",
    "href": "index.html#how-to-use-this-book",
    "title": "Learn crime mapping with R",
    "section": "How to use this book",
    "text": "How to use this book\nThere are two ways to use this book.\n\nYou can read it as you would any other technical how-to book, by reading through each chapter and working through each exercise in RStudio on your computer or online.\nYou can work through each chapter as an interactive tutorial in RStudio, getting immediate feedback on each exercise. The interactive tutorials also include quizzes to help you check your understanding.\n\nIf you want to use the interactive tutorials, there are a few extra set-up steps to work through at the bottom of this page. They will only take a few minutes.\nI recommend using the interactive tutorials if you can. The interactive tutorials are used for teaching crime mapping to BSc Crime and Security Science students at University College London so they have been tested by hundreds of people learning crime mapping before you."
  },
  {
    "objectID": "00_setup/index.html#step-1-install-r",
    "href": "00_setup/index.html#step-1-install-r",
    "title": "Install the software needed for this book",
    "section": "Step 1: install R",
    "text": "Step 1: install R\nThe first step is to download and install R, a programming language designed for analysing and visualising data, including making maps. To install R, visit the R website and download R for either Windows or Mac, depending on what type of computer you are using. If you already have R installed on your computer, please update it to the latest release.\nThis video talks you through the process of installing R:"
  },
  {
    "objectID": "00_setup/index.html#step-2-install-rstudio",
    "href": "00_setup/index.html#step-2-install-rstudio",
    "title": "Install the software needed for this book",
    "section": "Step 2: install RStudio",
    "text": "Step 2: install RStudio\nThe next step is to download RStudio, an app that you can use to work with the R programming language more efficiently. Download RStudio Desktop for your computer from the Posit website (that’s the company that makes RStudio) and install. If you already have RStudio Desktop installed on your machine, please update it to the latest release.\nThis video talks you through the process of installing RStudio:"
  },
  {
    "objectID": "00_setup/index.html#step-3-install-rtools-windows-only",
    "href": "00_setup/index.html#step-3-install-rtools-windows-only",
    "title": "Install the software needed for this book",
    "section": "Step 3: install RTools (Windows only)",
    "text": "Step 3: install RTools (Windows only)\nIf you are using a Windows computer you should install Rtools, which will be needed by RStudio for some tutorials. If you are using a Mac or Linux computer, you do not need to install Rtools.\nTo install RTools:\n\nDownload the latest version from the R website and open the downloaded file.\nFollow the installation instructions (accept all the default options).\nOpen RStudio.\nFind the panel (in the bottom-left) marked Console.\nFind the > symbol at the bottom of that panel.\nCopy and paste the following code to the right of the > symbol:\n\nwrite('PATH=\"${RTOOLS40_HOME}\\\\usr\\\\bin;${PATH}\"', file = \"~/.Renviron\", append = TRUE)\n\nPress Enter."
  },
  {
    "objectID": "00_setup/index.html#if-you-cannot-install-software-on-your-computer",
    "href": "00_setup/index.html#if-you-cannot-install-software-on-your-computer",
    "title": "Install the software needed for this book",
    "section": "If you cannot install software on your computer",
    "text": "If you cannot install software on your computer\nYou may not be able to install software on the computer you want to use for crime mapping, or you may prefer not to for various reasons. In that case, you can run RStudio online using Posit Cloud. Posit Cloud is free for a certain number of hours each month, after which you can pay to continue using it.\nUsing Posit Cloud allows you to avoid the installation steps for R, RStudio and RTools above, but remember that since Posit Cloud operates online, you should not use it work on confidential or personal data unless you have the necessary permission to do so."
  },
  {
    "objectID": "00_setup/index.html#running-the-interactive-tutorials",
    "href": "00_setup/index.html#running-the-interactive-tutorials",
    "title": "Install the software needed for this book",
    "section": "Running the interactive tutorials",
    "text": "Running the interactive tutorials\nIf you are going to use this book as you would any other technical how-to book, you can move ahead to the next chapter now. If you are going to use the interactive tutorials that accompany this book, there is one final step to set them up:\n\nOpen RStudio.\nFind the panel (in the bottom-left) marked Console.\nFind the > symbol at the bottom of that panel.\nCopy and paste the following code to the right of the > symbol:\n\nsource(\"https://github.com/mpjashby/crimemapping/raw/main/inst/initialise.R\")\n\nPress Enter.\n\n\nIt will take a few minutes for the tutorials to be set up. Once the process is complete you will see a message telling you this."
  },
  {
    "objectID": "01_getting_started/index.html#welcome",
    "href": "01_getting_started/index.html#welcome",
    "title": "\n1  Getting started\n",
    "section": "\n1.1 Welcome",
    "text": "1.1 Welcome\n\nWelcome to this course on crime mapping! This course uses interactive tutorials like this one to help you learn about using maps and spatial analysis techniques to understand crime. Watch this video to learn more about the course.\n\n\nWelcome to Learn Crime Mapping with R. This book will help you learn about using maps and spatial analysis techniques to understand crime. Watch this video to learn more.\n\n\n\nNow click Next Topic below to start this tutorial."
  },
  {
    "objectID": "01_getting_started/index.html#why-put-crimes-on-maps",
    "href": "01_getting_started/index.html#why-put-crimes-on-maps",
    "title": "\n1  Getting started\n",
    "section": "\n1.2 Why put crimes on maps?",
    "text": "1.2 Why put crimes on maps?\nThis course is about how we can use maps and other spatial analysis tools to help understand, prevent and respond to crime. Watch this video to understand why spatial analysis is a useful tool for understanding crime.\n\n\n\n\n\nWeisburd, D. (2015). The law of crime concentration and the criminology of place. Criminology, 53(2), 133-157.\nJohnson, S. (2010). A brief history of the analysis of crime concentration. European Journal of Applied Mathematics, 21(4-5), 349.\nFarrell, G. (2015). Crime concentration theory. Crime Prevention and Community Safety, 17(4), 233-248."
  },
  {
    "objectID": "01_getting_started/index.html#why-is-crime-concentrated-in-space",
    "href": "01_getting_started/index.html#why-is-crime-concentrated-in-space",
    "title": "\n1  Getting started\n",
    "section": "\n1.3 Why is crime concentrated in space?",
    "text": "1.3 Why is crime concentrated in space?\nWhy is crime concentrated in space? Watch this video to find out more about how our environment influences opportunities for crime and how that causes clusters of different crimes.\n\n\n\n\n\nSantos, R. B. (2015). Routine Activity Theory: A Cornerstone of Police Crime Analyst Work. In The Criminal Act: \nCohen, L. E., and Felson, M. (1979). Social Change and Crime Rate Trends: A Routine Activity Approach. American Sociological Review, 44(4), 588–608."
  },
  {
    "objectID": "01_getting_started/index.html#finding-your-way-around-rstudio",
    "href": "01_getting_started/index.html#finding-your-way-around-rstudio",
    "title": "\n1  Getting started\n",
    "section": "\n1.4 Finding your way around RStudio",
    "text": "1.4 Finding your way around RStudio\nWe will use RStudio for almost all of this course. Watch this video to find your way around the different panels in the RStudio window.\n\n\n1.4.1 Slightly adjusting how RStudio works\nBefore we start using RStudio, we should make a few changes to how it is set up that will make it easier to fix any mistakes we make while coding. To do this, click on the Tools menu in RStudio and then on Global Options…. In the dialogue box that opens, click on General in the left-hand panel if General is not selected already.\n\n\nIn the “Workspace” section of the right-hand panel, find an option that says “Restore .RData into workspace at startup” and make sure the check box to the left of that option is not checked. On the next line down, click the drop-down menu labelled “Save workspace to .RData on exit:” and choose the option Never. Click Apply and then OK to close the dialogue box.\n\nThe RStudio IDE Cheat Sheet highlights some of the features available in RStudio and gives a list of available keyboard short-cuts.\nWriting Code in RStudio is a webinar that talks you through RStudio in more detail."
  },
  {
    "objectID": "01_getting_started/index.html#navigating-these-tutorials",
    "href": "01_getting_started/index.html#navigating-these-tutorials",
    "title": "\n1  Getting started\n",
    "section": "\n1.5 Navigating these tutorials",
    "text": "1.5 Navigating these tutorials\nThe tutorials that make up this course include short chunks of R code that you can run directly in this window. To run the code in each chunk, just click the Run Code button in the top-right corner of each chunk. If you want to re-run a chunk of code, you can click the Start Over button in the top-left corner of each chunk. Some of the chunks will have pre-filled code for you to run, while for others you will be asked to type the code needed to complete a task based on what you have already learned.\nWhen you click Run Code, the output of the chunk will appear below it (there may be a delay of a few seconds for more-complicated chunks).\nTo try out using a code chunk, click Run Code below – you should see the message ‘Hello, world!’ printed below the box.\n\nmessage(\"Hello, world!\")\n\nHello, world!\n\n\n\nSome of the tutorials include boxes like this one that contain information that it is particularly important for you to know to avoid common mistakes in writing code. Pay special attention to these points and remember to ask questions if anything isn’t clear.\n\n\n\nMore information you might like to know <- click here\n\n\nIn these tutorials you will also see lines marked ‘Extra detail’ that you can click on to find out more information about a particular issue. This is generally information that you do not need to know to complete the tutorial, but which might be useful in other circumstances or which might answer some questions that you have. You can skip these boxes if you want to, or come back to them later if you have a question."
  },
  {
    "objectID": "01_getting_started/index.html#in-summary",
    "href": "01_getting_started/index.html#in-summary",
    "title": "\n1  Getting started\n",
    "section": "\n1.6 In summary",
    "text": "1.6 In summary\nNow that you know why crime mapping is useful for understanding crime, why crime is typically concentrated in space and how to find your way around RStudio, in the next tutorial we will produce our first crime map in R.\nIf you’re not feeling too confident at this point in the course, don’t worry – learning something new is always a bit of a roller coaster and there is lots of help available in subsequent tutorials.\n\n\n\nStats Illustrations by Allison Horst licensed under the Creative Commons Attribution licence."
  },
  {
    "objectID": "02_your_first_crime_map/index.html#introduction",
    "href": "02_your_first_crime_map/index.html#introduction",
    "title": "\n2  Your first crime map\n",
    "section": "\n2.1 Introduction",
    "text": "2.1 Introduction\nIn this tutorial we will use R to produce a simple crime map. To do this we will skip over lots of the detail of how R works and what choices we should make in creating maps. We will return to all these details in future sessions, so for-now please don’t worry about understanding every single line of code. Everything will become clear as we work through the tutorials in this course.\nThe map we’re going to create shows the locations of four homicides in downtown Atlanta in 2019:\n\n\n\n\n\n\n\n\nTo start off with, watch this video that walks through the code needed to make this map. Don’t worry if there are things in the video that you don’t understand – the rest of this tutorial will explain each line of code in turn.\n\n\n2.1.1 Running code in this tutorial\nIn the video above you saw code being run in RStudio, but to save switching between this tutorial and the RStudio console, the rest of this tutorial includes short chunks of R code that you can run directly in this window. We will use these chunks of code to walk through the code we need to produce a map. To run the code in each chunk, just click the Run Code button in the top-right corner of each chunk. If you want to re-run a chunk of code, you can click the Start Over button in the top-left corner of each chunk.\nWhen you click Run Code, the output of the chunk will appear below it (there may be a delay of a few seconds for more-complicated chunks).\nTo try out using a code chunk, click Run Code below – you should see the message ‘Hello, world!’ printed below the box.\n\nmessage(\"Hello, world!\")\n\nHello, world!\n\n\nmessage() is a function – a piece of R code that does something. The thing that the messages() function does is simple: it prints a message on the screen.\nNow click Continue below to start this tutorial."
  },
  {
    "objectID": "02_your_first_crime_map/index.html#loading-crime-data",
    "href": "02_your_first_crime_map/index.html#loading-crime-data",
    "title": "\n2  Your first crime map\n",
    "section": "\n2.2 Loading crime data",
    "text": "2.2 Loading crime data\n\n2.2.1 Loading packages\n\nBefore we can work with our data, we first load packages of functions for use in the analysis. To load these packages, click Run Code. This will produce various messages, all of which you can safely ignore for now.\n\n\nBefore we can work with our data, we first load packages of functions for use in the analysis. Loading packages will produce various messages, all of which you can safely ignore for now.\n\n\n# Load the R packages we need to analyse this data\nlibrary(ggspatial)\nlibrary(sf)\n\nLinking to GEOS 3.11.0, GDAL 3.5.3, PROJ 9.1.0; sf_use_s2() is TRUE\n\nlibrary(tidyverse)\n\n\n\nWhat do these messages mean?\n\n\nFor now you don’t need to worry about these messages, but if you really want to know what they mean …\n\n\n\n\nSome R packages make use of other apps and utilities on your computer. For example, the sf package makes use of a piece of software call GDAL that is used for managing spatial data. So that you know which version of GDAL is being used, sf prints a message telling you.\nThe tidyverse package itself loads several packages that are commonly used together for analysing data. When you load tidyverse, it will print a message telling you which packages it has loaded, along with the version number for each package. It also prints a message saying if any functions from the tidyverse packages have replaced (“masked”) any functions from packages that were previously loaded.\nIn general, R packages use start-up messages to remind you of information that is not likely to be critical to your work, but which it might be useful to know at some point in the future.\n\n\n\n2.2.2 Loading data\nThe first task in creating any crime map is to obtain the crime and other data necessary. In many cases preparing the data for analysis and mapping will be a substantial task, but in this case we are going to use some pre-prepared crime data together with a pre-drawn street map (which we will ask R to download automatically when it draws the final map).\nThe data we will use will be records of homicides in the Downtown neighbourhood of Atlanta, Georgia, in 2019. We can load the homicide data using the read_csv() function.\n\nClick the Run Code button to load the data.\n\n\n# Download the data directly from a URL and store it as an object\nhomicides <- read_csv(\"https://mpjashby.github.io/crimemappingdata/downtown_homicides.csv\")\n\nRows: 4 Columns: 4\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): label\ndbl (3): report_number, longitude, latitude\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nThe read_csv() function loads data from a file and prints a message showing the name of each column in the data and the type of data (number, text etc.) in each column. Again, you can ignore this message for now.\nWe have stored the results of the read_csv() function in an R object called homicides. An object in R is anything that stores any type of data. There are many types of objects, but for this tutorial we don’t need to explore these in any more detail. All you need to remember for now is that objects store data and functions do things.\n\n2.2.3 Viewing the data\nTo check the data has been loaded correctly, we can view the loaded data using the head() function. By default, head() prints the first six rows of the data stored in an object.\n\nClick Run Code to view the data.\n\n\n# Display the data\nhead(homicides)\n\n# A tibble: 4 × 4\n  report_number label                                            longi…¹ latit…²\n          <dbl> <chr>                                              <dbl>   <dbl>\n1     190191530 \"400 W PEACHTREE ST NW\\n19 January @ 15:00\"        -84.4    33.8\n2     190570315 \"171 AUBURN AVE NE @CITY WALK APARTMENTS\\n26 Fe…   -84.4    33.8\n3     192160018 \"241 FORSYTH ST SW\\n 4 August @ 00:00\"             -84.4    33.7\n4     193302338 \"80 JESSE HILL JR DR SE @GRADY\\n26 November @ 2…   -84.4    33.8\n# … with abbreviated variable names ¹​longitude, ²​latitude\n\n\nThe data contain four columns: a unique identifier for a homicide, a label describing when and where that homicide occurred, and the longitude and latitude of the homicide location. Depending on the width of your screen, you may need to click on the ‘▸’ symbol to view all the columns in the data. We can use this data to plot the homicides on a map.\n\nIn the code head(homicides), there are no quote marks around the word homicides.\nAlmost all programming languages will interpret words differently depending on whether they have quotes around them or not. In this case, if you type the code head(homicides) then R will print the first few rows of the data stored in the homicides object.\nOn the other hand, if you type the code head(\"homicides\") or head('homicides'), R will interpret this as an instruction to print the first few elements of the literal text ‘homicides’. Since the text ‘homicides’ contains only one element (more about that later), head(\"homicides\") will just print the word ‘homicides’."
  },
  {
    "objectID": "02_your_first_crime_map/index.html#processing-the-data",
    "href": "02_your_first_crime_map/index.html#processing-the-data",
    "title": "\n2  Your first crime map\n",
    "section": "\n2.3 Processing the data",
    "text": "2.3 Processing the data\nBefore we can plot the data on a map, we have to complete some pre-processing steps. Having to process data before being able to analyse or visualise it is common in all types of data analysis, but spatial analysis often involves additional processing that takes account of the special features of spatial data.\n\n2.3.1 Converting the data into a spatial format\nTwo data-processing tasks are needed to produce this map. The first is to convert the data into a simple features or SF object, which is a special type of R object that can be used by functions that process spatial data. We will cover the details of the st_as_sf() function that converts our data into into an SF object later on.\n\nClick Run Code to convert the data into SF format.\n\n\n# Convert the data to a simple features object, which we can use in functions \n# that work on spatial data\nhomicides_sf <- st_as_sf(\n  homicides, \n  coords = c(\"longitude\", \"latitude\"), \n  crs = \"EPSG:4326\"\n)\n\n\nWhen you clicked Run Code above, it looked like nothing happened. This is because the results of the code are stored in the homicides_sf object. Do you remember what R code to use to view the first few rows of this object?\nType that code into the box below and click Run Code to view the data.\n\n\nWhen you run this code, it looks like nothing happens. This is because the results of the code are stored in the homicides_sf object. Do you remember what R code to use to view the first few rows of this object?\n\n\nIf you cannot remember how to view the contents of an object, you can click on the Solution button to get help.\nAs you go through these tutorials, try to avoid using the Solution button unless you have tried to input the correct code yourself. You will learn much more if you try to work out the answer by referring back to an earlier page in the tutorial or to your own notes.\nDon’t worry about getting the answer wrong – nothing bad will happen if you run the wrong code in this tutorial and you can have as many attempts as you like to get the answer right.\n\n\n\n\n\n# To view an object in R, use the `head()` function. To view the contents of the\n# homicides object, copy the next line into the box below and click Run Code.\nhead(homicides_sf)\n\nSimple feature collection with 4 features and 2 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: -84.39732 ymin: 33.74827 xmax: -84.38185 ymax: 33.76614\nGeodetic CRS:  WGS 84\n# A tibble: 4 × 3\n  report_number label                                              geometry\n          <dbl> <chr>                                           <POINT [°]>\n1     190191530 \"400 W PEACHTREE ST NW\\n19 January @ …  (-84.3876 33.76614)\n2     190570315 \"171 AUBURN AVE NE @CITY WALK APARTME… (-84.38185 33.75546)\n3     192160018 \"241 FORSYTH ST SW\\n 4 August @ 00:00\" (-84.39732 33.74827)\n4     193302338 \"80 JESSE HILL JR DR SE @GRADY\\n26 No… (-84.38198 33.75168)\n\n\nThe data looks identical to before running the function st_as_sf(), except that the two columns called longitude and latitude have disappeared and there is now an extra column called geometry. The geometry column is important because lots of functions in R can recognise that the geometry column represents a location on the surface of the earth that can be used to analyse and map data in space.\n\n2.3.2 Changing the data projection\nThe geometry column in the homicides_sf object represents locations on the surface of the earth using co-ordinates (pairs of numbers). In this case, the co-ordinates are expressed as longitudes and latitudes, but there are lots of other types of co-ordinates (known as co-ordinate reference systems).\nWe’ll learn more about co-ordinate reference systems in a future tutorial, but for now it’s enough to know that each different system has advantages and disadvantages. To make the homicide locations easier to add to a map, we are going to first transform the co-ordinates from longitudes and latitudes to a co-ordinate reference system that is specifically designed for mapping data for the US state of Georgia.\nTo do this, we will use the st_transform() function, together with a code representing the co-ordinate reference system we want to use (you don’t need to understand this code at this stage).\n\nhomicides_sf_trans <- st_transform(homicides_sf, \"EPSG:26967\")\n\nhead(homicides_sf_trans)\n\nSimple feature collection with 4 features and 2 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 678630.6 ymin: 415608.5 xmax: 680065.5 ymax: 417588.4\nProjected CRS: NAD83 / Georgia West\n# A tibble: 4 × 3\n  report_number label                                             geometry\n          <dbl> <chr>                                          <POINT [m]>\n1     190191530 \"400 W PEACHTREE ST NW\\n19 January @ … (679535.4 417588.4)\n2     190570315 \"171 AUBURN AVE NE @CITY WALK APARTME… (680065.5 416402.8)\n3     192160018 \"241 FORSYTH ST SW\\n 4 August @ 00:00\" (678630.6 415608.5)\n4     193302338 \"80 JESSE HILL JR DR SE @GRADY\\n26 No… (680052.6 415983.6)\n\n\nAgain, the data looks almost identical, except that the values in the geometry column have changed (you don’t need to understand yet the details of how these numbers are different). Now that we’ve completed the data processing, we can go on to produce the map itself."
  },
  {
    "objectID": "02_your_first_crime_map/index.html#draw-the-map",
    "href": "02_your_first_crime_map/index.html#draw-the-map",
    "title": "\n2  Your first crime map\n",
    "section": "\n2.4 Draw the map",
    "text": "2.4 Draw the map\nWe are now ready to produce our map of homicides in downtown Atlanta. So that people viewing the map will understand where the homicides occurred, we will plot the homicides on top of a base layer showing streets, parks and other geographic features obtained from an online web mapping service.\n\nClick Run Code to create the map.\n\n\nggplot(homicides_sf_trans) + \n  annotation_map_tile(type = \"osm\", zoom = 15, progress = \"none\") + \n  geom_sf_label(\n    aes(label = label), \n    lineheight = 1, \n    size = 2.5, \n    hjust = 1, \n    vjust = 0\n  ) + \n  geom_sf(colour = \"white\", fill = \"orangered1\", size = 4, shape = 21) + \n  scale_x_continuous(expand = expansion(mult = 0.5)) + \n  scale_y_continuous(expand = expansion(mult = 0.2)) + \n  labs(\n    title = \"Homicides in Downtown Atlanta, 2019\",\n    caption = \"Background map by OpenStreetMap\"\n  ) +\n  theme_void()\n\n\n\n\nYou can change the appearance of the map by changing various parts of the code above. For example, you could change the colour of the points that mark the homicides by changing the code fill = \"orangered1\" to fill = \"mediumblue\", or change the base map to a different style by changing the code type = \"osm\" to type = \"cartolight\".\n\nEach time you change part of the code, click Run Code to see what changes on the map.\nOnce you have finished experimenting with changing the appearance of the map, click Next Topic below."
  },
  {
    "objectID": "02_your_first_crime_map/index.html#putting-the-code-together",
    "href": "02_your_first_crime_map/index.html#putting-the-code-together",
    "title": "\n2  Your first crime map\n",
    "section": "\n2.5 Putting the code together",
    "text": "2.5 Putting the code together\nNow we have walked through the different parts of the code, we can create a map from scratch in a single block of code. In this example, we will map homicides from in Glenrose Heights neighbourhood of Atlanta, and a different style of base map. Since the area covered by the map is derived from the data itself, the extent of the map will update automatically.\n\n# Load the R packages we need to analyse this data\nlibrary(ggspatial)\nlibrary(sf)\nlibrary(tidyverse)\n\n# Download the data directly from a URL and store it as an object\nhomicides <- read_csv(\"https://mpjashby.github.io/crimemappingdata/glenrose_heights_homicides.csv\")\n\n# Convert the data to a simple features object, which we can use in functions \n# that work on spatial data\nhomicides_sf <- st_as_sf(\n  homicides, \n  coords = c(\"longitude\", \"latitude\"), \n  crs = \"EPSG:4326\"\n)\n\n# Transform the data to a co-ordinate reference system for the state of Georgia\nhomicides_sf_trans <- st_transform(homicides_sf, \"EPSG:26967\")\n\n# Plot the map\nggplot(homicides_sf_trans) + \n  annotation_map_tile(type = \"osm\", zoom = 15, progress = \"none\") + \n  geom_sf_label(\n    aes(label = label), \n    lineheight = 1, \n    size = 2.5, \n    hjust = 1, \n    vjust = 0\n  ) + \n  geom_sf(colour = \"white\", fill = \"mediumblue\", size = 4, shape = 21) + \n  scale_x_continuous(expand = expansion(mult = 1.5)) + \n  scale_y_continuous(expand = expansion(mult = 0.2)) + \n  labs(\n    title = \"Homicides in Glenrose Heights, 2019\",\n    caption = \"Background map by OpenStreetMap\"\n  ) +\n  theme_void()"
  },
  {
    "objectID": "02_your_first_crime_map/index.html#in-summary",
    "href": "02_your_first_crime_map/index.html#in-summary",
    "title": "\n2  Your first crime map\n",
    "section": "\n2.6 In summary",
    "text": "2.6 In summary\n\nWell done – you have finished your first mapping tutorial. You may not have understood every line of code in this tutorial, but we will cover them all in more detail over the rest of this course. By the end of the tutorials, you will be able to write code like this to create many different types of crime map.\n\nIn this tutorial you have learned how to load data into R, prepare it for use in making a map and then used it to make your first crime map of this course.\nThe map we have produced in this tutorial is effective for showing the locations of just a few crimes, but is too limited to show more complicated patterns or larger datasets. In the following tutorials, we will learn how to produce more sophisticated maps and spatial analysis. We will also learn how each of the functions that we have used in this tutorial work."
  },
  {
    "objectID": "03_data_wrangling/index.html#introduction",
    "href": "03_data_wrangling/index.html#introduction",
    "title": "\n3  Wrangling data\n",
    "section": "\n3.1 Introduction",
    "text": "3.1 Introduction\nA major step in using any data to make decisions or draw conclusions is data wrangling: the process of transforming data from the format in which we originally have it to the format needed to analyse and present it to our audience.\n\n\nStart off by watching this video that walks through the different steps in wrangling a dataset. We will cover all the steps in the video in more detail during the rest of this tutorial.\n\n\n3.1.1 Functions\nIn this tutorial we will learn how to wrangle data in R using functions – specialised pieces of code that do something to the data we give it. The code to use a function (sometimes called calling the function) has two parts: the function name followed by a pair of parentheses, inside which are zero or more arguments separated by commas. Arguments are a way of providing input that a function works on, or to fine-tune the way the function works (we will see many examples of this later). Remember that you can identify a function in R because the name will always have parentheses after it.\nOne basic R function is sqrt(), which calculates the square root of a number. The sqrt() function has only one argument: the number that we want to find the square root of.\n\nType sqrt(2) in the box below and click Run Code to calculate the square root of 2.\n\n\n\n\n\nsqrt(2)\n\n[1] 1.414214\n\n\nWhen you run code in R, by default R prints the output of your code – in this case, just the number 1.414214 (for now, you can ignore the number [1] in square brackets).\n\n3.1.2 Packages\n\n\n\nR contains thousands of different functions that do different things. A few functions are contained in the default installation of R that you have already installed (this is sometimes referred to as base R). But most functions are contained in packages, which are extensions to base R. Most packages focus on a particular type of data analysis, so that there are packages devoted to time-series analysis, testing whether events are clustered in particular places, network analysis and thousands of other tasks. Packages are often developed by experts in the field, and are typically updated to introduce new features.\nTo use a package in R, we must do two things:\n\n\ninstall the package, which we have to do just once on each computer we want to use, then\n\nload the package, which we have to do each time we restart R (which happens when we open RStudio or switch between projects).\n\nThe install.packages() function downloads and installs packages from the Comprehensive R Archive Network (universally known as CRAN), which contains about 19,300 different packages. Some packages that are still in the early stages of development are not available on CRAN, but all the packages we will use are there.\n\nSo to install (for example) the package called tidyverse, which we will use extensively in this tutorial, we would run the R code:\ninstall.packages(\"tidyverse\")\nWe only have to install a package once for each computer that we will use to run R, although we would have to do it again if we updated to a new version of R. Once a package is installed on our computer, we have to load it so that we can use it in our code. We load packages using the library() function, which should probably have been called load_package() but isn’t. So to load the tidyverse package, we run the code:\nlibrary(tidyverse)\nMany packages are focused on specialist tasks and so are only used occasionally, but a few packages are likely to be useful in almost all the code we write. Fortunately, packages can themselves load other packages, and all the main packages we need are themselves loaded by the tidyverse package. That is why you will often see library(tidyverse) at the top of R code in subsequent tutorials – that short line of code loads several packages containing hundreds of functions that we can use in data analysis.\n\nStats Illustrations by Allison Horst licensed under the Creative Commons Attribution licence."
  },
  {
    "objectID": "03_data_wrangling/index.html#loading-data",
    "href": "03_data_wrangling/index.html#loading-data",
    "title": "\n3  Wrangling data\n",
    "section": "\n3.2 Loading data",
    "text": "3.2 Loading data\nBefore we can do anything with any data, we have to load it into R. In this course we will read tabular data in comma-separated values (CSV) and Excel formats, as well as spatial data in different formats (because there are lots of ways to store spatial data). We will learn how to read CSV and Excel data now, but leave loading spatial data until later.\nTabular data contains multiple columns where every column has the same number of rows. For example, crime data might have columns for the type of crime, date and address at which the crime occurred.\n\n\n\nCrime data in rectangular format\n\ntype\ndate\naddress\n\n\n\nhomicide\n01 Apr 2022\n274 Main St\n\n\nnon-residential burglary\n11 Nov 2022\n541 Station Rd\n\n\npersonal robbery\n14 Mar 2023\n10 North Av\n\n\n\n\n\n\n3.2.1 Loading CSV data\n\nData stored in CSV format is easy to load with the read_csv() function from the readr package. readr is one of the packages loaded by the tidyverse package, so all we need to do to use this package is include the code library(tidyverse) on the first line of our R script. We will use comments (lines of code beginning with #) to help explain as we go.\n# Load the tidyverse suite of packages, including the readr package \n# that contains the read_csv() function\nlibrary(tidyverse)\n\n# We can load data from a file in the same folder as our R script\nsan_fran_rob <- read_csv(\"san_francisco_robbery.csv\")\n\n# Or another folder on your computer ('../' is short for the parent \n# folder of the current folder)\nsan_fran_rob <- read_csv(\"../san_francisco_robbery.csv\")\n\n# Or directly from a file online\nsan_fran_rob <- read_csv(\"http://example.com/san_francisco_robbery.csv\")\nIn each of these examples, the code stores the result of the read_csv() function in an object named san_fran_rob. Objects are places where we can store data. To create an object and store our data in it, we use the assignment operator <- (a less-than sign followed by a dash). Continually typing <- can be tedious, so in RStudio we can use the keyboard short cut Option+- (on Mac) or Alt+- (on Windows or Linux) to insert the complete operator.\n\nWhen choosing object names, it is important to remember that if you assign a value (such as the number 1 or the result of the function read_csv()) to an object name, R will overwrite any existing value of that object name. We can see this in a simple example:\none_to_ten <- 1:10\none_to_ten <- sqrt(2)\nIf we were to run this code, the object one_to_ten would not actually hold the numbers from one to ten, but instead the value 1.414214 (the square root of two). There is also no way to undo assignment of a value to an object, so once you have run the code one_to_ten <- sqrt(2) it is not possible to recover any previous value that was assigned to the object one_to_ten.\n\nObjects come in several different types, with tabular data typically being stored as a data frame. The read_csv() function actually produces a modern variation on the data frame called (slightly strangely) a tibble, which makes use of some advances in how R handles data since the data-frame format was set 20 years ago. Tibbles behave just like data frames almost all of the time (so much so that people working with tibbles often call them data frames) except for a few occasions where they behave in a more-convenient way.\nWe can use read_csv() to load data from https://mpjashby.github.io/crimemappingdata/san_francisco_robbery.csv and store it in an object called san_fran_rob.\n\nUse the code above to help you, or click the Solution button to reveal the answer.\n\n\n\n\n\nsan_fran_rob <- read_csv(\"https://mpjashby.github.io/crimemappingdata/san_francisco_robbery.csv\")\n\nRows: 951 Columns: 5\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (1): offense_type\ndbl  (3): uid, longitude, latitude\ndttm (1): date_time\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nIf the data are loaded successfully, R will list the columns in the data and the type of variable (numeric, date etc.) stored in each column. The format of this is somewhat esoteric, but if you are interested they are explained in the ‘Extra detail’ box below.\n\n\nWhat do the messages produced by read_csv() mean?\n\n\nBy default, the read_csv() function prints a message when it loads data to summarise the format of each data column. In the case of the san_fran_rob dataset, read_csv() tells us that:\n\nthere is one column called offense_type that contains character (chr) values,\nthere are three columns called uid, longitude and latitude containing numeric (dbl) values, and\nthere is one column called date_time that contains values stored as dates and times (dttm).\n\nThere are some other possible types of data, but we will learn about these later on. The numeric values are referred to as dbl values because they are stored in a format that can handle numbers that are not whole numbers (e.g. 123.456). This format for storing numbers is called the double-precision floating-point format, which is often known as the double format for short. Most numbers in R are stored in double format, so you can think of the format code dbl as meaning ‘numeric’.\n\n\nTo see the first few rows of data currently stored in an object, we can use the head() function.\n\nType the code needed to view the first few rows of the san_fran_rob object and click the Run Code button.\n\n\n\n\n\nhead(san_fran_rob)\n\n# A tibble: 6 × 5\n       uid offense_type     date_time           longitude latitude\n     <dbl> <chr>            <dttm>                  <dbl>    <dbl>\n1 24103841 personal robbery 2019-01-01 19:50:00     -122.     37.8\n2 24103948 personal robbery 2019-01-02 08:00:00     -122.     37.8\n3 24104162 personal robbery 2019-01-03 00:30:00     -122.     37.8\n4 24104203 personal robbery 2019-01-03 03:13:00     -122.     37.8\n5 24104237 personal robbery 2019-01-03 09:30:00     -122.     37.7\n6 24104238 personal robbery 2019-01-03 09:30:00     -122.     37.7\n\n\n\n3.2.2 Loading Excel data\n\nLoading data from Microsoft Excel files is very similar to loading CSV data, with a few important differences. Functions to load Excel data are contained in the readxl package, which was installed automatically when we installed the tidyverse package.\nThere are two main things we must do to import Excel data that are not required for importing CSV data. The first is that the readxl package cannot directly load files from a URL, instead only loading files that are present on your computer. To get round this, we will first download an Excel file and store it in a temporary directory (to avoid cluttering up our computers).\n\nUsing download.file() on Windows\nIf you are using a Windows computer, you may find that the download.file() function in the code below does not work as expected. This is because Windows handles files in a way that distinguishes between plain-text files such as .txt and .csv files and binary files, which includes most other file types (including compressed files). Since aggravated_assaults.xlsx is not a plain-text file, on Windows you need to specify that you want it to be downloaded as a binary file. To do this, add the argument mode = \"wb\" to the download.file() function so that it reads:\ndownload.file(\n  url = \"https://mpjashby.github.io/crimemappingdata/aggravated_assaults.xlsx\",\n  destfile = temp_file,\n  mode = \"wb\"\n)\nIf you are using a Mac or a Linux computer then you do not need to worry about this.\n\n\n# Specify the name of and location of our temporary file: it does not matter\n# what this file is called or where it is stored, so we use the tempfile()\n# function to create a file in the correct location automatically\ntemp_file <- tempfile(fileext = \".xlsx\")\n\n# Download the Excel file and store it in the temporary location\ndownload.file(\n  url = \"https://mpjashby.github.io/crimemappingdata/aggravated_assaults.xlsx\",\n  destfile = temp_file,\n  mode = \"wb\"\n)\n\nThe download.file() function does not produce any output if the file has been successfully downloaded, so you will not see any output when you run this code.\nNow we have downloaded our data, we can load it into R. Since Excel files can contain multiple sheets, we need to specify which sheet we would like to load into a tibble. We can use the excel_sheets() function to get a list of sheets in an Excel file:\n\n\n\n\n# Load the readxl package\nlibrary(readxl)\n\n# Get a list of sheets in an Excel file\nexcel_sheets(temp_file)\n\n[1] \"Austin\"     \"Fort Worth\" \"Seattle\"   \n\n\nWe can now load the sheet containing data for Austin and view the first few rows of the resulting object:\n\nagg_assault_data <- read_excel(temp_file, sheet = \"Austin\")\n\nhead(agg_assault_data)\n\n# A tibble: 6 × 5\n  date                longitude latitude location_type location_category\n  <dttm>                  <dbl>    <dbl> <chr>         <chr>            \n1 2019-01-01 00:00:00     -97.7     30.3 residence     residence        \n2 2019-01-01 00:00:00     -97.8     30.2 residence     residence        \n3 2019-01-01 00:01:00     -97.7     30.3 <NA>          <NA>             \n4 2019-01-01 00:15:00     -97.7     30.3 <NA>          <NA>             \n5 2019-01-01 00:27:00     -97.8     30.2 residence     residence        \n6 2019-01-01 00:30:00     -97.7     30.3 <NA>          <NA>             \n\n\nNow we have learned how to load our data into an object, we can use other R functions to work with that data in many different ways.\n\nDifferent types of data are loaded into R with different functions, e.g. CSV files are loaded with the read_csv() function from the readr package and Microsoft Excel files are loaded with the read_excel() function from the readxl package. You can download a table of functions for reading data into R to remind you of which function to use to load each type of file.\n\n\nLearn more about how to read data into R by reading this chapter of the free online book R for Data Science.\nExcel data can often be messy and the readxl package contains various other functions that can be used to deal with this. You can learn more about how to handle messy Excel data in this online tutorial.\n\n\n3.2.3 Check your understanding\nAnswer the following questions to check your understanding of what we’ve learned so far in this tutorial. If you get a question wrong, you can keep trying until you get the right answer."
  },
  {
    "objectID": "03_data_wrangling/index.html#selecting-columns",
    "href": "03_data_wrangling/index.html#selecting-columns",
    "title": "\n3  Wrangling data\n",
    "section": "\n3.3 Selecting columns",
    "text": "3.3 Selecting columns\nIn this section we will learn how to reduce the size of our data by selecting only the columns we need and discarding the rest. This can be particularly useful if we are working with a very-large dataset, or if we want to produce a table containing only some columns.\n\n\nWe can use the select() function from the dplyr package (one of the packages that is loaded automatically when we call the library(tidyverse) function) to select columns.\nIf we wanted to select just the date and location_type columns from the agg_assault_data we loaded in the previous section:\n\nselect(agg_assault_data, date, location_type)\n\n# A tibble: 8,696 × 2\n   date                location_type\n   <dttm>              <chr>        \n 1 2019-01-01 00:00:00 residence    \n 2 2019-01-01 00:00:00 residence    \n 3 2019-01-01 00:01:00 <NA>         \n 4 2019-01-01 00:15:00 <NA>         \n 5 2019-01-01 00:27:00 residence    \n 6 2019-01-01 00:30:00 <NA>         \n 7 2019-01-01 00:51:00 <NA>         \n 8 2019-01-01 01:00:00 residence    \n 9 2019-01-01 01:00:00 <NA>         \n10 2019-01-01 01:12:00 residence    \n# … with 8,686 more rows\n\n\n\nIn a previous section, we mentioned that the code needed to run (or call) a function in R has two parts: the function name followed by a pair of parentheses, inside which are zero or more arguments separated by commas. The arguments in the select() function (and many other functions in the dplyr package) work in a slightly different way to many other functions. Here, the first argument is the name of the data object that we want to select from. All the remaining arguments (here, date and location_type) are the names of the columns we want to select from the data.\nWe can select as many columns as we want, by just adding the names of the columns separated by commas.\n\nWrite the code necessary to select the longitude and latitude columns from the agg_assault_data object:\n\n\n\n\n\nselect(agg_assault_data, longitude, latitude)\n\n# A tibble: 8,696 × 2\n   longitude latitude\n       <dbl>    <dbl>\n 1     -97.7     30.3\n 2     -97.8     30.2\n 3     -97.7     30.3\n 4     -97.7     30.3\n 5     -97.8     30.2\n 6     -97.7     30.3\n 7     -97.7     30.3\n 8     -97.7     30.4\n 9     -97.7     30.3\n10     -97.7     30.3\n# … with 8,686 more rows\n\n\nThe columns in our new dataset will appear in the order in which we specify them in the select() function.\nWe can also use select() to rename columns at the same time as selecting them. For example, to select the columns date and location_type while also renaming location_type to be called type:\n\nselect(agg_assault_data, date, type = location_type)\n\n# A tibble: 8,696 × 2\n   date                type     \n   <dttm>              <chr>    \n 1 2019-01-01 00:00:00 residence\n 2 2019-01-01 00:00:00 residence\n 3 2019-01-01 00:01:00 <NA>     \n 4 2019-01-01 00:15:00 <NA>     \n 5 2019-01-01 00:27:00 residence\n 6 2019-01-01 00:30:00 <NA>     \n 7 2019-01-01 00:51:00 <NA>     \n 8 2019-01-01 01:00:00 residence\n 9 2019-01-01 01:00:00 <NA>     \n10 2019-01-01 01:12:00 residence\n# … with 8,686 more rows\n\n\nIf we want to rename a column while keeping all the columns in the data, we can instead use the rename() function (also from the dplyr package):\n\nrename(agg_assault_data, type = location_type)\n\n# A tibble: 8,696 × 5\n   date                longitude latitude type      location_category\n   <dttm>                  <dbl>    <dbl> <chr>     <chr>            \n 1 2019-01-01 00:00:00     -97.7     30.3 residence residence        \n 2 2019-01-01 00:00:00     -97.8     30.2 residence residence        \n 3 2019-01-01 00:01:00     -97.7     30.3 <NA>      <NA>             \n 4 2019-01-01 00:15:00     -97.7     30.3 <NA>      <NA>             \n 5 2019-01-01 00:27:00     -97.8     30.2 residence residence        \n 6 2019-01-01 00:30:00     -97.7     30.3 <NA>      <NA>             \n 7 2019-01-01 00:51:00     -97.7     30.3 <NA>      <NA>             \n 8 2019-01-01 01:00:00     -97.7     30.4 residence residence        \n 9 2019-01-01 01:00:00     -97.7     30.3 <NA>      <NA>             \n10 2019-01-01 01:12:00     -97.7     30.3 residence residence        \n# … with 8,686 more rows\n\n\nRemember that functions in R generally do not change existing objects, but instead produce (or return) new ones. This means if we want to store the result of this function so we can use it later, we have to assign the value returned by the function to a new object (or overwrite the existing object):\n\nagg_assault_locations <- select(agg_assault_data, lon = longitude, lat = latitude)\n\nhead(agg_assault_locations)\n\n# A tibble: 6 × 2\n    lon   lat\n  <dbl> <dbl>\n1 -97.7  30.3\n2 -97.8  30.2\n3 -97.7  30.3\n4 -97.7  30.3\n5 -97.8  30.2\n6 -97.7  30.3\n\n\n\nYou can learn more about selecting, filtering and arranging data using the functions in the dplyr package by reading this Introduction to dplyr tutorial.\n\n\n3.3.1 Check your understanding\n\n\n\n\nStats Illustrations by Allison Horst licensed under the Creative Commons Attribution licence."
  },
  {
    "objectID": "03_data_wrangling/index.html#filtering-rows",
    "href": "03_data_wrangling/index.html#filtering-rows",
    "title": "\n3  Wrangling data\n",
    "section": "\n3.4 Filtering rows",
    "text": "3.4 Filtering rows\nOften in crime mapping we will only be interested in part of a particular dataset. In the same way that we can select particular columns in our data, we can filter particular rows using the filter() function from the dplyr package.\n\n\nIf we were only interested in offences in the agg_assault_data dataset that occurred in residences, we could use filter():\n\nfilter(agg_assault_data, location_type == \"residence\")\n\n# A tibble: 4,385 × 5\n   date                longitude latitude location_type location_category\n   <dttm>                  <dbl>    <dbl> <chr>         <chr>            \n 1 2019-01-01 00:00:00     -97.7     30.3 residence     residence        \n 2 2019-01-01 00:00:00     -97.8     30.2 residence     residence        \n 3 2019-01-01 00:27:00     -97.8     30.2 residence     residence        \n 4 2019-01-01 01:00:00     -97.7     30.4 residence     residence        \n 5 2019-01-01 01:12:00     -97.7     30.3 residence     residence        \n 6 2019-01-01 01:20:00     -97.7     30.4 residence     residence        \n 7 2019-01-01 01:50:00     -97.7     30.3 residence     residence        \n 8 2019-01-01 02:21:00     -97.8     30.2 residence     residence        \n 9 2019-01-01 02:26:00     -97.8     30.2 residence     residence        \n10 2019-01-01 02:35:00     -97.7     30.4 residence     residence        \n# … with 4,375 more rows\n\n\nNote that:\n\nthe column name location_type is not surrounded by quotes but the column value \"residence\" is, and\nthe == (equal to) operator is used, since a single equals sign = has another meaning in R.\n\nWe can filter using the values of more than one column simultaneously. To filter offences in which the location_category is ‘leisure’ and the location_type is ‘bar/club’:\n\nfilter(\n  agg_assault_data, \n  location_category == \"leisure\", \n  location_type == \"bar/club\"\n)\n\n# A tibble: 0 × 5\n# … with 5 variables: date <dttm>, longitude <dbl>, latitude <dbl>,\n#   location_type <chr>, location_category <chr>\n\n\nAs well as filtering using the == operator, we can filter using the greater-than (>), less-than (<), greater-than-or-equal-to (>=) and less-than-or-equal-to (<=) operators. For example, we can choose offences that occurred in residences on or after 1 July 2019:\n\nfilter(\n  agg_assault_data, \n  location_type == \"residence\", \n  date >= as.Date(\"2019-07-01\")\n)\n\n# A tibble: 2,286 × 5\n   date                longitude latitude location_type location_category\n   <dttm>                  <dbl>    <dbl> <chr>         <chr>            \n 1 2019-07-01 02:40:00     -97.8     30.4 residence     residence        \n 2 2019-07-01 07:26:00     -97.7     30.3 residence     residence        \n 3 2019-07-01 08:25:00     -97.8     30.2 residence     residence        \n 4 2019-07-01 09:39:00     -97.7     30.3 residence     residence        \n 5 2019-07-01 09:40:00     -97.7     30.4 residence     residence        \n 6 2019-07-01 16:24:00     -97.8     30.1 residence     residence        \n 7 2019-07-01 17:30:00     -97.8     30.2 residence     residence        \n 8 2019-07-01 17:41:00     -97.8     30.2 residence     residence        \n 9 2019-07-01 18:03:00     -97.8     30.2 residence     residence        \n10 2019-07-01 18:16:00     -97.8     30.1 residence     residence        \n# … with 2,276 more rows\n\n\nSometimes we will want to filter rows that are one thing or another. We can do this with the | (or) operator. For example, we can filter offences that occurred either in leisure facilities or shopping malls on or after 1 July 2019:\n\nfilter(\n  agg_assault_data, \n  location_category == \"leisure\" | location_type == \"mall\", \n  date >= as.Date(\"2019-07-01\")\n)\n\n# A tibble: 10 × 5\n   date                longitude latitude location_type location_category\n   <dttm>                  <dbl>    <dbl> <chr>         <chr>            \n 1 2019-07-01 09:00:00     -97.7     30.3 entertainment leisure          \n 2 2019-07-15 14:06:00     -97.8     30.3 mall          retail           \n 3 2019-08-06 23:20:00     -97.7     30.4 mall          retail           \n 4 2019-08-18 13:31:00     -97.7     30.3 mall          retail           \n 5 2019-08-25 15:35:00     -97.8     30.3 mall          retail           \n 6 2019-09-06 15:58:00     -97.7     30.4 mall          retail           \n 7 2019-10-05 16:47:00     -97.7     30.2 mall          retail           \n 8 2019-11-01 22:23:00     -97.7     30.3 entertainment leisure          \n 9 2019-11-18 16:29:00     -97.7     30.4 mall          retail           \n10 2019-11-27 14:14:00     -97.8     30.3 mall          retail           \n\n\nIf we want to filter offences that have any one of several different values of the same column, we can use the %in% (in) operator. To filter offences that occurred in either streets or publicly accessible open spaces:\n\nfilter(agg_assault_data, location_category %in% c(\"open space\", \"street\"))\n\n# A tibble: 165 × 5\n   date                longitude latitude location_type location_category\n   <dttm>                  <dbl>    <dbl> <chr>         <chr>            \n 1 2019-01-07 21:37:00     -97.8     30.5 green space   open space       \n 2 2019-01-08 14:02:00     -97.7     30.3 green space   open space       \n 3 2019-01-10 01:59:00     -97.6     30.3 green space   open space       \n 4 2019-01-11 17:00:00     -97.7     30.3 green space   open space       \n 5 2019-01-12 15:50:00     -97.8     30.2 green space   open space       \n 6 2019-01-12 16:00:00     -97.8     30.4 green space   open space       \n 7 2019-01-15 07:20:00     -97.7     30.2 green space   open space       \n 8 2019-01-17 00:00:00     -97.7     30.4 green space   open space       \n 9 2019-01-22 22:00:00     -97.7     30.2 green space   open space       \n10 2019-01-29 01:40:00     -97.7     30.2 green space   open space       \n# … with 155 more rows\n\n\nThe code c(\"open space\", \"street\") produces what is referred to in R as a vector (sometimes referred to as an atomic vector, especially in error messages). A vector is a one-dimensional sequence of values of the same type (i.e. all numbers, all character strings etc.). For example, a vector might hold several strings of text (as in the vector c(\"open space\", \"street\")) or a series of numbers such as c(1, 2, 3). There is lots we could learn about vectors, but for now it’s only necessary to know that we can create vectors with the c() or combine function.\nIf we wanted to re-use a vector of values several times in our code, it might make sense to store the vector as an object. For example:\n\n# Create vector of location types we are interested in\nlocation_types <- c(\"open space\", \"street\")\n\n# Filter the data\nfilter(agg_assault_data, location_category %in% location_types)\n\n# A tibble: 165 × 5\n   date                longitude latitude location_type location_category\n   <dttm>                  <dbl>    <dbl> <chr>         <chr>            \n 1 2019-01-07 21:37:00     -97.8     30.5 green space   open space       \n 2 2019-01-08 14:02:00     -97.7     30.3 green space   open space       \n 3 2019-01-10 01:59:00     -97.6     30.3 green space   open space       \n 4 2019-01-11 17:00:00     -97.7     30.3 green space   open space       \n 5 2019-01-12 15:50:00     -97.8     30.2 green space   open space       \n 6 2019-01-12 16:00:00     -97.8     30.4 green space   open space       \n 7 2019-01-15 07:20:00     -97.7     30.2 green space   open space       \n 8 2019-01-17 00:00:00     -97.7     30.4 green space   open space       \n 9 2019-01-22 22:00:00     -97.7     30.2 green space   open space       \n10 2019-01-29 01:40:00     -97.7     30.2 green space   open space       \n# … with 155 more rows\n\n\nFinally, you can filter based on the output of any R function that returns TRUE or FALSE. For example, missing values are represented in R as NA. We can test whether a value is missing using the is.na() function. If we wanted to remove rows from our data that had missing location types, we would filter for those rows that are not NA. We can do this by combining the is.na() function with the ! (not) operator:\n\nfilter(agg_assault_data, !is.na(location_type))\n\n# A tibble: 5,344 × 5\n   date                longitude latitude location_type location_category\n   <dttm>                  <dbl>    <dbl> <chr>         <chr>            \n 1 2019-01-01 00:00:00     -97.7     30.3 residence     residence        \n 2 2019-01-01 00:00:00     -97.8     30.2 residence     residence        \n 3 2019-01-01 00:27:00     -97.8     30.2 residence     residence        \n 4 2019-01-01 01:00:00     -97.7     30.4 residence     residence        \n 5 2019-01-01 01:12:00     -97.7     30.3 residence     residence        \n 6 2019-01-01 01:20:00     -97.7     30.4 residence     residence        \n 7 2019-01-01 01:35:00     -97.7     30.3 hotel         hotel            \n 8 2019-01-01 01:50:00     -97.7     30.3 residence     residence        \n 9 2019-01-01 02:21:00     -97.8     30.2 residence     residence        \n10 2019-01-01 02:26:00     -97.8     30.2 residence     residence        \n# … with 5,334 more rows\n\n\nWe will see lots more examples of how to use filter() in future tutorials.\n\n3.4.1 Check your understanding\n\n\n\n\nStats Illustrations by Allison Horst licensed under the Creative Commons Attribution licence."
  },
  {
    "objectID": "03_data_wrangling/index.html#transforming-values",
    "href": "03_data_wrangling/index.html#transforming-values",
    "title": "\n3  Wrangling data\n",
    "section": "\n3.5 Transforming values",
    "text": "3.5 Transforming values\nIt is often useful to create new columns in our data, or change the values of existing columns. The mutate() function in the dplyr package gives us a way to transform existing columns in our dataset using almost any R function.\n\n\n\nFor example, say we wanted to create a new column in our aggravated-assault dataset specifying the day of the week on which each crime occurred. We can do this using the wday() function from the lubridate package (using the label = TRUE argument to produce weekday names, rather than numbers):\n\nlibrary(lubridate)\n\nmutate(agg_assault_data, weekday = wday(date, label = TRUE))\n\n# A tibble: 8,696 × 6\n   date                longitude latitude location_type location_categ…¹ weekday\n   <dttm>                  <dbl>    <dbl> <chr>         <chr>            <ord>  \n 1 2019-01-01 00:00:00     -97.7     30.3 residence     residence        Tue    \n 2 2019-01-01 00:00:00     -97.8     30.2 residence     residence        Tue    \n 3 2019-01-01 00:01:00     -97.7     30.3 <NA>          <NA>             Tue    \n 4 2019-01-01 00:15:00     -97.7     30.3 <NA>          <NA>             Tue    \n 5 2019-01-01 00:27:00     -97.8     30.2 residence     residence        Tue    \n 6 2019-01-01 00:30:00     -97.7     30.3 <NA>          <NA>             Tue    \n 7 2019-01-01 00:51:00     -97.7     30.3 <NA>          <NA>             Tue    \n 8 2019-01-01 01:00:00     -97.7     30.4 residence     residence        Tue    \n 9 2019-01-01 01:00:00     -97.7     30.3 <NA>          <NA>             Tue    \n10 2019-01-01 01:12:00     -97.7     30.3 residence     residence        Tue    \n# … with 8,686 more rows, and abbreviated variable name ¹​location_category\n\n\nDepending on the width of your screen, you might need to click the ▸ button to see the new variable.\nWe can also categorise an existing variable, for example creating a variable to show whether an offence occurred in the northern or southern half of the city:\n\nmutate(\n  agg_assault_data, \n  region = if_else(latitude > median(latitude), \"northern\", \"southern\")\n)\n\n# A tibble: 8,696 × 6\n   date                longitude latitude location_type location_category region\n   <dttm>                  <dbl>    <dbl> <chr>         <chr>             <chr> \n 1 2019-01-01 00:00:00     -97.7     30.3 residence     residence         north…\n 2 2019-01-01 00:00:00     -97.8     30.2 residence     residence         south…\n 3 2019-01-01 00:01:00     -97.7     30.3 <NA>          <NA>              south…\n 4 2019-01-01 00:15:00     -97.7     30.3 <NA>          <NA>              south…\n 5 2019-01-01 00:27:00     -97.8     30.2 residence     residence         south…\n 6 2019-01-01 00:30:00     -97.7     30.3 <NA>          <NA>              south…\n 7 2019-01-01 00:51:00     -97.7     30.3 <NA>          <NA>              north…\n 8 2019-01-01 01:00:00     -97.7     30.4 residence     residence         north…\n 9 2019-01-01 01:00:00     -97.7     30.3 <NA>          <NA>              north…\n10 2019-01-01 01:12:00     -97.7     30.3 residence     residence         north…\n# … with 8,686 more rows\n\n\nWe can change existing columns, although (as with objects) there is no way to undo this so you should only replace columns if you are sure you will not need them. For example, if we wanted to remove the time portion of the date variable (which may sometimes be useful, as shown in the next section) using the as_date() function (also from the lubridate package) and at the same time create the weekday variable:\n\n\n\n\nmutate(\n  agg_assault_data, \n  date = as_date(date),\n  weekday = wday(date, label = TRUE)\n)\n\n# A tibble: 8,696 × 6\n   date       longitude latitude location_type location_category weekday\n   <date>         <dbl>    <dbl> <chr>         <chr>             <ord>  \n 1 2019-01-01     -97.7     30.3 residence     residence         Tue    \n 2 2019-01-01     -97.8     30.2 residence     residence         Tue    \n 3 2019-01-01     -97.7     30.3 <NA>          <NA>              Tue    \n 4 2019-01-01     -97.7     30.3 <NA>          <NA>              Tue    \n 5 2019-01-01     -97.8     30.2 residence     residence         Tue    \n 6 2019-01-01     -97.7     30.3 <NA>          <NA>              Tue    \n 7 2019-01-01     -97.7     30.3 <NA>          <NA>              Tue    \n 8 2019-01-01     -97.7     30.4 residence     residence         Tue    \n 9 2019-01-01     -97.7     30.3 <NA>          <NA>              Tue    \n10 2019-01-01     -97.7     30.3 residence     residence         Tue    \n# … with 8,686 more rows\n\n\nYou may sometimes want to change only some values in a column. We can do this in various ways, depending on which values we want to change:\n\nmutate(\n  agg_assault_data,\n  # Change a single value with a new value (and otherwise keep the existing \n  # value) using the if_else() function\n  location_type = if_else(location_type == \"street\", \"road\", location_type),\n  # Change multiple values in a categorical variable using the recode() \n  # function, in which values are changed using arguments in the format\n  # old_value = new_value\n  location_category = recode(\n    location_category, \n    \"open space\" = \"public open space\",\n    \"street\" = \"street or road\"\n  )\n)\n\n# A tibble: 8,696 × 5\n   date                longitude latitude location_type location_category\n   <dttm>                  <dbl>    <dbl> <chr>         <chr>            \n 1 2019-01-01 00:00:00     -97.7     30.3 residence     residence        \n 2 2019-01-01 00:00:00     -97.8     30.2 residence     residence        \n 3 2019-01-01 00:01:00     -97.7     30.3 <NA>          <NA>             \n 4 2019-01-01 00:15:00     -97.7     30.3 <NA>          <NA>             \n 5 2019-01-01 00:27:00     -97.8     30.2 residence     residence        \n 6 2019-01-01 00:30:00     -97.7     30.3 <NA>          <NA>             \n 7 2019-01-01 00:51:00     -97.7     30.3 <NA>          <NA>             \n 8 2019-01-01 01:00:00     -97.7     30.4 residence     residence        \n 9 2019-01-01 01:00:00     -97.7     30.3 <NA>          <NA>             \n10 2019-01-01 01:12:00     -97.7     30.3 residence     residence        \n# … with 8,686 more rows\n\n\nWe could also make changes based on more-complicated sets of criteria using the case_when() function, but we will return to that in a future tutorial.\nThe R functions that you use inside mutate() must return the same number of values as there are rows in the dataset. This is true for most R functions (which are referred to as vectorised functions), but there are some – such as mean() and max() – that return a single value. These summarising functions cannot be used inside mutate() (you will see an error message if you try) but are instead used with the next data-wrangling function we will learn about: summarise().\n\nStats Illustrations by Allison Horst licensed under the Creative Commons Attribution licence."
  },
  {
    "objectID": "03_data_wrangling/index.html#summarising-rows",
    "href": "03_data_wrangling/index.html#summarising-rows",
    "title": "\n3  Wrangling data\n",
    "section": "\n3.6 Summarising rows",
    "text": "3.6 Summarising rows\nSummarising data is often useful in crime analysis. We can use the summarise() function from the dplyr package to produce summaries of different columns in our data. There is an identical function called summarize() so that you do not have to remember whether to use the US or British spelling.\nBy default, summarise() collapses data into a single row, with each column summarised using a function that you specify. For example, suppose you want to find out which police station a specialist squad should be based at to most easily respond to reports of serious assaults. You might do this by working out the weighted centre of all the offence locations, i.e. the means of the longitudes and latitudes for all the crimes. You could then base the squad at the police station that was closest to the weighted centre.\n\nsummarise(\n  agg_assault_data, \n  mean_lng = mean(longitude, na.rm = TRUE),\n  mean_lat = mean(latitude, na.rm = TRUE)\n)\n\n# A tibble: 1 × 2\n  mean_lng mean_lat\n     <dbl>    <dbl>\n1    -97.7     30.3\n\n\n\n\nWhat does the argument na.rm = TRUE do?\n\n\nLots of functions in R have an argument called na.rm that can be set to either TRUE or FALSE. Setting na.rm = TRUE in this case specifies that the mean() function should remove (rm) any missing (NA) values before calculating the mean.\nIf we do not specify this and our data contain any missing values, the mean() function will return NA. Functions in R do this because it is not possible to completely answer the question ‘what is the mean of these values?’ if some of the values are missing.\nThis logic applies in lots of cases. For example, if you create an R object called value with the code value <- 2 and then run the R code value > 1, you will get the answer TRUE. But if you set the object value to be NA using the code value <- NA, when you run the R code value > 1 you will get the answer NA. This is because there is no way to know if the missing value represented by NA is greater than 1 or not. This is why it is often useful to calculate statistics such as a mean value after removing any missing values using the na.rm = TRUE argument.\n\n\nsummarise() becomes more useful if we first divide our data into groups, since we then get a summary for each group separately. We can use the group_by() function to specify which columns denote which group each row is in. So if we wanted to repeat the calculation above, but separately for each value of location_category:\n\nsummarise(\n  group_by(agg_assault_data, location_category),\n  mean_lng = mean(longitude, na.rm = TRUE),\n  mean_lat = mean(latitude, na.rm = TRUE)\n)\n\n# A tibble: 12 × 3\n   location_category mean_lng mean_lat\n   <chr>                <dbl>    <dbl>\n 1 commercial           -97.7     30.3\n 2 education            -97.7     30.3\n 3 government           -97.7     30.3\n 4 healthcare           -97.7     30.3\n 5 hotel                -97.7     30.3\n 6 leisure              -97.7     30.4\n 7 open space           -97.7     30.3\n 8 other                -97.7     30.3\n 9 residence            -97.7     30.3\n10 retail               -97.7     30.3\n11 transportation       -97.7     30.2\n12 <NA>                 -97.7     30.3\n\n\nThis code might be slightly difficult to read, since the group_by() function is called inside summarise(), but we will learn a way of writing this code that might be easier to read in the final section of this tutorial.\nYou can add multiple grouping variables if you want to generate summary values for groups within groups:\n\nsummarise(\n  group_by(agg_assault_data, location_category, location_type),\n  mean_lng = mean(longitude, na.rm = TRUE),\n  mean_lat = mean(latitude, na.rm = TRUE)\n)\n\n`summarise()` has grouped output by 'location_category'. You can override using\nthe `.groups` argument.\n\n\n# A tibble: 24 × 4\n# Groups:   location_category [12]\n   location_category location_type     mean_lng mean_lat\n   <chr>             <chr>                <dbl>    <dbl>\n 1 commercial        construction         -97.7     30.3\n 2 commercial        factory/warehouse    -97.7     30.3\n 3 commercial        finance              -97.7     30.2\n 4 commercial        office               -97.7     30.3\n 5 commercial        storage              -97.7     30.2\n 6 education         child care           -97.7     30.4\n 7 education         college              -97.8     30.3\n 8 education         school               -97.7     30.3\n 9 government        government           -97.7     30.3\n10 healthcare        healthcare           -97.7     30.3\n# … with 14 more rows\n\n\n\n\nDid you see a message saying `summarise()` has grouped output by 'location_category'?\n\n\nYou might have noticed the that code above produced a message specifying how groups have been handled by summarise(). This message reminds you that by default the summarise() functions strips the final level of grouping added by group_by(). This allows us to summarise data using a detailed set of groups and then summarise those summaries using more-general groups. In practice, this ability has been a source of some confusion, so R now prints a message to remind you that the data are still grouped after you run summarise(). If you want to hide this message then you can add .groups = \"drop\" to the summarise() function to remove all the groups in the data.\n\n\n\n3.6.1 Counting rows\nOne form of summarising grouped data is so common that it gets its own function: count(). This simply counts the number of rows of data in each group. So if you wanted to know how many aggravated assaults had occurred in each location category and type:\n\ncount(group_by(agg_assault_data, location_category, location_type), sort = TRUE)\n\n# A tibble: 24 × 3\n# Groups:   location_category, location_type [24]\n   location_category location_type         n\n   <chr>             <chr>             <int>\n 1 residence         residence          4385\n 2 <NA>              <NA>               3352\n 3 hotel             hotel               248\n 4 other             other               205\n 5 open space        green space         165\n 6 retail            convenience store    90\n 7 healthcare        healthcare           36\n 8 retail            supermarket          32\n 9 retail            other retail         30\n10 commercial        office               28\n# … with 14 more rows\n\n\nThe sort = TRUE argument sorts the counts in descending order. You can do more-sophisticated sorting using the arrange() function, which we will learn about in the next section."
  },
  {
    "objectID": "03_data_wrangling/index.html#arranging-rows",
    "href": "03_data_wrangling/index.html#arranging-rows",
    "title": "\n3  Wrangling data\n",
    "section": "\n3.7 Arranging rows",
    "text": "3.7 Arranging rows\nIt is sometimes useful to be able to place rows in a dataset into a particular order. We can do this using the arrange() function from the dplyr package. For example, we can sort the aggravated-assault data by date:\n\narrange(agg_assault_data, date)\n\n# A tibble: 8,696 × 5\n   date                longitude latitude location_type location_category\n   <dttm>                  <dbl>    <dbl> <chr>         <chr>            \n 1 2019-01-01 00:00:00     -97.7     30.3 residence     residence        \n 2 2019-01-01 00:00:00     -97.8     30.2 residence     residence        \n 3 2019-01-01 00:01:00     -97.7     30.3 <NA>          <NA>             \n 4 2019-01-01 00:15:00     -97.7     30.3 <NA>          <NA>             \n 5 2019-01-01 00:27:00     -97.8     30.2 residence     residence        \n 6 2019-01-01 00:30:00     -97.7     30.3 <NA>          <NA>             \n 7 2019-01-01 00:51:00     -97.7     30.3 <NA>          <NA>             \n 8 2019-01-01 01:00:00     -97.7     30.4 residence     residence        \n 9 2019-01-01 01:00:00     -97.7     30.3 <NA>          <NA>             \n10 2019-01-01 01:12:00     -97.7     30.3 residence     residence        \n# … with 8,686 more rows\n\n\nBy default, arrange() sorts rows in ascending order, i.e. it sorts numeric values from the smallest to the largest, dates from earliest to latest and character values alphabetically. We can instead sort values in descending order by wrapping the name of a column in the desc() function:\n\narrange(agg_assault_data, desc(date))\n\n# A tibble: 8,696 × 5\n   date                longitude latitude location_type location_category\n   <dttm>                  <dbl>    <dbl> <chr>         <chr>            \n 1 2019-12-31 23:33:00     -97.7     30.4 residence     residence        \n 2 2019-12-31 23:26:00     -97.7     30.4 residence     residence        \n 3 2019-12-31 23:20:00     -97.7     30.3 <NA>          <NA>             \n 4 2019-12-31 23:19:00     -97.7     30.3 <NA>          <NA>             \n 5 2019-12-31 22:39:00     -97.7     30.3 other         other            \n 6 2019-12-31 22:29:00     -97.7     30.3 residence     residence        \n 7 2019-12-31 21:13:00     -97.8     30.2 residence     residence        \n 8 2019-12-31 20:52:00     -97.7     30.3 residence     residence        \n 9 2019-12-31 20:44:00     -97.7     30.3 residence     residence        \n10 2019-12-31 20:19:00     -97.7     30.3 <NA>          <NA>             \n# … with 8,686 more rows\n\n\nWe can also sort the data based on multiple columns – the data are sorted first on the first column that you specify, with tied rows then sorted on the subsequent columns in order.\n\narrange(agg_assault_data, date, desc(location_type), location_category)\n\n# A tibble: 8,696 × 5\n   date                longitude latitude location_type location_category\n   <dttm>                  <dbl>    <dbl> <chr>         <chr>            \n 1 2019-01-01 00:00:00     -97.7     30.3 residence     residence        \n 2 2019-01-01 00:00:00     -97.8     30.2 residence     residence        \n 3 2019-01-01 00:01:00     -97.7     30.3 <NA>          <NA>             \n 4 2019-01-01 00:15:00     -97.7     30.3 <NA>          <NA>             \n 5 2019-01-01 00:27:00     -97.8     30.2 residence     residence        \n 6 2019-01-01 00:30:00     -97.7     30.3 <NA>          <NA>             \n 7 2019-01-01 00:51:00     -97.7     30.3 <NA>          <NA>             \n 8 2019-01-01 01:00:00     -97.7     30.4 residence     residence        \n 9 2019-01-01 01:00:00     -97.7     30.3 <NA>          <NA>             \n10 2019-01-01 01:12:00     -97.7     30.3 residence     residence        \n# … with 8,686 more rows\n\n\n\n3.7.1 Check your understanding\nType the code necessary to arrange agg_assault_data in order of latitude, in descending order (from largest to smallest)\nRun your code using the Run Code button, then (if necessary) correct your code and run it again. Once you are happy that your code does what it is intended to do, click the Solution button to check.\n\n\n\n\narrange(agg_assault_data, desc(latitude))\n\n# A tibble: 8,696 × 5\n   date                longitude latitude location_type location_category\n   <dttm>                  <dbl>    <dbl> <chr>         <chr>            \n 1 2019-07-05 10:33:00     -97.7     30.5 residence     residence        \n 2 2019-09-28 19:31:00     -97.8     30.5 residence     residence        \n 3 2019-09-20 13:30:00     -97.8     30.5 residence     residence        \n 4 2019-08-25 22:04:00     -97.8     30.5 <NA>          <NA>             \n 5 2019-11-19 12:00:00     -97.8     30.5 school        education        \n 6 2019-01-26 19:52:00     -97.8     30.5 residence     residence        \n 7 2019-07-15 13:37:00     -97.8     30.5 <NA>          <NA>             \n 8 2019-01-28 21:14:00     -97.8     30.5 residence     residence        \n 9 2019-02-03 00:53:00     -97.8     30.5 residence     residence        \n10 2019-07-31 01:37:00     -97.8     30.5 residence     residence        \n# … with 8,686 more rows"
  },
  {
    "objectID": "03_data_wrangling/index.html#saving-data",
    "href": "03_data_wrangling/index.html#saving-data",
    "title": "\n3  Wrangling data\n",
    "section": "\n3.8 Saving data",
    "text": "3.8 Saving data\nOnce we have finished wrangling a particular dataset, it is often useful to save it to a file so that we can use it again in future without going through all the steps of data wrangling again.\nMost R functions that begin with read_ (like read_csv() and read_excel()) have equivalent functions that begin write_ and which save data into a particular file format. In this example, we will use the write_csv() function from the readr package, which is loaded when we load the tidyverse package.\n# We can write data to a file in the same folder as our R script\nwrite_csv(agg_assault_data, \"fort_worth_agg_assault.csv\")\n\n# Or another folder on your computer ('../../' is short for the parent folder of\n# the parent folder of the current folder)\nwrite_csv(agg_assault_data, \"../../fort_worth_agg_assault.csv\")\nFor very large datasets, we can save a compressed version of the file by adding .gz (for gzip) to the end of the file name, which tells R to compress the file after creating it.\nwrite_csv(agg_assault_data, \"fort_worth_agg_assault.csv.gz\")\nread_csv() can read gzipped CSV files, but some other programs (such as Excel) cannot, so only use this option if you are sure you will only need to open the file in software that can handle it.\nThere are corresponding write functions for other types of data (which we will come back to when we learn how to handle spatial data), but in this course we will store all non-spatial data in CSV format because it can be read by many different programs."
  },
  {
    "objectID": "03_data_wrangling/index.html#stringing-functions-together",
    "href": "03_data_wrangling/index.html#stringing-functions-together",
    "title": "\n3  Wrangling data\n",
    "section": "\n3.9 Stringing functions together",
    "text": "3.9 Stringing functions together\nIn this tutorial we have learned how to use the dplyr functions select(), filter(), mutate(), summarise() and arrange() to wrangle data from one format to another. Data wrangling is part of almost all data analysis, so these are skills we will use frequently.\nData wrangling often involves multiple steps. For example, we might want to load some data, select certain columns, filter some rows, mutate some of the variables, summarise the dataset and save the result. We can do each of these steps separately, assigning the result of each step to a new object.\n\n# Load the lubridate package, since we will need it below\nlibrary(lubridate)\n\n# Read San Francisco robbery data\nrobbery1 <- read_csv(\"https://mpjashby.github.io/crimemappingdata/san_francisco_robbery.csv\")\n\nRows: 951 Columns: 5\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (1): offense_type\ndbl  (3): uid, longitude, latitude\ndttm (1): date_time\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n# Select only the columns we need\nrobbery2 <- select(robbery1, date_time)\n\n# Filter only those offences that occurred in the first quarter of 2019\nrobbery3 <- filter(robbery2, as.Date(date_time) <= as.Date(\"2019-03-31\"))\n\n# Create a new weekday variable\nrobbery4 <- mutate(robbery3, weekday = wday(date_time, label = TRUE))\n\n# Count how many offences occurred on each weekday\nq1_weekday_counts <- count(robbery4, weekday)\n\n# Print the first few rows of the result\nhead(q1_weekday_counts, n = 7)\n\n# A tibble: 7 × 2\n  weekday     n\n  <ord>   <int>\n1 Sun        34\n2 Mon        27\n3 Tue        28\n4 Wed        20\n5 Thu        34\n6 Fri        27\n7 Sat        26\n\n\nThis code works, but involves creating six new objects, even though we only need the final object for our analysis. You may notice that the first argument expected by select(), filter(), mutate() and count() is always the data tibble produced by the previous step. This means we can skip saving the result of each step as a new object, and just run the previous function inside the first function. This approach produces the following code, which produces exactly the same result as the code above.\n\nq1_weekday_counts <- count(\n  mutate(\n    filter(\n      select(\n        read_csv(\"https://mpjashby.github.io/crimemappingdata/san_francisco_robbery.csv\"),\n        -offense_type\n      ), \n      as.Date(date_time) <= as.Date(\"2019-03-31\")\n    ),\n    weekday = wday(date_time, label = TRUE)\n  ),\n  weekday\n)\n\nRows: 951 Columns: 5\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (1): offense_type\ndbl  (3): uid, longitude, latitude\ndttm (1): date_time\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nhead(q1_weekday_counts, n = 7)\n\n# A tibble: 7 × 2\n  weekday     n\n  <ord>   <int>\n1 Sun        34\n2 Mon        27\n3 Tue        28\n4 Wed        20\n5 Thu        34\n6 Fri        27\n7 Sat        26\n\n\nThis code works and takes up less space, but it’s quite difficult to read – which can be a problem for finding and fixing problems with your code. For example, it’s quite hard (without counting pairs of parentheses) to work out that the reference to the column weekday on line 13 of this code belongs to the group_by() function on line 2.\nIt’s possible to write this code so that it is readable and does not require us to create multiple different objects to store the result of each step in our code. This method uses the |> (or pipe) operator. The pipe operator works by using the result of the code on the left-hand side of the pipe as the first argument to a function on the right-hand side. So the code x  |> fun1()  |> fun2(y) is the same as the code fun2(fun1(x), y), but it is much easier to see that fun1() is run before fun2(). It may be useful to read the pipe operator as ‘and then’, since piped code does the first thing and then the second thing with the result and then the third thing with the result of that, and so on. Piped code (sometimes called a pipeline) is a lot like the series of steps in a recipe.\nSince each function we are using returns the new data, and the first argument to all of those functions is the name of the input data object, the pipe means we can just omit the first argument to all except the first function.\n\nsan_fran_rob <- read_csv(\"https://mpjashby.github.io/crimemappingdata/san_francisco_robbery.csv\")\n\nRows: 951 Columns: 5\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (1): offense_type\ndbl  (3): uid, longitude, latitude\ndttm (1): date_time\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nq1_weekday_counts <- san_fran_rob |> \n  select(-offense_type) |> \n  filter(as.Date(date_time) <= as.Date(\"2019-03-31\")) |> \n  mutate(weekday = wday(date_time, label = TRUE)) |> \n  count(weekday)\n\nhead(q1_weekday_counts, n = 7)\n\n# A tibble: 7 × 2\n  weekday     n\n  <ord>   <int>\n1 Sun        34\n2 Mon        27\n3 Tue        28\n4 Wed        20\n5 Thu        34\n6 Fri        27\n7 Sat        26\n\n\nThis code strikes a good balance between being easy to read and not requiring us to manage lots of intermediate variables. You might not find the pipe operator completely intuitive at the moment, but it will become easier as you see more examples in future tutorials.\n\n\nWhat about the %>% pipe operator?\n\n\nIf you have learned any R coding before, you might have learned to use the %>% pipe operator from the magrittr package. The %>% pipe operator was introduced several years ago to allow people to construct pipelines of code in R. The %>% operator was so widely used that the team that writes the R programming language decided to provide a pipe operator in R itself, to avoid the need to load the magrittr package.\nYou will still see the %>% pipe operator used in lots of R code examples online. In almost all cases, when you see %>% you can replace it with the R pipe operator |>, since they both work in very similar ways.\n\n\n\nYou can find out more about how to use the pipe operator in the Introducing magrittr online tutorial."
  },
  {
    "objectID": "03_data_wrangling/index.html#in-summary",
    "href": "03_data_wrangling/index.html#in-summary",
    "title": "\n3  Wrangling data\n",
    "section": "\n3.10 In summary",
    "text": "3.10 In summary\n\nIn this tutorial, you have learned how to wrangle data in R using functions from packages in the tidyverse suite of packages. You can now construct a complete pipeline of R code to take raw data and transform it into the format(s) we need to effectively map crimes.\n\n\nDeveloping your data wrangling skills will help you to produce better, faster analysis of crime (and other) data. If you would like to develop your skills further, you might be interested in:\n\n\nData Wrangling with R by Claudia Engel, a free online book that explains the functions introduced in this tutorial (and some others) in more detail.\n\nData transformation with dplyr cheat sheet by the team that makes RStudio, which provides a handy two-page guide to the main functions in the dplyr package, which is very useful for reminding you of the code needed to run each of the functions we have used in this tutorial.\n\nData wrangling with R and RStudio by Garrett Grolemund, a recording of a webinar covering the data-wrangling functions introduced in this tutorial and some other very-useful related functions.\n\nR for Data Science by Hadley Wickham and Garrett Grolemund, a free online book that is the bible for wrangling data in R."
  },
  {
    "objectID": "04_code_with_style/index.html#introduction",
    "href": "04_code_with_style/index.html#introduction",
    "title": "\n4  Code with style\n",
    "section": "\n4.1 Introduction",
    "text": "4.1 Introduction\nNow that you’re beginning to write code in R, it’s time to introduce a few conventions for how to write code so that it’s easier to read. This is important because “good coding style is like correct punctuation: you can manage without it, butitsuremakesthingseasiertoread”.\nWriting readable code is particularly important because it is much easier to find mistakes in well-written code, and your code will inevitably contain mistakes (everyone’s code contains mistakes!).\nWriting code has been compared to performing in a band:\n\nI liken the situation to a group of musicians trying to form a band. Each one comes in believing that their way of doing things is best (their “method” or “process”). The band will struggle so long as everyone is trying to do their own thing. It’s impossible to create good music unless everyone in the band agrees on the tempo, the style and who should take lead during a song. Anyone who has ever heard a high school band perform knows this to be true. Unless everyone is on the same page, you aren’t going to accomplish much.\n\nYou might be relaxing at this point, thinking “that isn’t a problem for me, because I’m the only person who is going to be working on my code”. If so, think again. It’s been said that in data science that there are always at least two people working on a project: the you who is working on the code now, and the past you who has worked on the same code previously. The problem is that past you does not answer emails. So you can save future you a lot of hassle later by writing readable code.\nThis tutorial introduces some basic guidelines on formatting your code. This is a condensed version of the The tidyverse style guide, which provides lots more detail. All the code you see in the tutorials in this course was written following this style guide.\n\n\n\nOn Perl from Three Panel Soul. Some content on this page contains public sector information licensed under the Open Government Licence v3."
  },
  {
    "objectID": "04_code_with_style/index.html#organising-your-code",
    "href": "04_code_with_style/index.html#organising-your-code",
    "title": "\n4  Code with style\n",
    "section": "\n4.2 Organising your code",
    "text": "4.2 Organising your code\nUp until now we have only written code in the code boxes within these interactive tutorials, but when you move on to make maps using your own R code you will typically write that code in the RStudio source panel. Your code will usually be in one or more files with the .R file extension. Whether you keep all your code for a specific project in one file or split it into multiple files is up to you. Generally you should do whatever makes it easier to understand how your code is structured.\n\n4.2.1 Leaving notes for future you\nWithin each .R file, you can make your code easier to understand in several ways. First, add a comment (one or more lines beginning with # followed by a space) at the top of the file to explain what the code in that file does. This will make it easier for you to know that you’ve found the right file if you are looking for it in a few weeks when you’ve forgotten (which you will) what file contains what code.\n# This code produces a density map of bike thefts in Vancouver in 2020\nComments should usually start with a capital letter and follow normal English rules of punctuation, spacing, etc.\n\n4.2.2 Letting your code breathe\nUnless your code is very simple, it will probably consist of several separate tasks that are completed one after another. For example your code might download some data, wrangle it and then plot it on a map. In that case, it can be useful to split your code up into blocks by leaving a blank line between the code needed for each task. For example, if we take the code:\nlibrary(lubridate)\nlibrary(tidyverse)\ncrimes <- read_csv(\"crime_data.csv\")\ncrimes <- janitor::clean_names(crimes)\nburglaries <- filter(crimes, type == \"burglary\")\nburglaries <- mutate(burglaries, month = month(date_time))\nggplot() + \n  geom_point(aes(x = lon, y = lat, colour = month)) +\n  theme_void()\nit becomes easier to read if we split the code up into four tasks: loading the necessary packages, reading the data, wrangling the data and plotting the data.\nlibrary(lubridate)\nlibrary(tidyverse)\n\ncrimes <- read_csv(\"crime_data.csv\")\n\ncrimes <- janitor::clean_names(crimes)\nburglaries <- filter(crimes, type == \"burglary\")\nburglaries <- mutate(burglaries, month = month(date_time))\n\nggplot() + \n  geom_point(aes(x = lon, y = lat, colour = month)) +\n  theme_void()\nSince data wrangling involves several steps and each function uses the result of the previous step, we could use the pipe operator |> to make that code a bit cleaner:\nlibrary(lubridate)\nlibrary(tidyverse)\n\ncrimes <- read_csv(\"crime_data.csv\")\n\nburglaries <- crimes |> \n  janitor::clean_names() |> \n  filter(type == \"burglary\") |> \n  mutate(month = month(date_time))\n\nggplot() + \n  geom_point(aes(x = lon, y = lat, colour = month)) +\n  theme_void()\n\n4.2.3 Header comments\nIf your code includes very long tasks (e.g. where the code takes up more than a full screen on your computer), you might want to use header comments to divide your code into sections. You can do this by writing a comment that is followed by four of more hyphens (----):\n# Load data ----\n\n… some code …\n\n\n# Wrangle data ----\n\n… some code …\n\n\n# Plot data ----\n\n… some code …\nRStudio will recognise lines that end in four or more hyphens as being headings, and will create a table of contents for your code. You can use this to move between headings by clicking on the Jump To menu at the bottom of the Source panel in RStudio:\n\n\n\n\n\n\n\n\nIn general, writing code that is readable is more important than writing the shortest code possible, so don’t be afraid to let your code breathe by using space to separate your code into meaningful chunks."
  },
  {
    "objectID": "04_code_with_style/index.html#naming-objects",
    "href": "04_code_with_style/index.html#naming-objects",
    "title": "\n4  Code with style\n",
    "section": "\n4.3 Naming objects",
    "text": "4.3 Naming objects\nR objects can have any name you like, as long as the name starts with a letter and contains only letters, numbers, dots (.) and underscores (_). That said, you will find coding easier if you follow a few conventions.\n\nUse only lower-case letters in the names of objects, which avoids you having to remember whether a particular letter was upper- or lower-case.\nUse snake case (object_name, with words separated by underscores) for object names rather than camel case (objectName) or kebab case (object-name).\nDon’t use dots in object names.\nDon’t give objects the same names as R functions, because re-using function names makes reading your code more difficult.\n\n\n\n\n\n\n\n\n\nJust as crime_data_atlanta_2020.csv is a more-useful file name than data_file_23.csv, you will find it easier to read your code if you give your objects meaningful names. So when you load data into R (e.g. with read_csv()) don’t just call it data (not least because there is a function named data()) but instead give it a name like atlanta_crimes if it contains (for example) crime data from Atlanta.\n\n\n\n\nStats Illustrations by Allison Horst licensed under the Creative Commons Attribution licence."
  },
  {
    "objectID": "04_code_with_style/index.html#spacing",
    "href": "04_code_with_style/index.html#spacing",
    "title": "\n4  Code with style\n",
    "section": "\n4.4 Spacing",
    "text": "4.4 Spacing\nSpacing out code makes it much easier to read, but (just as in any language) code is easiest to read if spaces are used where people expect them to be by convention.\nMostly in R, we use spaces where we would expect them in English: after commas but not before, outside parentheses but not inside, etc.\n# Good\nread_csv(\"crime_data.csv\", skip = 4)\n\n# Bad\nread_csv(\"crime_data.csv\",skip = 4)\nread_csv(\"crime_data.csv\" ,skip = 4)\nread_csv(\"crime_data.csv\" , skip = 4)\nDon’t put spaces inside parentheses, or between the names of functions and the parentheses:\n# Good\nmean(x, na.rm = TRUE)\n\n# Bad\nmean (x, na.rm = TRUE)\nmean( x, na.rm = TRUE )\nDo put spaces around most operators (==, +, -, <-, etc.), including either side of = when specifying the values of function arguments:\n# Good\nheight <- (feet * 12) + inches\nmean(x, na.rm = TRUE)\n\n# Bad\nheight<-feet*12+inches\nmean(x, na.rm=TRUE)\nAlthough there are some operators that shouldn’t have spaces around them: $, @, [, [[, ^, : and ?."
  },
  {
    "objectID": "04_code_with_style/index.html#functions",
    "href": "04_code_with_style/index.html#functions",
    "title": "\n4  Code with style\n",
    "section": "\n4.5 Functions",
    "text": "4.5 Functions\nWe’ve now got used to calling functions to do things in R, like calling read_csv() to load data from a CSV file or filter() to choose certain rows from a dataset. We know that we can change the behaviour of functions by using arguments. For example, we can wrap a a string of text into shorter lines using the str_wrap() function from the stringr package. str_wrap() needs two arguments: the text to be wrapped into multiple lines and the maximum length of a line of text before the next word is wrapped onto a new line. These arguments are called string and width, so we can call the function as:\nstr_wrap(string = \"some text to be wrapped\", width = 10)\nThe string argument provides the data that the str_wrap() function will work on, while the width argument provides the details of how that work should be done. Since the data argument to a function is typically required (the function makes no sense without it) and is often the first argument, you can omit the name of data arguments to functions. For all other arguments, it is best to give the argument name. So to use str_wrap(), you can write:\nstr_wrap(\"some text to be wrapped\", width = 10)\nIn general, you should keep lines of code to a maximum of 80 characters long, since they can easily fit on most screens and are easy to read. When calling a function, put all of the parameters on a single line if they will fit into 80 characters or less:\ndo_something_simple(\"something\", with = \"only\", short, \"arguments\")\nBut if the function call is longer than 80 characters, use one line each for the function name, each argument, and the closing ), with the arguments indented by two spaces. This makes the code much easier to read.\n# Good\ndo_something_very_complicated(\n  something = \"that\",\n  requires = many,\n  arguments = \"some of which may be long\"\n)\n\n# Bad\ndo_something_very_complicated(\"that\", requires, many, arguments,\n                              \"some of which may be long\"\n                              )\ndo_something_very_complicated(\n  something = \"that\",\n  requires = many,\n  arguments = \"some of which may be long\")\nWhen combining multiple functions using the pipe operator (|>), put each function on a single line, with all but the first line indented by two spaces:\na_function() |> \n  another_function() |> \n  and_a_third_function()\nOnce you learn about other types of R code you will need to know how best to style it, but we will learn about those when we need to."
  },
  {
    "objectID": "04_code_with_style/index.html#styling-your-code-automatically",
    "href": "04_code_with_style/index.html#styling-your-code-automatically",
    "title": "\n4  Code with style\n",
    "section": "\n4.6 Styling your code automatically",
    "text": "4.6 Styling your code automatically\nYou can get help on styling your R code using the styler package, which can automatically format your code for you. After you install the styler package with the code install.packages(\"styler\"), you can style your code by:\n\nselecting the code you want to style,\nopening the Addins menu at the top of the Source panel in RStudio,\nclicking ‘Style selection’ in the ‘Styler’ section of the list of addins.\n\n\n\n\n\n\n\n\n\nRStudio will also try to help style your code as you type, for example by automatically indenting lines."
  },
  {
    "objectID": "04_code_with_style/index.html#in-summary",
    "href": "04_code_with_style/index.html#in-summary",
    "title": "\n4  Code with style\n",
    "section": "\n4.7 In summary",
    "text": "4.7 In summary\n\nYou now know how to write your R code so that it is easy to read, which makes it much easier to understand. Understanding code when you read it is important because it allows you to work out what the code is trying to achieve and because it makes it much easier to find and fix problems when your code is not behaving as you want it to.\n\n\nWriting readable, understandable code is important. To find out more about this, read some of these articles:\n\n\nWhy coding style matters by Nicholas Zakas.\n\nThe tidyverse style guide by Hadley Wickham, which is the basis for the rules outlined in this tutorial.\n\n\n\nThe tidyverse style guide licensed under the Creative Commons Attribution-ShareAlike licence."
  },
  {
    "objectID": "05_your_second_crime_map/index.html#introduction",
    "href": "05_your_second_crime_map/index.html#introduction",
    "title": "\n5  Your second crime map\n",
    "section": "\n5.1 Introduction",
    "text": "5.1 Introduction\nWe’ve already produced a simple map of crime in a neighbourhood, but we skipped over a lot of the details of how to do it. In this tutorial we will make another crime map, this time focusing more on each step in the process.\nIn this tutorial we will make a map of bicycle thefts in Vancouver in 2020. At the end of this tutorial, the final map will look something like this. We will then improve it further in a future tutorial on what makes a good map.\n\n\n\n\n\n\n\n\n\n\n\nBefore we get into the detail of how to make this map, watch this video that goes over the main points of the code we will use:"
  },
  {
    "objectID": "05_your_second_crime_map/index.html#handling-spatial-data",
    "href": "05_your_second_crime_map/index.html#handling-spatial-data",
    "title": "\n5  Your second crime map\n",
    "section": "\n5.2 Handling spatial data",
    "text": "5.2 Handling spatial data\nMaps are visual representations of spatial data. Spatial data is special because each row in the data is associated with some geographic feature such as a building, a street or the boundary of a neighbourhood. This adds some quirks that we have to understand to work with spatial data successfully.\n\n5.2.1 Vectors and rasters\nSpatial data can be represented in two general formats: vectors and rasters. Watch this video from the UK mapping agency Ordnance Survey to find out more about the advantages and disadvantages of using each type of data.\n\nAs you saw in the video, generally vector data gives you more control over the appearance of a map, but at the cost of your having to put together each part of the data yourself, choose the styles etc. In contrast, raster data (such as the base map of Atlanta we used in the first map we made for this course) gives you less control but allows you to add pre-formatted data to your map quickly.\n\n5.2.2 Spatial layers\nMaps are made up of multiple layers of spatial data that are styled to represent features of interest and then stacked on top of one another to make the finished map. Watch this video to learn more about spatial layers and the three different types of vector data that we can use in maps.\n\nPoints, lines and polygons in spatial data are known as geometric objects or simply geometries. Spatial data is data that has a geometric object associated with each row.\n\n5.2.3 Representing places on the earth\nSince maps are two-dimensional representations of the curved surface of the earth, map-makers must make choices about how to translate that curved surface onto a flat screen or piece of paper. This video introduces these choices, and explains why they are relevant to crime mapping.\n\nWith any spatial data, we need a way of describing where on the earth a particular point (such as the location of a crime or the corner of a building) is located. Watch this video to find out about the different co-ordinate systems we can use to do this."
  },
  {
    "objectID": "05_your_second_crime_map/index.html#spatial-data-in-r",
    "href": "05_your_second_crime_map/index.html#spatial-data-in-r",
    "title": "\n5  Your second crime map\n",
    "section": "\n5.3 Spatial data in R",
    "text": "5.3 Spatial data in R\n\n\nThere are several packages that handle raster map data from different sources – one of them is the ggspatial package that we have already used to load the base map of Atlanta for the homicide map we made in one of the earlier tutorials.\n\nVector data can be handled in R using functions from the sf package. SF stands for ‘simple features’, which is a standard for storing spatial data. SF objects are data frames that have a special column to hold the geometry (point, line or polygon) associated with each row in the data. SF objects also understand what co-ordinate system the geometry are described in. This means SF objects can be transformed between co-ordinate systems and combined together in layers on a map.\nThere a lots of functions in the sf package for handling spatial data. Almost all of these functions begin with the letters st_ (e.g. st_read()), which makes it easy to identify that those functions are designed to be used on SF objects.\n\n5.3.1 Reading spatial data\nThe special features of spatial data – needing to store geometries, details of the projection used etc. – mean that spatial data is often stored in special file formats. There are lots of spatial-data formats, but fortunately almost all of them can be read by the st_read() function. This means we do not need to learn a different function for each spatial-data format.\nWhile datasets with line or polygon geometries must almost always be stored in specific spatial-data formats, point data can also be stored in common data formats such as Excel and CSV files. The data for this tutorial is provided by the Vancouver Police Department in a CSV file (gzipped to reduce the file size). The file is located at:\nhttps://mpjashby.github.io/crimemappingdata/vancouver_thefts.csv.gz\n\nThinking back to the tutorial on data wrangling, what R code is needed to load this data into a tibble called thefts? Type the R code into the box below and click Run Code to see the result. If you need help, click the Solution button, but try to remember the code (or look up your notes from the data-wrangling tutorial) before revealing the solution – you’ll learn a lot more that way. Remember to load any necessary packages using the library() function!\n\n\n\n\n# Since the data are stored in a regular CSV file, we can use the `read_csv()`\n# function from the readr package to read the file, and the assignment operator\n# `<-` to store the data in the object `thefts`. `read_csv()` can read directly\n# from a URL, so there is no need to download the data first.\nlibrary(tidyverse)\n\nthefts <- read_csv(\"https://mpjashby.github.io/crimemappingdata/vancouver_thefts.csv.gz\")\n\nRows: 21918 Columns: 10\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (3): TYPE, HUNDRED_BLOCK, NEIGHBOURHOOD\ndbl (7): YEAR, MONTH, DAY, HOUR, MINUTE, X, Y\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nNow that you have stored the data in the thefts object, what code is needed to view the first few rows of data? Type the code into the box and click Run Code to check the result.\n\n\n\n\nhead(thefts)\n\n# A tibble: 6 × 10\n  TYPE               YEAR MONTH   DAY  HOUR MINUTE HUNDR…¹ NEIGH…²      X      Y\n  <chr>             <dbl> <dbl> <dbl> <dbl>  <dbl> <chr>   <chr>    <dbl>  <dbl>\n1 Other Theft        2020     1     1     0      0 11XX B… West E… 4.90e5 5.46e6\n2 Other Theft        2020     1     1     0      0 13XX W… Marpole 4.90e5 5.45e6\n3 Other Theft        2020     1     1     0      1 18XX E… Grandv… 4.95e5 5.46e6\n4 Other Theft        2020     1     1     0      0 2X ALE… Centra… 4.92e5 5.46e6\n5 Theft from Vehic…  2020     1     1     0      0 11XX S… Hastin… 4.98e5 5.46e6\n6 Theft from Vehic…  2020     1     1     0     10 14XX L… Kitsil… 4.89e5 5.46e6\n# … with abbreviated variable names ¹​HUNDRED_BLOCK, ²​NEIGHBOURHOOD\n\n\n\nThe data consists of 21,918 rows, each representing one theft. Before we can map this data, we will need to do some minor data wrangling to get it into the format we want.\n\n5.3.2 Cleaning column names\n\n\nIn a previous tutorial I recommended choosing snake_case object names, e.g. calling a data object atlanta_robberies_2020 rather than atlantarobberies2020, AtlantaRobberies2020 or ATLANTAROBBERIES2020. This makes your code easier to read and means you don’t have to remember whether you named a variable using upper- or lower-case letters, since you know that you only use lower case.\nThe same recommendation applies to variable names in datasets, for the same reasons. Doing this makes it much easier to refer to objects and columns in your code, without having to worry about whether a particular letter was upper- or lower-case, or whether it had an accent etc.\n\nAt the moment, the column names in the thefts dataset are upper-case letters. Rather than having to remember this, we can easily convert them to snake case using the clean_names() function from the janitor package. To use a function from a package, we usually first load the package using the library() function. In this case, we probably won’t want to use any other functions from the janitor package, so instead of loading the whole package we will use this one function directly. To do this, we write the function name with the package name added to the front, separated by two colons ::.\n\nthefts <- janitor::clean_names(thefts)\n\nhead(thefts)\n\n# A tibble: 6 × 10\n  type               year month   day  hour minute hundr…¹ neigh…²      x      y\n  <chr>             <dbl> <dbl> <dbl> <dbl>  <dbl> <chr>   <chr>    <dbl>  <dbl>\n1 Other Theft        2020     1     1     0      0 11XX B… West E… 4.90e5 5.46e6\n2 Other Theft        2020     1     1     0      0 13XX W… Marpole 4.90e5 5.45e6\n3 Other Theft        2020     1     1     0      1 18XX E… Grandv… 4.95e5 5.46e6\n4 Other Theft        2020     1     1     0      0 2X ALE… Centra… 4.92e5 5.46e6\n5 Theft from Vehic…  2020     1     1     0      0 11XX S… Hastin… 4.98e5 5.46e6\n6 Theft from Vehic…  2020     1     1     0     10 14XX L… Kitsil… 4.89e5 5.46e6\n# … with abbreviated variable names ¹​hundred_block, ²​neighbourhood\n\n\nYou can see that the data has stayed the same but all the column names are now in snake case. clean_names() would also have replaced any spaces with underscores, tried to separate words in the variable names and cleaned up several other potential problems. For this reason it is common to call janitor::clean_names() straight away after loading a dataset so that you can be confident that the column names will be in the format you expect.\nIf we wanted to use the clean_names() function again, we would have to include the package name and :: each time, so if our code was going to make repeated use of the function then it would probably be easier to load the package using the library() function as we have done in previous tutorials.\n\n5.3.3 Converting our data to an SF object\nAt present, the data in the thefts object is just a regular tibble. We could not use it to make a map because R does not know which columns represent the geometry, or what co-ordinate system the locations are recorded in. We can deal with this by converting the data to an SF object using the st_as_sf() function from the sf package.\nThe data provided by the Vancouver Police use the UTM zone 10N co-ordinate system. UTM is a system for assigning co-ordinates to any location on earth relative to a local origin point for the UTM zone covering that part of the planet. It is therefore similar to the British National Grid that we have already learned about, but for any part of the globe. The ‘N’ at the end of the zone name refers to the northern hemisphere.\n\nIn almost all cases, co-ordinate reference systems only work for the part of the world that they were designed for. So we should not use the UTM zone 10N co-ordinate system to map data outside the area for which it was designed (broadly speaking, the west coast of North America from Los Angeles to Vancouver, and the part of Canada directly north of Vancouver extending as far as the north pole). If we were to use the UTM zone 10N co-ordinate system for data from another part of the world, we would be very likely to get error messages or strange results.\n\n\n\n\n\n\n\n\n\n\n\n\nWe can convert the thefts tibble to an SF object using the st_as_sf() function (remember, all functions in the sf package start with st_, which can sometimes make the function names a little confusing). We specify which columns in the data represent the geometry (in this case, the x and y columns), and what co-ordinate system the data uses.\nCo-ordinate systems can be specified in lots of ways (some very complicated), but the easiest is to specify the EPSG code for the relevant system. An EPSG code is a unique reference number for a particular co-ordinate system that R can look up in a database to get the information needed to display the data on a map. The EPSG code for the UTM zone 10N is EPSG:32610.\n\nlibrary(sf)\n\nthefts_sf <- st_as_sf(thefts, coords = c(\"x\", \"y\"), crs = \"EPSG:32610\")\n\nhead(thefts_sf)\n\nSimple feature collection with 6 features and 8 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 488991.7 ymin: 5450474 xmax: 497938.7 ymax: 5458985\nProjected CRS: WGS 84 / UTM zone 10N\n# A tibble: 6 × 9\n  type   year month   day  hour minute hundr…¹ neigh…²           geometry\n  <chr> <dbl> <dbl> <dbl> <dbl>  <dbl> <chr>   <chr>          <POINT [m]>\n1 Othe…  2020     1     1     0      0 11XX B… West E… (490320.3 5458607)\n2 Othe…  2020     1     1     0      0 13XX W… Marpole (489998.2 5450474)\n3 Othe…  2020     1     1     0      1 18XX E… Grandv… (495141.1 5458406)\n4 Othe…  2020     1     1     0      0 2X ALE… Centra… (492471.3 5458985)\n5 Thef…  2020     1     1     0      0 11XX S… Hastin… (497938.7 5457949)\n6 Thef…  2020     1     1     0     10 14XX L… Kitsil… (488991.7 5457823)\n# … with abbreviated variable names ¹​hundred_block, ²​neighbourhood\n\n\nIf you look at the contents of the thefts_sf object, you’ll see that there is a new column called geometry (you may need to use the ▸ button to see it). This column contains the co-ordinates of each bike theft in a format that R recognises represent locations on the surface of the earth, which means they can be used to make maps.\n\n\n5.3.4 Finding bike thefts in our data\n\nIf you look through the contents of the thefts_sf object, you will see that not all of the rows relate to bicycle thefts. The type column shows that the dataset also includes thefts from vehicles, for example. To choose only those rows containing bicycle thefts, which function from the dplyr package would we used? If you need help, you can think back to the data-wrangling tutorial or have a look at the Data transformation with dplyr cheat sheet.\n\n\n\nType the code needed to choose only the rows of data that relate to bicycle thefts and store it in a new object called bike_thefts. If you get stuck, you can click the Hint buttons to get help, but try to find the answer on your own first!\n\n\n\n\n# Use the `filter()` function to choose particular rows in a dataset. The syntax \n# for `filter()` is `filter(dataset, column_name == \"value\")`\n\n\n# In the code `filter(dataset, column_name == \"value\")`, replace `dataset` with\n# the name of the SF object you have already created from the `thefts` tibble,\n# `column_name` with the name of the column containing the offence type and\n# `value` with the offence type for bicycle theft.\n\n\n# The correct code to store only bicycle thefts in a new object is:\nbike_thefts <- filter(thefts_sf, type == \"Theft of Bicycle\")\n\nOur data is now ready for us to make our crime map!\n\n\nIf you look through the contents of the thefts_sf object, you will see that not all of the rows relate to bicycle thefts. The type column shows that the dataset also includes thefts from vehicles, for example. To choose only those rows containing bicycle thefts, we can use the filter() function from the dplyr package.\n\nbike_thefts <- filter(thefts_sf, type == \"Theft of Bicycle\")\n\n\n\nStats Illustrations by Allison Horst licensed under the Creative Commons Attribution licence."
  },
  {
    "objectID": "05_your_second_crime_map/index.html#producing-maps-in-r",
    "href": "05_your_second_crime_map/index.html#producing-maps-in-r",
    "title": "\n5  Your second crime map\n",
    "section": "\n5.4 Producing maps in R",
    "text": "5.4 Producing maps in R\nNow that we have our data, we can use it to create a map of bicycle theft in Vancouver. Before we start, let’s take another look at our dataset so that we know which columns contain which data.\n\nType the code needed to view the first few rows of the bike_thefts dataset.\n\n\n\n\n\nhead(bike_thefts)\n\nSimple feature collection with 6 features and 8 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 488371.3 ymin: 5452696 xmax: 494295.1 ymax: 5458232\nProjected CRS: WGS 84 / UTM zone 10N\n# A tibble: 6 × 9\n  type   year month   day  hour minute hundr…¹ neigh…²           geometry\n  <chr> <dbl> <dbl> <dbl> <dbl>  <dbl> <chr>   <chr>          <POINT [m]>\n1 Thef…  2020     1     1     0      0 12XX V… Strath… (494295.1 5458232)\n2 Thef…  2020     1     1     0      0 20XX M… Kitsil… (489053.7 5457225)\n3 Thef…  2020     1     1    13      0 7XX PA… Centra… (492005.9 5458174)\n4 Thef…  2020     1     1    20      0 53XX V… Arbutu… (488371.3 5453909)\n5 Thef…  2020     1     3    11     55 65XX A… Kerris… (489232.5 5452696)\n6 Thef…  2020     1     3    14      0 4XX E … Mount … (493137.8 5456541)\n# … with abbreviated variable names ¹​hundred_block, ²​neighbourhood\n\n\n\n5.4.1 Introduction to ggplot2\n\n\nA map is a specialised type of chart, so we can make maps using the ggplot2 package that is widely used to create other types of chart in R. ggplot2 charts are made up of layers, so they’re well suited to making maps.\n\nThe most-basic map that we can make simply plots the locations of crimes with no context. This almost never makes a good crime map, but we can use this type of map as the foundation around which we can build a better map.\nggplot2 plots work by building up a chart using different functions, each of which adds or modifies some part of the chart. Building a plot starts with calling the ggplot() function, with each subsequent function being added to the plot definition using the + operator. Note that while the package is called ggplot2, the function in that package used to create plots is called ggplot(), not ggplot2().\nThe most-important of the ggplot2 functions are those beginning with geom_, which add graphical elements to the chart. If you want to add a layer to your chart showing a scatter plot, you use the geom_point() function, while if you want to make a line chart you use geom_line().\n\n\n\n\n\n\n\n\nThere are lots of geom_ functions available for representing data on charts in different ways. For maps, the SF package includes the geom_sf() function that is designed to add spatial data (in the form of an SF object such as our bike_thefts data) to a chart, making it into a map. So to simply plot the points in our bicycle-theft data, we can use the code:\n\nlibrary(ggplot2)\n\nggplot() +\n  geom_sf(data = bike_thefts)\n\n\n\n\nBy convention, each function that we add to ggplot() to change the appearance of our map goes on a new line (this makes the code easier to read) and all but the first line is indented by two spaces. RStudio does this indenting automatically if the previous line ends with a + symbol, since RStudio then understands that there is more code to come on the next line.\nThis map shows the bike-theft data, but it is obviously not a very useful map. Fortunately, we can use the features of the ggplot2 package to build on this basic map.\n\n5.4.2 Controlling aesthetics\nWe can change the appearance of the points by specifying various arguments to the geom_sf() function. These arguments are called aesthetics, because they control the aesthetic appearance of the geometric objects (points, lines etc.) produced by a geom_ function. There are lots of aesthetics, but some of the most common are:\n\n\ncolour controls the colour of points and lines (for polygons, it controls the colour of the border around the polygon edge) – you can also use the spelling color for this argument and get an identical result,\n\nfill controls the colour used to fill polygons or points that use a shape capable of having different colours in the centre and around the edge (fill has no meaning for lines),\n\nshape controls the shape (circle, triangle, square etc.) of points (it has no meaning for lines or polygons),\n\nsize controls the size of points and text,\n\nlinewidth controls the width of lines, including the borders around the edges of polygons, and\n\nalpha controls the transparency of a layer (alpha = 1 equals fully opaque, alpha = 0 means fully transparent).\n\ncolour and fill can be specified using any one of 657 R colour names or using a hexidecimal (‘hex’) colour code. Values of size don’t relate to any unit of size (e.g. millimetres or points), so it’s easiest to set the size of points and text by trial and error.\nThere are 25 built-in shapes for points in R (shape 16 is the default):\n\nggplot(data.frame(shape = 0:24), aes(0, 0, shape = shape)) +\n  geom_point(aes(colour = \"darkblue\", fill = \"lightblue\", shape = shape), size = 5) +\n  scale_colour_identity(aesthetics = c(\"colour\", \"fill\")) +\n  scale_shape_identity() +\n  labs(\n    title = \"Available shapes in R\", \n    subtitle = str_wrap(\"Shapes 0 to 20 use the colour aesthetic. Shapes 21 to 24 have separate values of the colour and fill aesthetics\", 90)\n  ) +\n  facet_wrap(vars(shape)) +\n  theme_void()\n\n\n\n\n\n\n\n\nChange the code below so that the points on our map are red squares instead of black circles (red is one of the 657 R colour names) and click Run Code to see the result. Use the hints if you need help, but try to work it out on your own first.\n\nggplot() +\n  geom_sf(data = bike_thefts)\n\n\n\n\n\n# Use the `shape` aesthetic to change the points to squares and the `colour`\n# aesthetic to change the point colour to red\n\n\n# Add the arguments `shape = 15` and `colour = \"red\"` to the `geom_sf()`\n# function, remembering that arguments are separated by commas\n\n\n\nWe can change the black circles on the map above into red squares using the colour and shape arguments to the geom_sf() function.\n\n\nggplot() +\n  geom_sf(data = bike_thefts, colour = \"red\", shape = 15)\n\n\n\n\nAs we have said, this basic map is not very useful. We can see that there seems to be a cluster of bike thefts towards the top (north) of the map, but it is difficult to see how important this cluster is because so many of the points overlap. Overlapping points are a particular problem in maps, because if there are multiple crimes at the same location then the points representing those crimes will be exactly on top of one another and it will be impossible to see whether there is one crime at a particular location or 100.\nOne way to deal with this problem is to make the points semi-transparent so that overlapping points appear darker. This often works better if we also make the points slightly smaller at the same time. You can use the alpha and size aesthetics to make the points smaller (relative to the default for points of size = 1) and semi-transparent.\n\nKeep changing the values of the two aesthetics until you are happy that the map makes it as easy as possible to see the distribution of bike thefts in Vancouver.\n\nggplot() +\n  geom_sf(data = bike_thefts)\n\n\n\n\n\n\n# Which values you've chosen will depend on your personal aesthetic preferences,\n# but these values produce a map that makes it easier to see the distribution of\n# points\nggplot() +\n  geom_sf(data = bike_thefts, size = 0.75, alpha = 0.1)\n\n\n\n\nMaking the points semi-transparent goes some way to making it easier to see where bike theft is most common in Vancouver, but the pattern is not clear and it is not possible to tell which darker points represent a handful of crimes at the same location and which represent hundreds of crimes at the same location. To make our map useful, we need to use a different technique."
  },
  {
    "objectID": "05_your_second_crime_map/index.html#mapping-crime-density",
    "href": "05_your_second_crime_map/index.html#mapping-crime-density",
    "title": "\n5  Your second crime map\n",
    "section": "\n5.5 Mapping crime density",
    "text": "5.5 Mapping crime density\n\nUnless we want to produce a map of only a very small number of crimes (like the Atlanta downtown homicides map we produced in a previous tutorial), it is unlikely that a point map will be very useful.\nIn fact, if you find yourself making map with each crime represented by a separate point, you should probably stop and ask yourself if that is really the best way to achieve your goal.\n\nA better way to show where crime is concentrated on a map is to work out the density of crime in each area and then map that density. By density in this context, we mean the relative concentration of points in each part of the area we are studying, i.e. how many points (representing bike thefts) are there in each part of the map relative to all the other areas of the map.\nTo estimate the density of points in different areas of the map, R uses a technique called kernel density estimation (KDE). To do this, R must:\n\ndivide the map into a grid of cells, each the same size,\ncount the number of points in each cell,\nfor each cell, count the number of points in nearby cells, but give less weight to (i.e. systematically undercount) those cells that are further away,\nfor each cell, total up the count of points in that cell and the (weighted) count of points in nearby cells – this is the estimate of the density of points in that cell.\n\nThis procedure has the effect of producing a smooth surface representing crime density.\n\n\n\n\n\n\n\n\n\n\n\nWe don’t need to worry at this point about the details of how the counts are used to estimate the density of crimes – we will return to this in a later tutorial.\n\nThere are several ways we can make density maps in R. In this course we will use the sfhotspot package because it makes reasonable decisions about how or density maps should look, while still giving us control over their appearance if we want it. sfhotspot also has other useful functions that we will use in future tutorials.\nTo create a density map using sfhotspot, we first use the hotspot_kde() function to convert a dataset of offence locations to an estimate of the density of offences for each cell in a grid. hotspot_kde() automatically chooses how big the cells in the grid should be (but we can set this ourselves if we want to).\n\nlibrary(sfhotspot)\n\nbike_theft_density <- hotspot_kde(bike_thefts)\n\n\nDone: [--------------------------------------------------------------------] .\nDone: [==========================================--------------------------] .\nDone: [===========================================-------------------------] .\nDone: [============================================------------------------] .\nDone: [=============================================-----------------------] .\nDone: [==============================================----------------------] .\nDone: [===============================================---------------------] .\nDone: [================================================--------------------] .\nDone: [=================================================-------------------] .\nDone: [==================================================------------------] .\nDone: [===================================================-----------------] .\nDone: [====================================================----------------] .\nDone: [=====================================================---------------] .\nDone: [======================================================--------------] .\nDone: [=======================================================-------------] .\nDone: [========================================================------------] .\nDone: [=========================================================-----------] .\nDone: [==========================================================----------] .\nDone: [===========================================================---------] .\nDone: [============================================================--------] .\nDone: [=============================================================-------] .\nDone: [==============================================================------] .\nDone: [===============================================================-----] .\nDone: [================================================================----] .\nDone: [=================================================================---] .\nDone: [==================================================================--] .\nDone: [===================================================================-] .\nDone: [====================================================================] .\n                                                                              \n\nhead(bike_theft_density)\n\nSimple feature collection with 6 features and 2 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 490447.2 ymin: 5450107 xmax: 491647.2 ymax: 5450307\nProjected CRS: WGS 84 / UTM zone 10N\n# A tibble: 6 × 3\n      n   kde                                                           geometry\n  <dbl> <dbl>                                                      <POLYGON [m]>\n1     0 11.1  ((490447.2 5450107, 490447.2 5450307, 490647.2 5450307, 490647.2 …\n2     0 11.1  ((490647.2 5450107, 490647.2 5450307, 490847.2 5450307, 490847.2 …\n3     0 11.0  ((490847.2 5450107, 490847.2 5450307, 491047.2 5450307, 491047.2 …\n4     0 10.8  ((491047.2 5450107, 491047.2 5450307, 491247.2 5450307, 491247.2 …\n5     0 10.5  ((491247.2 5450107, 491247.2 5450307, 491447.2 5450307, 491447.2 …\n6     0  9.96 ((491447.2 5450107, 491447.2 5450307, 491647.2 5450307, 491647.2 …\n\n\nThe bike_theft_density object created by hotspot_kde() contains three columns: n contains the count of bike thefts in each cell, kde contains the estimate of the density of thefts in each cell, and geometry contains the outline of each grid cell.\nSince hotspot_kde() produces an SF object, we can add it to a map using the geom_sf() function. We can also use the fill aesthetic to specify that the fill colour of each grid cell should be determined based on the values of the kde column in the bike_theft_density object.\n\nggplot() +\n  geom_sf(aes(fill = kde), data = bike_theft_density, colour = NA)\n\n\n\n\nWe have already seen that we can set aesthetics such as colour and shape manually, but the aes() function allows us to specify that the values of different aesthetics should be controlled by columns in the data. The aes() function takes as its arguments pairs of values (combined with an = symbol) where the first value is an aesthetic and the second value is the name of a column in the data. For example, to use the colour of points on a map to represent different types of crime that were stored in a column in the data called type, we could use aes(colour = type).\n\nWhen should you specify the values of aesthetics inside aes() and when should you do it outside aes()?\n\nIf you want an aesthetic to have a constant value for all the points, lines or other shapes in a layer, control the aesthetic outside aes(). For example, you could use geom_sf(bike_thefts, colour = \"mediumblue\") to make all the shapes in that layer blue.\nIf you want to vary the appearance of shapes according to values in the data, you should control the aesthetic inside aes(). For example, you could use geom_sf(aes(colour = month), bike_thefts) to vary the colour of shapes in a layer according to values of the month column in the data.\n\naes() must be the first argument in the geom_*() function.\n\nIn this map, instead of seeing each crime as a separate point, we see the density of crime as the filled colour of cells in a grid. By comparing this density map to the point map we produced before, we can see that the density map makes the areas with the highest frequency of thefts easier to identify.\nYou can also see that our map now has a legend, showing that higher densities of bike thefts are shown on the map in dark blue and lower densities are shown in light blue. The exact values shown in the legend are not particularly meaningful, so we can ignore these for now.\n\n5.5.1 Fine-tuning density maps\nWe can control the appearance of KDE maps in several ways. For example, we can vary the number of cells in the grid and the definition of what cells the kernel density estimation process should consider to be ‘nearby’ for the purposes of calculating weighted counts. Cells are considered to be ‘nearby’ to a particular cell if they are closer to that cell than a distance known as the KDE bandwidth.\nBy default, hotspot_kde() chooses the cell size and the bandwidth automatically. The maps below show how changing these defaults changes the appearance of our map (with the legend and axes removed to make the small maps clearer).\n\n\n\n\n\n\n\n\n\n\n\nBy looking at the maps on the right-hand side, you can see that reducing the number of grid cells leads to a map that looks blocky and lacks information. Looking at maps higher up, you can see that increasing the bandwidth relative to the default makes the density surface smoother. The smoother the surface, the less detail we can see about where crime is most concentrated, until on the top row we can see almost no information at all. On the other hand, if we reduce the bandwidth too much (the bottom row of maps) then almost no ‘nearby’ cells are included in the count and so it becomes more difficult to identify patterns.\nIn most cases, you will not need to change the cell size used in calculating the density of points on a map, but if you do then you can do this using the cell_size argument to hotspot_kde().\nAlthough you can set the bandwidth manually using the bandwidth argument to hotspot_kde(), you will almost never want to do this. Instead, you can vary the bandwidth relative to the automatically chosen default bandwidth by using the bandwidth_adjust argument. For example, if you wanted to see more detail in your map by using a smaller bandwidth, you could use adjust = 0.75 or adjust = 3/4 to set the bandwidth to be three-quarters of the default bandwidth.\n\nChange the code below so that the bandwidth is half of the default bandwidth.\n\nhotspot_kde(bike_thefts) |> \n  ggplot() +\n  geom_sf(aes(fill = kde), colour = NA)\n\nCell size set to 200 metres automatically\nBandwidth set to 1,856 metres automatically based on rule of thumb\n\n\n\nDone: [--------------------------------------------------------------------] .\nDone: [==========================================--------------------------] .\nDone: [===========================================-------------------------] .\nDone: [============================================------------------------] .\nDone: [=============================================-----------------------] .\nDone: [==============================================----------------------] .\nDone: [===============================================---------------------] .\nDone: [================================================--------------------] .\nDone: [=================================================-------------------] .\nDone: [==================================================------------------] .\nDone: [===================================================-----------------] .\nDone: [====================================================----------------] .\nDone: [=====================================================---------------] .\nDone: [======================================================--------------] .\nDone: [=======================================================-------------] .\nDone: [========================================================------------] .\nDone: [=========================================================-----------] .\nDone: [==========================================================----------] .\nDone: [===========================================================---------] .\nDone: [============================================================--------] .\nDone: [=============================================================-------] .\nDone: [==============================================================------] .\nDone: [===============================================================-----] .\nDone: [================================================================----] .\nDone: [=================================================================---] .\nDone: [==================================================================--] .\nDone: [===================================================================-] .\nDone: [====================================================================] .\n                                                                              \n\n\n\n\n\n\n\n# You can set either `bandwidth_adjust = 1/2` or `bandwidth_adjust = 0.5` to \n# get the same result\nhotspot_kde(bike_thefts, bandwidth_adjust = 2/3) |> \n  ggplot() +\n  geom_sf(aes(fill = kde), colour = NA)\n\nCell size set to 200 metres automatically\nBandwidth set to 1,856 metres automatically based on rule of thumb\n\n\n\nDone: [--------------------------------------------------------------------] .\nDone: [=========================================---------------------------] .\nDone: [==========================================--------------------------] .\nDone: [===========================================-------------------------] .\nDone: [============================================------------------------] .\nDone: [=============================================-----------------------] .\nDone: [==============================================----------------------] .\nDone: [===============================================---------------------] .\nDone: [================================================--------------------] .\nDone: [=================================================-------------------] .\nDone: [==================================================------------------] .\nDone: [===================================================-----------------] .\nDone: [====================================================----------------] .\nDone: [=====================================================---------------] .\nDone: [======================================================--------------] .\nDone: [=======================================================-------------] .\nDone: [========================================================------------] .\nDone: [=========================================================-----------] .\nDone: [==========================================================----------] .\nDone: [===========================================================---------] .\nDone: [============================================================--------] .\nDone: [=============================================================-------] .\nDone: [==============================================================------] .\nDone: [===============================================================-----] .\nDone: [================================================================----] .\nDone: [=================================================================---] .\nDone: [==================================================================--] .\nDone: [===================================================================-] .\nDone: [====================================================================] .\n                                                                              \n\n\n\n\n\nComparing this map to the first density map we produced, we can see that the slightly smaller bandwidth means we can see slightly more detail in the patterns of bike thefts.\n\nI recommend using a slightly smaller bandwidth than the default, so that your maps show a bit more detail. Try setting bandwidth_adjust = 0.5 whenever you produce a density layer using hotspot_kde(), but remember to look at the map to see if you are happy with the result.\n\nThe final way we can control the appearance of our density layer is to change the colour scheme used to represent density. To do this, we can use another type of ggplot2 function: scales. There are lots of scales available, but the scale_fill_distiller() function produces several different colour scales that are specifically designed to be effective on maps.\nAll the available colour schemes are on the Color Brewer website. Colour schemes can be divided into three types:\n\n\nsequential colour schemes are useful for showing values from low to high,\n\ndiverging colour schemes are useful for showing values relative to a meaningful central point, and\n\nqualitative colour schemes are useful for showing separate categories that can appear in any order and still be meaningful.\n\nIn crime mapping we’re usually interested in showing how crime varies from low to high, so we need to use a sequential colour palette. There are 18 sequential colour schemes (or palettes) available in scales_fill_distiller(), each with a name:\n\n\n\n\n\n\nIt is important to only use the right type of colour scale in the right circumstances, since using the wrong type of scale could end up misleading people reading your map. For example, a diverging colour scale gives the strong impression that the central point in the scale is meaningful.\nIn some circumstances this might be useful, for example if you wanted to show areas in which crime had increased in shades of one colour and areas in which crime had decreased in shades of another colour. In that case, a diverging scale would be appropriate because the central point represents something meaningful: no change in crime. If the central point is not meaningful, use a sequential colour scheme instead.\nIf you want to represent a categorical variable, you should use a categorical colour scale unless the categories have a natural order. For example, if you wanted to show ethnic groups on a map you would use a categorical colour scale since there is no one order of ethnic groups that is any more meaningful than any other. If you wanted to represent days of the week with colour, then you might want to use a sequential colour scheme since the days of the week have a meaningful order.\n\nBy default, scale_fill_distiller() sets the lowest values to have the darkest colour. This often does not work well, but we can change this by setting the argument direction = 1. I recommend doing this in all cases.\n\n\n\n\n\n\n\n\nYou can think of all the functions that we can add to ggplot() as being like a stack of pancakes, with each new function being placed on the top of the stack. To change the colour of our map, we just add scale_fill_distiller() to the existing stack.\n\nggplot() +\n  geom_sf(aes(fill = kde), data = bike_theft_density, colour = NA) +\n  scale_fill_distiller(palette = \"Oranges\", direction = 1)\n\n\n\n\nTry changing the code in this box to use one of the other Color Brewer palettes above. Which palette do you like best?\n\n5.5.2 Check your understanding\nAnswer the following questions to check your understanding of what we’ve learned in this section of this tutorial. If you get a question wrong, you can keep trying until you get the right answer.\n\n\n\nYou now have the skills to make a density map of bike theft in Vancouver. Using your notes and the code in this section, complete the code needed to generate a map with:\n\nkernel density calculated using the default number of grid cells and half the default band width, and\nthe density shown using the Color Brewer ‘Purples’ colour scheme.\n\nRemember the data are stored in an object called bike_thefts.\n\n\n\n\nbike_theft_density_bw_half <- hotspot_kde(bike_thefts, bandwidth_adjust = 0.5)\n\nCell size set to 200 metres automatically\nBandwidth set to 1,856 metres automatically based on rule of thumb\n\n\n\nDone: [--------------------------------------------------------------------] .\nDone: [=========================================---------------------------] .\nDone: [==========================================--------------------------] .\nDone: [===========================================-------------------------] .\nDone: [============================================------------------------] .\nDone: [=============================================-----------------------] .\nDone: [==============================================----------------------] .\nDone: [===============================================---------------------] .\nDone: [================================================--------------------] .\nDone: [=================================================-------------------] .\nDone: [==================================================------------------] .\nDone: [===================================================-----------------] .\nDone: [====================================================----------------] .\nDone: [=====================================================---------------] .\nDone: [======================================================--------------] .\nDone: [=======================================================-------------] .\nDone: [========================================================------------] .\nDone: [=========================================================-----------] .\nDone: [==========================================================----------] .\nDone: [===========================================================---------] .\nDone: [============================================================--------] .\nDone: [=============================================================-------] .\nDone: [==============================================================------] .\nDone: [===============================================================-----] .\nDone: [================================================================----] .\nDone: [=================================================================---] .\nDone: [==================================================================--] .\nDone: [===================================================================-] .\nDone: [====================================================================] .\n                                                                              \n\nggplot() +\n  geom_sf(aes(fill = kde), data = bike_theft_density_bw_half, colour = NA) +\n  scale_fill_distiller(palette = \"Purples\", direction = 1)"
  },
  {
    "objectID": "05_your_second_crime_map/index.html#clipping-map-layers",
    "href": "05_your_second_crime_map/index.html#clipping-map-layers",
    "title": "\n5  Your second crime map\n",
    "section": "\n5.6 Clipping map layers",
    "text": "5.6 Clipping map layers\nThere is one limitation of the KDE layer on our map that we need to deal with. The area covered by the KDE layer is determined by the area covered by the point data that we provided to hotspot_kde(). More specifically, hotspot_kde() will calculate density values for every cell in the convex hull around the point data, i.e. for the smallest polygon that contains all the points in the data.\nThis can be a problem in some circumstances, because we do not necessarily have crime data for all the areas within the convex hull of the data, even though KDE values will be calculated for those areas. This could be misleading, since it will look like such areas have low crime density, when in fact we do not know what the density of crime in such areas is.\nFortunately, we can easily deal with this problem by clipping the KDE layer to the boundary of the area for which we have crime data. This means we will only show densities for cells for which we actually have data.\n\n\n\n\n\nA. We only have data on bike thefts from the City of Vancouver, so all the bike thefts in the data necessarily occurred within the city. We do not know what the density of crime outside the city is.\n\n\n\nB. The KDE function only knows the theft locations, not the area in which thefts could have occurred. So the convex hull created by the KDE layer will not necessarily match the area of the data.\n\n\n\n\n\nC. In this case, that means some areas (shaded) will be included in the KDE layer even though they happened outside the area covered by the data, which could be misleading.\n\n\n\nD. To avoid suggesting we know the density of crimes in areas for which we do not have data, we should clip the KDE layer to the boundary of the area for which we have data.\n\n\n\nWe can clip the KDE layer produced by hotspot_kde() to the boundary of the City of Vancouver using the st_intersection() function from the sf package. st_intersection() removes any rows from the dataset provided as the first argument that do not fall within the area covered by the dataset provided as the second argument. If we have the boundary of the City of Vancouver stored in an object called vancouver_boundary and the a KDE layer showing the density of bike thefts stored in the bike_theft_density object, we can use st_intersection() to remove any cells in bike_theft_density that are outside vancouver_boundary.\n\n# Before clipping the KDE layer, we can use the `nrow()` function to check how\n# many cells there are in the KDE grid (each grid cell is one row in the data)\nnrow(bike_theft_density)\n\n[1] 3318\n\n# Clip the density layer\nbike_theft_density_clip <- st_intersection(\n  bike_theft_density, \n  vancouver_boundary\n)\n\nWarning: attribute variables are assumed to be spatially constant throughout\nall geometries\n\n# Now we can check the number of rows in the clipped layer, which will be\n# lower than in the original KDE layer\nnrow(bike_theft_density_clip)\n\n[1] 2927\n\n\nst_intersection() produces a warning message:\nWarning: attribute variables are assumed to be spatially constant throughout all\ngeometries\nAs long as you are simply using st_intersection() to remove parts of the data outside a boundary, you can ignore this message.\nIn most cases, we can use the pipe operator to clip a KDE layer just after we produce it with hotspot_kde():\nbike_theft_density_clip <- bike_thefts |> \n  hotspot_kde(bandwidth_adjust = 0.5) |> \n  st_intersection(vancouver_boundary)\n\nThe st_intersection() function requires that both spatial layers have the same co-ordinate system. If the two layers use different co-ordinate systems, you will need to transform one of the layers using st_transform() so that it uses the same co-ordinate system as the other layer.\nIf you do not know which co-ordinate systems the layers use, you can use the st_crs() function to extract the co-ordinate system from one layer and pass that value as the second argument to st_transform(). For example:\nst_transform(bike_theft_density, crs = st_crs(vancouver_boundary))"
  },
  {
    "objectID": "05_your_second_crime_map/index.html#adding-a-base-map",
    "href": "05_your_second_crime_map/index.html#adding-a-base-map",
    "title": "\n5  Your second crime map\n",
    "section": "\n5.7 Adding a base map",
    "text": "5.7 Adding a base map\nThe density map we have made is much more effective than a point map at allowing us to identify where the highest number of bike thefts in Vancouver occur. However, it’s still quite difficult to know where those places are, because we cannot easily work out where in the city these places are. We can make this much easier by adding a base map underneath the density layer.\nWe can add a base map using annotation_map_tile() function from the ggspatial package. We can add annotation_map_tile() to a ggplot() stack in the same way that we would add geom_*() functions.\n\nggplot() +\n  # We add the base map to the `ggplot()` stack *before* the density layer \n  # because we want the base map to appear below the density layer\n  annotation_map_tile(zoomin = 0) +\n  # When adding a base map, it is useful to make any filled layers (such as the\n  # density layer) semi-transparent so that readers can see the base map\n  geom_sf(\n    aes(fill = kde), \n    data = bike_theft_density_bw_half, \n    alpha = 0.67, \n    colour = NA\n  ) +\n  scale_fill_distiller(palette = \"Purples\", direction = 1) +\n  theme_void() +\n  # Suppress the map legend -- we will learn more about these next lines of code\n  # in a future tutorial\n  theme_void() + \n  theme(legend.position = \"none\")\n\nZoom: 12\n\n\n\n\n\nThe base maps returned by annotation_map_tile() are available at many different zoom levels, from level 1 that is useful for mapping the whole world in one map, to level 20 that can be used to map a single building. By default, annotation_map_tile() downloads tiles with slightly less detail than we might want, but we can fix this by using the argument zoomin = 0. We could also set a specific zoom level using the zoom argument.\nFor example, these maps show the same area around the UCL Jill Dando Institute with base maps at different zoom levels.\n\n\n\n\n\n\n\n\n\n\n\nChoosing the right zoom level is a matter of balancing the level of detail in the map and the clarity of the image. In the maps above, zoom levels less than 12 tend to have pixelated images because they do not contain enough detail, while zoom levels greater than 12 contain too much detail and so the information is hard to read. But if this map covered a smaller or larger area, a different zoom level might be better. In general, setting zoomin = 0 and not setting any value for the zoom argument – so that annotation_map_tile() chooses the zoom level automatically – will produce an acceptable map.\nannotation_map_tile() also gives us access to several different types of base map. The default style (seen in the maps above) is called ‘osm’ because it is the default style used by Open Street Map, the organisation that provides the map data. We can specify which style of base map we want using the type argument to annotation_map_tile().\n\n\n\n\n\n\n\n\n\n\n\nYou may want to experiment with different base map styles by using the type argument to annotation_map_tile() in the map above, e.g. using annotation_map_tile(type = \"cartolight\", zoomin = 0)."
  },
  {
    "objectID": "05_your_second_crime_map/index.html#adding-more-layers",
    "href": "05_your_second_crime_map/index.html#adding-more-layers",
    "title": "\n5  Your second crime map\n",
    "section": "\n5.8 Adding more layers",
    "text": "5.8 Adding more layers\n\nAdding a base map underneath our density layer makes it much easier to understand where the highest densities of bike theft in Vancouver are. But our map could make it easier still to see where clusters of thefts occur. We could, for example, add the names of different neighbourhoods in the city, and show the city limits so that we can tell which areas have no crime because crimes in those areas are not included in our data.\nIn an earlier section I suggested we can think of ggplot charts, including maps, as being like stacks of pancakes – each function we use to amend the appearance of our chart is added to the top of the stack. So to add another layer to our map, we just add another geom_ function to our plot.\nThe City of Vancouver provides boundary data for city neighbourhoods on its website in GeoJSON format. This is a spatial data format, so it can be read by st_read(). We can then add the layer to our map using geom_sf() in the same way as for the point layer in our first map.\n\n# This is a version of the data saved from the City of Vancouver website, so \n# that this tutorial continues to work if the original data is ever removed\nnbhds <- st_read(\"https://mpjashby.github.io/crimemappingdata/vancouver_neighbourhoods.geojson\")\n\nReading layer `vancouver_neighbourhoods' from data source \n  `https://mpjashby.github.io/crimemappingdata/vancouver_neighbourhoods.geojson' \n  using driver `GeoJSON'\nSimple feature collection with 22 features and 3 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: -123.2248 ymin: 49.19894 xmax: -123.0232 ymax: 49.29581\nGeodetic CRS:  WGS 84\n\n\n\nAt the same time, we can also add labels to the plot at the centre of each neighbourhood using the geom_sf_label() function. We use the aes() function to specify which column in the nbhds dataset we want to use for the label text. Normally, we would use the code aes(label = name) to do this, but in this case we want to wrap the labels so that they don’t overlap adjacent neighbourhoods, To do this we can use the str_wrap() from the stringr package (part of the tidyverse), so that our code instead becomes:\naes(label = str_wrap(name, width = 10))\nThe geom_sf_label() function in the map below uses quite a lot of arguments to control the appearance of the labels:\n\n\nalpha = 0.5 to make the label background semi-transparent so that we can see the density layer underneath it,\n\ncolour = \"seagreen3\" to slightly reduce the prominence of the label text to avoid distracting attention from the density layer,\n\nlineheight = 1 to reduce the gap between lines in each label,\n\nsize = 2.5 to slightly reduce the size of the label text,\n\nlabel.size = NA to remove the default border around the label background.\n\nPutting all this together, we get our final map:\n\n# Load packages\nlibrary(ggspatial)\nlibrary(sf)\nlibrary(sfhotspot)\nlibrary(tidyverse)\n\n# Load data\nbike_thefts <- read_csv(\"https://mpjashby.github.io/crimemappingdata/vancouver_thefts.csv.gz\") |> \n  janitor::clean_names() |> \n  st_as_sf(coords = c(\"x\", \"y\"), crs = \"EPSG:32610\") |> \n  filter(type == \"Theft of Bicycle\")\nnbhds <- st_read(\"https://mpjashby.github.io/crimemappingdata/vancouver_neighbourhoods.geojson\")\n\nReading layer `vancouver_neighbourhoods' from data source \n  `https://mpjashby.github.io/crimemappingdata/vancouver_neighbourhoods.geojson' \n  using driver `GeoJSON'\nSimple feature collection with 22 features and 3 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: -123.2248 ymin: 49.19894 xmax: -123.0232 ymax: 49.29581\nGeodetic CRS:  WGS 84\n\n# Create KDE layer\nbike_theft_density_clip <- bike_thefts |> \n  # Calculate density\n  hotspot_kde(bandwidth_adjust = 0.5) |> \n  # Transform the density data to use the same CRS as the neighbourhoods layer\n  st_transform(\"EPSG:4326\") |> \n  # Clip the density layer to the area for which we have data\n  st_intersection(nbhds)\n\n\nDone: [--------------------------------------------------------------------] .\nDone: [==========================================--------------------------] .\nDone: [===========================================-------------------------] .\nDone: [============================================------------------------] .\nDone: [=============================================-----------------------] .\nDone: [==============================================----------------------] .\nDone: [===============================================---------------------] .\nDone: [================================================--------------------] .\nDone: [=================================================-------------------] .\nDone: [==================================================------------------] .\nDone: [===================================================-----------------] .\nDone: [====================================================----------------] .\nDone: [=====================================================---------------] .\nDone: [======================================================--------------] .\nDone: [=======================================================-------------] .\nDone: [========================================================------------] .\nDone: [=========================================================-----------] .\nDone: [==========================================================----------] .\nDone: [===========================================================---------] .\nDone: [============================================================--------] .\nDone: [=============================================================-------] .\nDone: [==============================================================------] .\nDone: [===============================================================-----] .\nDone: [================================================================----] .\nDone: [=================================================================---] .\nDone: [==================================================================--] .\nDone: [===================================================================-] .\nDone: [====================================================================] .\n                                                                              \n\n# Plot map\nggplot() +\n  # Add base map\n  annotation_map_tile(type = \"cartolight\", zoomin = 0) +\n  # Add density layer\n  geom_sf(\n    aes(fill = kde), \n    data = bike_theft_density_clip, \n    alpha = 0.75, \n    colour = NA\n  ) +\n  # Add neighbourhood boundaries (note `fill = NA` stops the neighbourhood\n  # shapes being filled with a colour, which would obscure the density layer\n  # underneath)\n  geom_sf(data = nbhds, colour = \"seagreen3\", fill = NA) +\n  # Add neighbourhood names\n  geom_sf_label(\n    aes(label = str_wrap(name, 10)), \n    data = nbhds, \n    alpha = 0.5,\n    colour = \"seagreen\", \n    lineheight = 1, \n    size = 2.5,\n    label.size = NA\n  ) +\n  # Set the colour scale\n  scale_fill_distiller(direction = 1) +\n  # Remove the axes, legend and other elements from the map that we don't need\n  theme_void()\n\n\n\n\nNow we can see that bike theft in Vancouver is heavily concentrated in a handful of neighbourhoods, particularly the Downtown and West End neighbourhoods. This map is much more useful than the first map that we produced in this tutorial showing only the point location of each crime, since in this latest map we can see not only the greatest concentrations of bike thefts but how they relate to the different areas of the city."
  },
  {
    "objectID": "05_your_second_crime_map/index.html#in-summary",
    "href": "05_your_second_crime_map/index.html#in-summary",
    "title": "\n5  Your second crime map\n",
    "section": "\n5.9 In summary",
    "text": "5.9 In summary\n\nIn this tutorial we have learned to produce a density map of crime. This type of map can be very useful in identifying where practitioners should focus efforts to respond to crime. For example, a map like this might help local police to decide where to send officers to carry out extra patrols, while a crime-prevention charity might decide to run events in particular areas to educate people on how best to protect their bikes.\n\nIn the next tutorial, we will learn how to improve this map further.\n\nYou can find out more about some of the things we have covered in this tutorial using these resources:\n\nUnderstand more about the history of trying to develop accurate map projections in this short video: Why all world maps are wrong.\nFind out more about making all sorts of charts (not just maps) with the ggplot2 package in the Data Visualisation chapter of R for Data Science by Hadley Wickham and Garrett Grolemund.\nLearn more about making maps using simple features in Chapter 1 of Spatial Data Science by Edzer Pebesma and Roger Bivand."
  },
  {
    "objectID": "06_map_context/index.html#introduction",
    "href": "06_map_context/index.html#introduction",
    "title": "\n6  Giving a map context\n",
    "section": "\n6.1 Introduction",
    "text": "6.1 Introduction\nIn this tutorial we will create this map of shootings in the Bronx borough of New York City in 2019. You’ll see that unlike the maps we have made so far, this map includes contextual elements such as a title, a legend and a scale bar.\n\n\n\n\n\nIn this tutorial we will learn to add important context to our maps using these elements."
  },
  {
    "objectID": "06_map_context/index.html#map-choices",
    "href": "06_map_context/index.html#map-choices",
    "title": "\n6  Giving a map context\n",
    "section": "\n6.2 Map choices",
    "text": "6.2 Map choices\nAmong the most important decisions you make when you are creating a map is what information to include and what to leave out. Watch this video to learn more about why this is important and how you can make those decisions.\n\nThe most important thing to remember when designing a map is to keep in mind the purpose that the map will be used for. Research on how people use maps has repeatedly shown that “the nature of the task or function to be performed by the map user is the single most important factor in determining how [someone] processes the information on the map”.\nAs explained in the video, when you create a crime map you should ask yourself:\n\n\nHow much does my audience know about this topic? so that you know how much information you should provide to help them understand it.\n\nHow well does my audience know this geographic area? so that you know what information to include to help people orient themselves, e.g. landmarks, main roads of natural features such as rivers.\n\nWhat will my audience use this map for? so that you can make sure everything on your map is relevant to that goal.\n\nIn what context will they be using this map? so that you can make the format of the map suitable to that context (e.g. if the map is going to be viewed by a large audience in a lecture hall, or by a police officer on their phone at night).\n\nWhat biases or opinions about this topic might my audience have? so that you can consider how the information (and your presentation of it) might influence those opinions or biases one way or another.\n\nMaps are powerful communication tools, which can sometimes knowingly or unknowingly mislead the reader. Watch this video to learn more about how maps can be misleading.\n\nWhenever you make a map, think about your own biases – are your own views on a topic likely to influence the results of your analysis? One way to test your own assumptions about a topic is to test them against other potential assumptions using an approach to crime analysis called hypothesis testing. To find out more about the hypothesis testing approach, read the paper Improving the explanatory content of analysis products using hypothesis testing"
  },
  {
    "objectID": "06_map_context/index.html#visual-hierarchy",
    "href": "06_map_context/index.html#visual-hierarchy",
    "title": "\n6  Giving a map context\n",
    "section": "\n6.3 Visual hierarchy",
    "text": "6.3 Visual hierarchy\nMaps are among the most complex types of data visualisation. Even if we have chosen wisely what to include and what to leave out, there is likely to be lots of information on our map. For all but the simplest maps, there is a risk of readers – especially those in a hurry – might be overwhelmed or mislead by competing pieces of information (such as different layers of data) on a map.\nTo help readers understand what parts of a map they should focus most of their attention on and which are of less importance, we can establish a visual hierarchy. Watch this video to learn more about visual hierarchies in mapping.\n\nWe have used some of the principles of visual hierarchy in the maps we have already made. For example, in the density map of bike thefts in Vancouver, we used strong colours to represent the data and shades of grey for the base map. This helped readers intuitively understand that they should focus most attention on the data."
  },
  {
    "objectID": "06_map_context/index.html#supporting-elements",
    "href": "06_map_context/index.html#supporting-elements",
    "title": "\n6  Giving a map context\n",
    "section": "\n6.4 Supporting elements",
    "text": "6.4 Supporting elements\nWe can often make our maps much more useful by adding supporting elements that explain the map content, give context or provide extra information. Watch this video to find out what elements you can add to your maps to make them more useful.\n\nWe will not need to include every supporting element mentioned in the video in all the maps we make. The visual hierarchy that you establish in your map by use of size, colour, etc. should make it clear which elements are most important. The data should always come first in the visual hierarchy, usually followed by the title and then the legend. Other elements should be lower down the hierarchy. In every map, the supporting elements should be designed so that they do not distract from the data.\n\n\n\nVisual hierarchy of elements in a crime map\n\nplace in hierarchy\nmap element\nhow often needed\n\n\n\n1st\ndata layers\nalways\n\n\n2nd\ntitle\nvirtually always\n\n\n3rd\nlegend\nusually\n\n\n4th\nbase map\nalmost always\n\n\n5th\nauthor and date\nvirtually always\n\n\n=6th\nscale\nsometimes\n\n\n=6th\nnorth arrow\nsometimes\n\n\n7th\ngrid\nrarely\n\n\n\n\n\nElements that are almost always needed on a crime map are not necessarily highest on the visual hierarchy. For example, the author name is virtually always needed but is relatively low on the visual hierarchy. This is because it is important information for readers who need it to judge the reliability of a map, or to get in touch to ask questions, but should not distract from the data for those readers who do not need it.\n\n\n\n\n\nBefore we start learning the code needed to add titles and legends to our maps, watch this video walk-though of the main steps."
  },
  {
    "objectID": "06_map_context/index.html#creating-and-storing-a-map",
    "href": "06_map_context/index.html#creating-and-storing-a-map",
    "title": "\n6  Giving a map context\n",
    "section": "\n6.5 Creating and storing a map",
    "text": "6.5 Creating and storing a map\nSince we will be adding various elements to a map in this tutorial, we will first create a map and save it as an R object. Any map or chart produced using the ggplot() function can be saved as an object using the assignment operator <-. Just as for the result of any other R function, if we save it to an object the result will not be printed to the screen, but we can easily see the plot by simply typing the object name in the R console.\n\n\nRows: 267 Columns: 5\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl  (3): incident_key, longitude, latitude\nlgl  (1): murder\ndate (1): occur_date\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nReading layer `nyc_precincts' from data source \n  `https://mpjashby.github.io/crimemappingdata/nyc_precincts.gpkg' \n  using driver `GPKG'\nSimple feature collection with 77 features and 4 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -74.25559 ymin: 40.49614 xmax: -73.70001 ymax: 40.91554\nGeodetic CRS:  WGS 84\n\n\nBandwidth set to 2,890 metres automatically based on rule of thumb\n\n\n\nDone: [--------------------------------------------------------------------] .\nDone: [==================================================================--] .\nDone: [===================================================================-] .\nDone: [====================================================================] .\n                                                                              \n\n\nWarning: attribute variables are assumed to be spatially constant throughout\nall geometries\n\n\n\n\n\nThe code used to create this map is very similar to the code we used in one of the previous tutorials to make a map of bike theft in Vancouver, although there are a few differences.\nLooking at the code above, we have used the grid argument to the hotspot_kde() function, specifying hotspot_grid(precincts, cell_size = 100). By default, hotspot_kde() calculates density estimates (KDE values) for every grid cell within the area covered by the crime data. This is fine when at least a few crimes have occurred in all the parts of the area for which we have data. But if crime is heavily concentrated in a few places and there are large areas with no crimes, the density layer will not cover the whole area for which we have data. In this case, it is important to extend the density layer manually to make it clear that the apparent low density of crime in some places is due to a genuine lack of crime there, rather than because we do not have data for those places.\nWe can do this by creating a grid of cells for which KDE values should be calculated, rather than letting hotspot_kde() do this automatically. In the code above, we have used the hotspot_grid() function to create a grid that covers all the precincts in the Bronx (the area we have data for), and then passed the resulting grid to the grid argument of hotspot_kde(). This ensures that KDE values are calculated for every part of the Bronx.\n\nThe only other difference from the Vancouver map is that we have used the ordinal() function from the scales package to convert the precinct numbers to ordinal numbers (1st, 2nd, 3rd, etc.) for the map labels. This is because police precincts in New York City are usually referred to using ordinal numbers (e.g. “the 1st Precinct” rather than “Precinct 1”) and it will be easier for people to read the map if it uses terms they are familiar with.\nThere are many other functions in the scales package that format numbers in different ways, including comma() to add thousands separators to numbers and dollar() to format numbers as values in dollars or other currencies. There is a full list of scales functions on the package website.\n\nWe now have a basic map of shootings in the Bronx. This map isn’t good enough on its own, but we can use it to learn how to add supporting elements to a map."
  },
  {
    "objectID": "06_map_context/index.html#titles",
    "href": "06_map_context/index.html#titles",
    "title": "\n6  Giving a map context\n",
    "section": "\n6.6 Titles",
    "text": "6.6 Titles\nA map title is one of the most important ways to add context to a map. Titles can either be descriptive or declarative. Descriptive titles simply state what data are shown on the map. For example, we might give our map the title “Shootings in the Bronx, 2019”. Declarative titles, on the other hand, state what you think the main conclusion should be that readers remember about the map. For example, we might use the title “Shootings are focused in the South Bronx”.\nDeclarative titles are usually more useful than descriptive titles because they help the reader to interpret the map. But writing a good declarative title is harder than writing a descriptive title, because it requires you to think about what is the main point that you want to make with the map. To help you come up with a good declarative title, you might want to try several different titles so that you can choose the one that communicates your message most clearly.\nWe can add a title to our map using the labs() (short for ‘labels’) function from the ggplot2 package. We can use labs() to add labels to various different parts of a map or plot, but for now we will just use the argument title to set the title.\n\n\n\n\n\nSometimes our preferred title might be too long to fit on a map. In this case, we can break the title across two or more lines. We can do this manually by adding the characters \\n (the character code for a new line) at the point where we want the text to start a new line. Alternatively, we can use the str_wrap() function from the stringr package to wrap the text automatically into lines of a given maximum length (specified using the wrap argument).\nWhen you use a declarative title for your map, it is often useful to provide a subtitle containing descriptive information. Adding a subtitle is very easy using the subtitle argument to the labs() function.\n\nUse the code above as a template to add a subtitle to your map explaining that the map shows fatal and non-fatal shootings in 2019.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n6.6.1 Using captions to add author and other information\nThe labs() function has another argument that we can use to add text to our map for context. The caption argument is used to add information such as the author, date and source of the data to a map or chart. We can put any information we like into the caption, using str_wrap() or the new-line character \\n if necessary to stop the text overflowing the map.\n\n\n\n\n\nThis code uses the str_glue() function from the stringr package to incorporate some automatically updated information (in this case, the current date) into the caption. str_glue() glues together any number of character strings separated by commas – in this case, the code includes two strings so that the lines of code do not become too long to easily read.\nstr_glue() can also include the values of R objects and the results of R functions that are placed inside braces {}. So the code {lubridate::today()} runs the function today() from the lubridate package and glues the result (the current date) into the text. If we had already loaded the lubridate library we could have just used the code {today()}, but since we do not need any other functions from lubridate for this analysis, we instead do not load the package but call the function by specifying which package it comes from using the :: operator as in the previous tutorial.\n\nWhen you use data to make maps, the data provider will often require you to acknowledge the source of that data. This is a legal requirement so it is important that you do this when required.\nThe base maps we use in this course use data from OpenStreetMap, which requires people using its data to acknowledge that they have done so. The easiest way to do this is to add the text “Map data from OpenStreetMap” to the caption of your maps if they use base maps from OpenStreetMap.\n\n\n6.6.2 Changing the appearance of titles and captions\nWe have added a title, subtitle and caption to our map, but you might not be happy with their appearance. You might want, for example, to move the caption down the visual hierarchy by making the text smaller and/or a lighter colour, or add some space between the subtitle and the map itself.\nWe can exercise almost complete control over the supporting elements of maps or charts made with ggplot() using the theme() function. One important thing to remember about theme() is that it only controls the non-data elements of a map – nothing you do with the theme() function will have any effect on the data elements of a map (in this case, the layer showing the density of shootings). To change the appearance of data layers within ggplot() maps, use the geom_, and scale_ families of functions as we have learned in previous tutorials.\nThe theme() function has a lot of potential arguments. If you need help using the theme() function (or any function in R) you can view a manual page (including a list of arguments) for the function by:\n\ntyping a question mark followed by the function name without parentheses (e.g. ?theme) into the R console,\ntyping the function name without parentheses into the search box in the Help panel in RStudio, or\nclicking on the function name anywhere in your R code to place the cursor on the function name, then pressing F1 on your keyboard.\n\nTry opening the manual page for theme() now to see the list of possible arguments it can take. Fortunately, we will not need most of these arguments most of the time – ggplot() has default values built in for every value that can be changed using theme(), and these defaults will be reasonable in almost all cases.\nTo reduce the visual prominence of the map caption, we can change the value of the plot.caption argument to theme(). Since the caption is a text element (rather than a polygon, line, etc.), we can use the helper function element_text() to do this. The following code changes the colour of the caption text to a lighter grey and makes the text smaller relative to the default using the helper function rel() (for relative sizing) – 0.7 means the text will be 70% as big as it would have been by default.\n\n\n\n\n\nThe helper function element_text() has arguments to control the appearance of text in different ways. As well as colour (or color, either is fine) and size, there are:\n\n\nfamily controls the font used, e.g. Times New Roman or Helvetica,\n\nface controls the style of the font, i.e. ‘plain’, ‘italic’, ‘bold’ or ‘bold.italic’,\n\nhjust controls the horizontal justification of the text, where 0 means left aligned, 0.5 means centred and 1 means right aligned,\n\nvjust controls the vertical justification, and\n\nangle controls the angle (in degrees) of the text (0 means horizontal),\n\nlineheight controls the space between lines if you have created a value that has more than one line (e.g. using \\n or str_wrap()).\n\nThe margin argument controls the space around the text. It is easiest to specify the value of margin using the helper function margin() designed for that purpose. You specify the top, right, bottom and left margin separately in that order – to remember the order, think ‘trouble’. The following code changes the margin around the map subtitle.\n\nChange this code so that it also makes the subtitle 80% of the default size and changes the caption so that it is left aligned.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNow that we have finished setting the text elements for our map, we can save it as a new object that we can use as the basis for the other objects we want to add."
  },
  {
    "objectID": "06_map_context/index.html#legends",
    "href": "06_map_context/index.html#legends",
    "title": "\n6  Giving a map context\n",
    "section": "\n6.7 Legends",
    "text": "6.7 Legends\nLegends are important for all but the simplest crime maps because they help readers to interpret the points, lines and polygons used to represent data on a particular map. Except for point maps containing only a small number of crimes (such as the map of homicide in downtown Atlanta that we produced in the first tutorial), crime maps will almost always need a legend to help users interpret them.\nProducing a legend manually could be quite complicated, but fortunately ggplot() produces legends automatically. The reason the maps we have produced up to now haven’t included legends is that we have been suppressing the legends using the argument legend.position = \"none\" to the theme() function – look back at the code on the page ‘Creating and storing a map’ in this tutorial to see this code on the final line of the stack of functions added to ggplot().\nggplot() will add a legend to a map or chart whenever one or more layers of data are represented using an aesthetic property such as size, shape, colour or fill. In our current map, the density of shootings is represented using the fill colour of the polygons produced by the hotspot_kde() function, with darker colours representing more shootings.\nTo reveal the legend automatically generated, we can add another call to the theme() function to our existing ggplot() object. This overrides the previous code that set the legend position to none, instead placing the legend on the right-hand side of the plot. We only need to do this because we previously suppressed the legend – if we had not suppressed it, the legend would have appeared automatically.\n\n\n\n\n\nOur map now has a legend, but we may want to adjust its appearance by:\n\nchanging the default legend title from “kde” to something more meaningful,\nmoving the legend down the visual hierarchy by making it smaller (at the moment it is almost as visually prominent as the data),\nremoving the potentially confusing raw density values.\n\nWe can change the default legend title by once again using the labs() function. Since we want to change the title of the legend, you might reasonably think that we would do this using something like labs(legend = \"density\") but unfortunately that code would do nothing at all. Instead, we have to set the legend title using the aesthetic (colour, size, shape, etc.) that the legend represents. This makes it possible to specify multiple titles if there are separate legends for different layers that use different aesthetics. For example if a map used lines of different colours to show streets of different types and filled polygons to show the density of crime, it would be possible to have separate legends explaining each aesthetic. In this case, we’ve specified that the kde column in the data should control the fill aesthetic, so we can set the title for that legend using fill = \"title we want\".\n\n\n\n\n\nTo make the legend smaller, we can use theme() in the same way as we did to change the appearance of the caption. We use the legend.title argument to format the legend title and the legend.text argument to format the labels for each value in the legend.\nWe will also make the colour bar in the legend (called the key by ggplot()) slightly smaller using the legend.key.width argument. To do this we will use the helper function unit(), which allows us to specify the size using any of several common units. In this case, we will specify the key size in lines (1 line = the height of one line of text) so that it is relative to the text size we have chosen.\nAt the same time, we will reduce the size of the legend title and the legend text (the numbers next to the colour bar), using the legend.text and legend.title arguments together with the the helper function element_text(). In both cases, we will set the text size relative to the default text size using the rel() helper function.\n\n\n\n\n\nFinally, we want to remove the raw density values, since these are difficult to interpret and might distract readers from the key message that darker colours on the map represent higher densities of shootings.\nBy default, ggplot() sets the label for each legend key based on the data. To specify our own labels, we can use the labels argument to the scale_fill_distiller() function that we previously used to set the colour scheme of the density layer on the map.\nWhen we set colour bar labels manually, we have to also specify where on the colour bar we want those labels to appear. We do this using the breaks argument to scale_fill_distiller(), making sure the number of values we supply to the breaks argument is the same as the number of labels we’ve given to the labels argument (otherwise R will produce an error).\nIn this case, we want to add two labels (“higher” and “lower”), one at either end of the colour bar. We could look at kde column of the shootings_kde object to find the minimum and maximum values, but that would introduce the risk of us accidentally entering the wrong values. Instead, we can use the pull() function to extract the kde column from the shootings_kde dataset and then use the range() function to find the minimum and maximum values. Putting this together, we get breaks = range(pull(shootings_kde, \"kde\")).\n\n\nScale for fill is already present.\nAdding another scale for fill, which will replace the existing scale.\n\n\n\n\n\nNow that we have finished formatting the legend, we can again store the map as an object that we can build on further.\n\n\nScale for fill is already present.\nAdding another scale for fill, which will replace the existing scale."
  },
  {
    "objectID": "06_map_context/index.html#scales-and-north-arrows",
    "href": "06_map_context/index.html#scales-and-north-arrows",
    "title": "\n6  Giving a map context\n",
    "section": "\n6.8 Scales and north arrows",
    "text": "6.8 Scales and north arrows\nThe final elements we can add to our map are a scale bar and a north arrow, which can both be added using functions from the ggspatial package.\n\n6.8.1 Scale bars\nTo add a scale bar, we can add a call to the annotation_scale() function to our existing ggplot() object.\n\n\n\n\n\nThe default scale bar is a little too visually dominant for the low place it should have in the visual hierarchy of our map, and the default placement in the bottom-left corner happens to overlap with the highest density of shootings. We can change the scale bar using arguments to the annotation_scale() function:\n\n\nwidth_hint = 1/5 changes the (approximate) proportion of the map width across which the scale bar stretches,\n\nstyle = \"ticks\" changes the style of the scale bar to the less visually prominent line-and-tick-marks style, and\n\nlocation = \"br\" moves the scale bar to the bottom-right corner of the map.\n\n\n\n\n\n\n\n6.8.2 North arrows\nWe can add a north arrow using the annotation_north_arrow() function. The default arrow is too obtrusive to fit its position in the visual hierarchy, so we will change its appearance using the arguments:\n\n\nlocation = \"tr\" to move the north arrow to the top-right corner, since we have put the scale bar in the bottom-right where the north arrow would be placed by default,\n\nheight = unit(1.5, \"lines\") to make the arrow smaller, and\n\nstyle = north_arrow_minimal(text_size = 8) to use a smaller style of arrow, at the same time reducing the font size (measured in points) of the N symbol."
  },
  {
    "objectID": "06_map_context/index.html#saving-maps",
    "href": "06_map_context/index.html#saving-maps",
    "title": "\n6  Giving a map context\n",
    "section": "\n6.9 Saving maps",
    "text": "6.9 Saving maps\nOnce you have a complete map, it is often useful to save it as an image file so that you can share it with others or embed it into a report or presentation. You can save plots created with ggplot() using the ggsave() function.\nBefore we do that, lets create a final map that includes a base map as well as some supporting elements. We won’t include a north arrow because north is at the top of the map and it’s unlikely anyone will be using this map for navigation. We will include a scale bar so that people looking at the map can see the approximate size of shooting hotspots. Remember we have already loaded the necessary packages, as well as data stored in the shootings, precincts and shootings_kde objects.\n\nThere is usually no need to save the map into several different objects (e.g.  shootings_map_titled or shootings_map_legend) as we have done in this tutorial. We have only done that here so you could learn about the different contextual elements one by one.\nIt is usually much better to create a whole ggplot() stack in one go, since it is easier to keep track of all the elements that way. In the code below all the elements have been combined into a single ggplot() stack.\n\n\n\n\n\n\nggsave() can create image files in many different formats, including PNG, JPEG and PDF. ggsave() will determine which type of file to create according to the file extension of the file name that you specify. So ggsave(\"bronx_shootings_2019.pdf\", plot = shootings_map_legend) produces a PDF file, while ggsave(\"bronx_shootings_2019.jpg\", plot = shootings_map_legend) produces a JPEG image file.\nYou can specify the size of the image that will be saved using the height and width arguments. Note that for historical reasons these values are in inches by default, but you can change this to either centimetres (using units = \"cm\"), millimetres (using units = \"mm\") or pixels (using units = \"px\").\nTo share our map with others, lets save it as an A4-size PDF.\n\n\n\nWe can now use share this file by email, upload it to a website or embed it in another document."
  },
  {
    "objectID": "06_map_context/index.html#in-summary",
    "href": "06_map_context/index.html#in-summary",
    "title": "\n6  Giving a map context\n",
    "section": "\n6.10 In summary",
    "text": "6.10 In summary\n\nIn this tutorial we have learned about the importance of understanding the purpose for which people will use a map when making decisions about map design. We have also learned about how establishing a visual hierarchy on our map can help steer readers towards the most-important elements and how to add titles, legends and scale bars to maps in R.\n\n\nYou can find out more about the topics we have covered in this tutorial:\n\nFor a short summary of research into how people read maps and what that tells us about how to design a map, see Cartography, visual perception and cognitive psychology by Amy Griffin.\nFor a more-detailed explanation of how visual hierarchy can be applied to maps, see Visual Hierarchy and Layout.\nFor more examples of how maps can mislead, read How to lie with maps by Alan Smith.\n\n\n\n\n\n\nXKCD.com comic ‘Geography’ licensed under the Creative Commons Attribution-NonCommercial license."
  },
  {
    "objectID": "07_handling_bugs/index.html#introduction",
    "href": "07_handling_bugs/index.html#introduction",
    "title": "\n7  Handling bugs in your code\n",
    "section": "\n7.1 Introduction",
    "text": "7.1 Introduction\nIn this tutorial we will learn how to deal with bugs in the R code that we write. It’s inevitable that you will make mistakes in the R code that you write. Everyone types the wrong command every now and again, just as everyone sometimes clicks the wrong button in other pieces of software. Learning how to identify and fix bugs is part of the process of learning to code.\nSome errors are easy to identify and easy to fix. Imagine you are working on this dataset of frauds in Kansas City:\n\n\n\n\n\n\n\n\n\n\n\n\nuid\noffense_code\noffense_type\ndate\nlongitude\nlatitude\n\n\n\n9362175\n26B\ncredit card/automated teller machine fraud\n2015-02-14 11:05:00\n-94.5715\n39.1086\n\n\n9362176\n26B\ncredit card/automated teller machine fraud\n2015-02-14 11:05:00\n-94.5715\n39.1086\n\n\n9362201\n26A\nfalse pretenses/swindle/confidence game\n2015-02-14 13:30:00\n-94.6568\n39.2453\n\n\n9362202\n26A\nfalse pretenses/swindle/confidence game\n2015-02-14 13:30:00\n-94.6568\n39.2453\n\n\n9362213\n26C\nimpersonation\n2015-02-14 15:00:00\n-94.5886\n38.9956\n\n\n9362214\n26C\nimpersonation\n2015-02-14 15:00:00\n-94.5886\n38.9956\n\n\n\n\n\nIf you try to run this code:\n\nselect(frauds, offense_category)\n## Error in `select()`:\n## ! Can't subset columns that don't exist.\n## ✖ Column `offense_category` doesn't exist.\n\n\nYou will get an error message saying something like:\nError: Can't subset columns that don't exist. x Column `offense_category` doesn't exist\n\n\nIt causes an error.\n\nIn this case it is fairly easy to identify that one of the columns you have tried to select does not exist. Maybe you mis-remembered the name of the column. To find out what the correct column name is, you can either print the first few rows of the object (by typing the code head(frauds) into the R console) or use the names() function to print a list of column names present in the data:\n\nnames(frauds)\n## [1] \"uid\"          \"offense_code\" \"offense_type\" \"date\"         \"longitude\"   \n## [6] \"latitude\"\n\n\nChange the following code to correct the mistaken column name:\n\nselect(frauds, offense_category)\n## Error in `select()`:\n## ! Can't subset columns that don't exist.\n## ✖ Column `offense_category` doesn't exist.\n\n\nselect(frauds, offense_type)\n## # A tibble: 6 × 1\n##   offense_type                              \n##   <chr>                                     \n## 1 credit card/automated teller machine fraud\n## 2 credit card/automated teller machine fraud\n## 3 false pretenses/swindle/confidence game   \n## 4 false pretenses/swindle/confidence game   \n## 5 impersonation                             \n## 6 impersonation\n\n\nOther errors will be harder to identify and fix. In this tutorial we will go through the process of debugging – identifying, understanding and fixing errors in your code. Sometimes fixing issues with your code can feel like a bit of a roller coaster, but (like most things) it becomes much easier with practice, and if you approach errors in a systematic way.\n\n\n\nStats Illustrations by Allison Horst licensed under the Creative Commons Attribution licence."
  },
  {
    "objectID": "07_handling_bugs/index.html#errors-warnings-and-messages",
    "href": "07_handling_bugs/index.html#errors-warnings-and-messages",
    "title": "\n7  Handling bugs in your code\n",
    "section": "\n7.2 Errors, warnings and messages",
    "text": "7.2 Errors, warnings and messages\nWhen something is not quite right, or just when there is an issue in your code that you should be aware of, R has three ways of communicating with you: messages, warnings and errors.\n\n7.2.0.1 Messages\nMessages are usually for information only and typically don’t require you to take any action. For example, the function get_crime_data() from the crimedata package issues a message to tell you what data it is downloading. It does this because downloading data sometimes takes a few seconds and without a message you might wonder if the code was working.\nBy default, messages are displayed in RStudio in dark red text, although this might vary if you have changed any settings in the Appearance tab of the RStudio Options dialogue. You can generate a message for yourself using the message() function. This is useful if you are writing code and you want to remind yourself of something, or record a particular value. For example, if your code is likely to take a long time to run, you might want to record the time your code started running by generating a message on the first line of your code:\n\nmessage(stringr::str_glue(\"Code started: {Sys.time()}\"))\n## Code started: 2023-03-22 16:40:45\n\nWhen R prints a message about your code, any code underneath the code that generated the message will still run:\n\n2 * 2\n## [1] 4\n\nmessage(\"This is a message. It might be important so make sure you understand it.\")\n## This is a message. It might be important so make sure you understand it.\n\n2 / 2\n## [1] 1\n\n\n7.2.0.2 Warnings\nWarnings are generated when there is a potential problem with your code, but the problem was not serious enough to stop your code running entirely. For example, ggplot2 functions like geom_point() will send a warning if some of the rows in your data contain missing values (e.g. if some crimes have missing co-ordinates).\nWarnings are important and you should take time to read and understand them, but it is possible that having done so it will be safe to not take any action. Whether it is safe to take no action will often depend on exactly what you are trying to do, which is why it is important that you understand each warning that you see. For example, if you already know that some rows in your data contain missing values and are happy to plot the remaining values, it will be safe to ignore the warning produced by geom_point(). But if your dataset should not have any missing values in it, you will need to investigate why geom_point() is warning you about missing values and whether those values have been accidentally introduced by some part of your code.\n\nIt is not safe to ignore warnings unless you are sure why they occurred and certain that you don’t need to take any action.\nOne particularly dangerous scenario is where your code produces warnings but still produces what looks like a reasonable result. In these circumstances it can be tempting to ignore the warnings and assume that everything is fine, since the code still produced roughly what you were expecting. However, it’s possible that the plausible answer is nevertheless wrong because of whatever problem is generating the warning in R. Do not assume that warnings are safe to ignore just because they don’t stop your code running.\n\nWarnings are displayed in the same font as messages, but with the text Warning message: at the start. You can generate your own warning messages using the warning() function:\n\nwarning(\"Something might be wrong. Check to make sure.\")\n## Warning: Something might be wrong. Check to make sure.\n\nAs with messages, warnings will not stop your code running. This means that if the warning signalled a genuine problem with your code, the results of the lines underneath the warning might not be reliable. That is why it is important to understand warnings when you see them.\n\n7.2.0.3 Errors\nErrors are generated when there is something wrong with your code or the results it produces that means the code cannot continue running any further. An error might occur, for example, because the function read_csv() could not open the requested file (maybe because the file does not exist). In this case, it would make no sense for the rest of the code to run because it probably depends on the data that read_csv() was supposed to load but could not.\nIt would make sense to be able to generate your own errors using the error() function, but this is one of those times when the function to do something in R has a different name from that you might be expecting. In fact, you can generate an error using the stop() function:\n\nstop(\"Something is defintely wrong. Don't go any further until you've fixed it.\")\n## Error in eval(expr, envir, enclos): Something is defintely wrong. Don't go any further until you've fixed it.\n\nThere are two types of errors in R: syntax errors and logical errors. Syntax errors happen when R cannot read your code because of a mistake in how it has been typed out. For example, if you forget to put a comma between the arguments of a function, you will get this error:\n\nmessage(\"A\" \"B\")\n## Error: <text>:1:13: unexpected string constant\n## 1: message(\"A\" \"B\"\n##                 ^\n\nWhen you run R code, R reads all the code and checks if it can be interpreted as valid R code. If not, R will produce a syntax error. Because all of your code is checked for syntax errors before any code is actually run, a syntax error anywhere in your code will stop all of your code running. Syntax errors are typically fairly easy to fix, because they are usually caused by typos.\nThe second type of error is a logical error. This happens when R is able to interpret what you have written, but something is wrong with what you have asked it to do. These are called logical errors because there is usually some problem with the logic of what you are asking R to do. Like syntax errors, logical errors can be caused by typos, but logical errors can also have many other causes.\nThere is a saying in programming that a computer will do exactly what you tell it to do, which may not be the same thing as what you wanted it do. Logical errors happen when you have told R to do something that it cannot do. For example, you might be asking R to multiply together a numeric value and a character value, which is illogical.\nSince every step in your code depends on the steps that went before it, it is only possible to identify a logical error during the process of running the code. This means that a lot of your code might run successfully before an error occurs.\nLogical errors are typically harder to fix than syntax errors are, because fixing a logical error involves understanding (a) what you have asked R to do and (b) the current state of everything in your code at the moment when the error occurs. Fortunately, there are lots of ways to identify and fix logical errors.\nNow we know what errors, warning and messages are, we need to find out how to deal with them when they happen."
  },
  {
    "objectID": "07_handling_bugs/index.html#finding-problems",
    "href": "07_handling_bugs/index.html#finding-problems",
    "title": "\n7  Handling bugs in your code\n",
    "section": "\n7.3 Finding problems",
    "text": "7.3 Finding problems\nIf an error or warning has a simple cause, such as the example of the incorrect column name in the previous section, you can just fix the problem and re-run the code. For problems that are more difficult to handle, you will need to follow a step-by-step process to find and fix them. Think of this as like being a mechanic fixing a car – first you work out what the problem is, then you fix it.\nIf you code only has one line, it will probably be obvious where any problem lies. But most of your code does several things to achieve a particular goal, such as making a map. The first task in dealing with a problem is therefore to work out exactly which function or other piece of code has caused it. For example, take this data-wrangling code to reduce the size of a dataset and sort it in date order:\n\nlibrary(tidyverse)\n\nfrauds |> \n  select(offense_type, date, longitude, latitude) |> \n  filter(offence_type == \"impersonation\") |> \n  arrange(date)\n## Error in `filter()`:\n## ℹ In argument: `offence_type == \"impersonation\"`.\n## Caused by error:\n## ! object 'offence_type' not found\n\nThis code produces a fairly complicated error message. As is often the case, the most useful part of the error message is the last part:\nInput `..1` is `offence_type == \"impersonation\"`\nThis suggests the error is on line 3 of the code, since that is the only line containing the code offence_type == \"impersonation\". To check this, we can comment out that line by placing a # symbol at the start of the line. Do this and re-run the code above – it should now run without a problem.\nNow we know the problem is on the line filter(offence_type == \"impersonation\"), we can look at that line in more detail. Can you spot the problem with that line?\nThe error message in this case has been caused by a typo – the code offence_type == \"impersonation\" uses the British spelling of the word ‘offence’ but in the dataset the variable is spelled using the American English ‘offense’ (you can see the US spelling in the line of code above the line that is causing the error).\n\nIf you correct the spelling in the code above, it should now run without a problem.\n\nSometimes it will not be as clear as this where to start in looking for the problem. In particular, some errors can be caused by a problem on one line of code, but only actually have a negative effect on a subsequent line of code. For example, if you run this code:\n\nfrauds |> \n  select(offense_code, longitude, latitude) |> \n  filter(offense_code == \"26E\") |> \n  arrange(date)\n## Error in `arrange()`:\n## ℹ In argument: `..1 = date`.\n## Caused by error:\n## ! `..1` must be a vector, not a function.\n\nThe error message produced suggests the problem is with the arrange() function, but everything is correct with that function since arrange() is a correct function name and we already know that date is a column in the tibble named frauds. So the problem must lie elsewhere. In cases like this, it can be helpful to comment out all the lines of code except the first one and then uncomment one line at a time until you find the one that causes the problem.\n\nWith the following code, uncomment one line at a time starting on line 2, re-run the code and then uncomment another line. After you run the code each time, look at the output produced until an error occurs or you find that the output is not what you expected.\n\nfrauds |> \n#  select(offense_code, longitude, latitude) |> \n#  filter(offense_code == \"26E\") |> \n#  arrange(date)\n## Error: <text>:5:0: unexpected end of input\n## 3: #  filter(offense_code == \"26E\") |> \n## 4: #  arrange(date)\n##   ^\n\n\n\n\nDid you get an error beginning <text>:5:0: unexpected end of input 3?\n\n\nWhen commenting out lines of code in a code pipeline, it is important to remember to remove the pipe operator |> from the end of the last line of your code. Otherwise you will get an error that begins <text>:5:0: unexpected end of input 3 because R is expecting the final |> to have some code after it. Remember that R will ignore the lines that are commented out, so make sure every line in your pipeline ends with a |> except the last line that is not commented out.\n\n\nIf you uncommented all but the last line and run the code, you would see that the line select(offense_code, longitude, latitude) removes the column date from the dataset, so it’s not possible to use date as an argument to the arrange() function later on.\nUncommenting one line at a time until you find an error or output that is not what you expected is a useful way to isolate problems, but it will not always work. In particular, it will not work if the problem is caused by some code that should have been included but is missing from your code entirely. For example, if you try to run the function st_transform() on a tibble without first changing it into an SF object:\n\nlibrary(sf)\n## Linking to GEOS 3.11.0, GDAL 3.5.3, PROJ 9.1.0; sf_use_s2() is TRUE\n\nfrauds |> \n  select(offense_code, longitude, latitude) |> \n  filter(offense_code == \"26A\") |> \n  st_transform(\"EPSG:3603\")\n## Error in UseMethod(\"st_transform\"): no applicable method for 'st_transform' applied to an object of class \"c('tbl_df', 'tbl', 'data.frame')\"\n\nIn these cases it is particularly useful to check every argument that you have used in a function to track down the error. We will look at this later in this tutorial.\n\n7.3.1 Errors caused by data problems\nMany logical errors will be caused by problems with the code you have written, such as when you try to use a function (e.g. st_intersection()) that only works on an SF object but specify that it should use an object of another type. But sometimes logical errors are caused not by your code, but by a mis-match between the structure that your data actually has and the structure you think your data has. We have already seen an example of this in this tutorial, in the code that tried to refer to a column called offense_category in a dataset that did not have a column with that name.\nErrors caused by a mismatch between the data you think you have and the data you actually have can be particularly frustrating, because there is no way to identify them from just looking at your code. For this reason, it is often important when trying to identify problems in your code to look at the data that is used as the input for your code, and the data that is produced by each step in your code. We used this technique to find the error in our code above that was caused by filter() removing all the rows from a dataset because we had told filter() to only keep rows containing a value that was not present in the dataset. There were no obvious problems with the code we had written, so the only way to find the cause of this problem was to view the dataset returned by the filter() function.\nFinding data problems is one of the reasons why we have used the head() function so often in these tutorials to look at the data at each step in writing a block of code. head() only shows us the first few rows of a dataset, which will not always be enough to identify a problem if the problem is caused by values that are only present in a few rows in the data. For small datasets, we can use the View() function (note the capital letter) to open the entire dataset in a new tab in RStudio.\nFor bigger datasets, this will not work. In that case, we can use the sample_n() or sample_frac() functions from the dplyr package to return a random sample of rows from the data. This can be useful to let us look at a representative sample of a large dataset. sample_n() returns a specific number of rows, e.g. sample_n(frauds, 10) returns 10 rows at random from the frauds dataset. sample_frac() returns a specific proportion of the dataset, e.g. sample_frac(frauds, 0.1) returns a sample of 10% of rows from the data."
  },
  {
    "objectID": "07_handling_bugs/index.html#understanding-problems",
    "href": "07_handling_bugs/index.html#understanding-problems",
    "title": "\n7  Handling bugs in your code\n",
    "section": "\n7.4 Understanding problems",
    "text": "7.4 Understanding problems\nSo far, we have tried two ways to deal with errors:\n\nreading a simple error message that makes the problem obvious,\ncommenting out all the code and then uncommenting one line a time until the error appears.\n\nSometimes you will encounter an error that is still not simple to solve. In this case, it is still important to identify the line of code causing the problem, either by working it out from the text of the error message or commenting out lines of code in turn.\nOnce you know what line is causing the problem, you should focus on understanding exactly what that line of code does. In particular:\n\nWhat data do any functions on that line expect to work on?\nWhat are the values of any arguments to the functions on that line?\nWhat value do the functions on that line produce?\n\nYou can get a lot of help in understanding each function by referring to its manual page. You can access the manual page for a function by:\n\ntyping a question mark followed by the function name without parentheses (e.g. ?mutate) into the R console,\ntyping the function name without parentheses into the search box in the Help panel in RStudio, or\nclicking on the function name anywhere in your R code to place the cursor on the function name, then pressing F1 on your keyboard.\n\nAny of these options opens up a manual page for that function in the Help panel in RStudio. For example, this is the manual page for the str_wrap() function from the stringr package. You can load it by typing ?str_wrap in the R console.\n\n\nDid you get an error beginning No documentation for ‘str_wrap’?\n\n\nWhen you use the ? operator to look up the manual page for a function, R searches through all the functions in the packages you have loaded. If there is no function of the name you have given in the loaded packages, R will produce an error.\nTo access the manual page of a function in a package that is not loaded, you can specify the package in which R should look for the function using the package name and the :: operator (this is the same way that we have already learned to call functions from packages that haven’t been loaded). For example, you can use the code ?stringr::str_wrap to access the manual page for the str_wrap() function from the stringr package even if the stringr package is not loaded.\n\n\n\n\nAll manual pages have the same format.\n\n\nDescription gives a short description of what the function does. If multiple related functions are described in a single manual page, this section will explain the differences between them. For example, the manual page for the mutate() function from the dplyr package explains the difference between the mutate() function and the closely related transmute() function.\n\nUsage shows a single example of how the function works. If there are any optional arguments to the function, this section will show what the default values of those optional arguments are. For example, the manual page for the str_wrap() function from the stringr package show that the default value of the width argument is width = 80.\n\nArguments gives a list of arguments and the values they can take. It is particularly important to note the type of value expected. So the st_transform() function from the sf package expects an SF object as its first argument – if you provide another sort of object (such as a tibble), this will cause an error.\n\nValue explains the type of value that the function will return, and whether this value might be of a different type depending on the values of particular arguments. For example, the mean() function in base R returns the arithmetic mean of a vector of numbers. However, if any of the numbers is NA then mean will return NA unless the argument na.rm = TRUE is used. In that case, mean() will ignore the missing values and return the mean of the values that are present.\n\nExamples gives more examples of how the function can be used.\n\nChecking the manual page for a function can often help you understand why a particular piece of code is not working. If you have set any optional arguments for a function that is producing an error, it may help to reset those arguments to their default values (as shown in the Usage section of the manual page) one by one to understand what effect this has on your code.\nBy reading the error message, isolating the error by commenting out and then reading the manual page, you will be able to fix almost all the errors you will come across in writing R code. Occasionally, however, you will find an error that you just can’t understand. In that case, you will need to get some help from others."
  },
  {
    "objectID": "07_handling_bugs/index.html#how-to-fix-some-common-errors",
    "href": "07_handling_bugs/index.html#how-to-fix-some-common-errors",
    "title": "\n7  Handling bugs in your code\n",
    "section": "\n7.5 How to fix some common errors",
    "text": "7.5 How to fix some common errors\nThere are some mistakes that it is common for people to make when writing code. The table on this page gives some common error messages and how to fix them. Read through this list now but you can also refer back to it later on if you need to. You can download a PDF version of this table for future reference.\nIn these error messages, the code blah represents the function, object or value that the error relates to.\n\n\n\n\nError message\nHow to fix it\n\n\n\nthere is no package called 'blah'\nYou have either mis-typed the package name or the package is not installed. Check the spelling or use install.packages() to install the package.\n\n\ncould not find function \"blah\"\nYou have either mis-typed the function name or the package containing that function is not loaded. Check the spelling or use library() to load the package.\n\n\nobject 'blah' not found\nYou have either mis-typed the name of the object or the object does not exist. Check the spelling and make sure you have run the code that creates the object. This error can also occur when you have forgotten to put quotes around a character value in an argument, since R treats words without quotes around them as the names of objects. In this case, check that you have used quote marks around any character values in your code.\n\n\n\n'blah' does not exist in current working directory\n\nYou have either mis-typed the name of a file that R is trying to access or the file does not exist in the location you have specified. Check the spelling and make sure the file exists.\n\n\nnon-numeric argument to binary operator\nYou have tried to use a mathematical operator such as + or - with a non-numeric value. For example, you might have written the code 1 + blah thinking that blah holds a numeric value, but if blah actually holds a character value then trying to add it to 1 makes no sense. Check that any objects in your code have the values you expect them to.\n\n\n\nobject of type 'closure' is not subsettable\n\nYou have tried to use a function as if it is an object, which can happen when you store data in an object that has the same name as an R function (most commonly, when you store some data in an object called data, since there is a function called data()). The circumstances that produce this error are often not simple to understand, so the best way to handle this error is to avoid it by not naming objects using the names of functions.\n\n\n\nno applicable method for 'blah' applied to an object of class \"blah\"\n\nSome R functions (called generic functions) work in different ways depending on what type of object you use them on. But if you use a generic function on an object that it does not know how to handle, you will see this error. Check the object(s) that you provided to the function causing the error to make sure it is the type of object you are expecting it to be.\n\n\n\nunexpected numeric constant in \"blah\", unexpected string constant in \"blah\" or unexpected symbol in \"blah\"\n\nYou have a typo somewhere in your code. Check the line of code producing the error to make sure it is formatted correctly. The most common typos that cause this error are a missing comma or closing parenthesis, but there are several other typos that can cause similar errors.\n\n\nunused argument (var = \"blah\")\nYou have used an argument name in a function that does not understand it. Check the manual page for that function.\n\n\n\nargument \"blah\" is missing, with no default\n\nYou have used a function without providing all the necessary arguments. Check the manual page for that function.\n\n\n\nThe pipe operator requires a function call as RHS\n\nThere is a pipe operator at the end of the final line of a code pipeline (or what R thinks should be the final line)."
  },
  {
    "objectID": "07_handling_bugs/index.html#getting-external-help",
    "href": "07_handling_bugs/index.html#getting-external-help",
    "title": "\n7  Handling bugs in your code\n",
    "section": "\n7.6 Getting external help",
    "text": "7.6 Getting external help\nIf you cannot fix an error using any of the techniques we have already covered, it is probably time to get some help from others. Fortunately, one of the big benefits of using R is that there is a friendly, welcoming community of R coders online who are willing to help fix problems. Almost everyone in this community will remember when they were knew to using R and so will be gentle with people who are asking their first question.\n\n7.6.1 Reproducible examples\n\nOne of the things that makes it much more likely that you will find help with your problem in the R community is if you phrase your plea for help in a way that makes it easier to help you. We can do this by providing a reproducible example or reprex of our problem (also sometimes called a minimum working example).\nProducing a reprex makes it much easier for someone to understand your issue. This not only makes it easier for someone to help you, but also shows that you know it will make it easier for them and that you value their time.\n\nImagine that you’ve made a cake, and for some reason it’s turned out absolutely awful – we’re talking completely inedible. Asking a question without a reprex is like asking, “Why didn’t my cake turn out right?” – there are hundreds of possible answers to that question, and it’s going to take a long time to narrow in on the exact cause for your inedible cake creation.\nAsking a question with a reprex is like asking, “My cake didn’t turn out, and here’s the recipe I used and the steps that I followed. Where did I go wrong?” Using this method is going to significantly increase the likelihood of you getting a helpful response, faster!\n\nTo make a reprex, we have to do two things:\n\nRemove everything from our code that does not contribute to causing the error. We do this by removing each line from our code and only keeping those lines that are necessary to produce the error – this is why a reproducible example is sometimes called a minimum working example.\nMake sure that someone trying to help us can reproduce the issue on their own computer even if they don’t have access to the particular dataset we are using. We do this by replacing our own dataset with a publicly available one, preferably one of the datasets that are built into R for exactly this purpose.\n\nWatch this video to see the process of making a reprex in RStudio:\n\n\n7.6.2 Reproducible code\nOur first step is to remove every line from our code that isn’t necessary to produce the error. For example, run the following code to see what error it produces.\n\n\n\n\n# Load packages\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(readr)\n\n# Load a dataset from your computer and wrangle it\nroad_deaths <- read_csv(\"road_deaths_data.csv\") |> \n  janitor::clean_names() |> \n  rename(ksi_drivers = drivers, ksi_pass_front = front, ksi_pass_rear = rear) |> \n  select(-petrol_price, -van_killed) |> \n  mutate(\n    law = as.logical(law),\n    ksi_driver_rate = ksi_drivers / (kms / 1000)\n  )\n## Rows: 192 Columns: 9\n## ── Column specification ────────────────────────────────────────────────────────\n## Delimiter: \",\"\n## dbl  (8): DriversKilled, drivers, front, rear, kms, PetrolPrice, VanKilled, law\n## date (1): month_beginning\n## \n## ℹ Use `spec()` to retrieve the full column specification for this data.\n## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n# Make a time-series chart of two continuous variables, coloured by a \n# categorical variable, then add a trend line\nroad_deaths + \n  ggplot(aes(x = month_beginning, y = ksi_driver_rate)) + \n  geom_point(aes(colour = law)) + \n  geom_smooth() +\n  scale_x_date(date_breaks = \"2 years\", date_labels = \"%Y\") +\n  scale_y_continuous(labels = scales::comma_format(), limits = c(0, NA)) +\n  scale_colour_brewer(type = \"qual\") +\n  labs(\n    x = NULL, \n    y = \"drivers killed or seriously injured per 1,000km travelled\", \n    colour = \"after seat belts made mandatory\"\n  ) +\n  theme_minimal() +\n  theme(\n    axis.line.x = element_line(colour = \"grey90\"),\n    axis.ticks = element_line(colour = \"grey90\"),\n    panel.grid.major.x = element_blank(),\n    panel.grid.minor.x = element_blank(),\n    legend.position = \"bottom\"\n  )\n## Error in `fortify()`:\n## ! `data` must be a <data.frame>, or an object coercible by `fortify()`,\n##   not an S3 object with class <uneval>.\n## ℹ Did you accidentally pass `aes()` to the `data` argument?\n\nAs you can see, this code produces an error message that is not easy to decipher, so we might need help to deal with it. As the first stage to making a reprex, remove all the lines of the code above that it is possible to remove while still producing the same error. If the error disappears and the chart is produced successfully, you have probably found the line that contains the error. If you remove a line and the code starts to produce a different error, you have removed a line that is needed to produce the original error and should put that line back into the code.\nIt is actually possible to remove a lot of the original code and still produce the same error. You can remove:\n\nAll the lines of code that fine-tune the appearance of the chart (lines 23–38 above).\nThe line creating the trend line (line 22).\nThe code that wrangles our data in ways that don’t affect the error (lines 9–15).\nThe line that loads the dplyr package (line 2), since we are no-longer using the data wrangling functions from that package.\n\nWe cannot remove the code that loads necessary packages (lines 3 and 5), loads the data (line 8) or produces the basic unformatted chart (lines 19–21), because if we remove any of those then the error message changes or disappears.\nThis leaves us with the following code, which produces the same error message but is much easier for someone to check for errors because it is much shorter. Because we have removed the data-wrangling code, we have had to change the name of the argument on line 20 of the code above from y = ksi_driver_rate to y = drivers, since the column ksi_driver_rate is no longer in the data.\n\nIf we forgot to change y = ksi_driver_rate to y = drivers then the code would still produce an error, but it would be a different error. The purpose of producing a reprex is to find the minimum code that still produces the same error we are interested in. If you remove a line of code and the error message you see changes, put that line of code back.\n\nIf this shortened code were to run successfully then the resulting chart would look quite different to the original chart we wanted, but that does not matter because what we are interested in is showing code that produces a specific error. If you run this code, you will see it produces the same error as the code above.\n\n# Load packages\nlibrary(ggplot2)\nlibrary(readr)\n\n# Load a dataset from your computer and wrangle it\nroad_deaths <- read_csv(\"road_deaths_data.csv\")\n## Rows: 192 Columns: 9\n## ── Column specification ────────────────────────────────────────────────────────\n## Delimiter: \",\"\n## dbl  (8): DriversKilled, drivers, front, rear, kms, PetrolPrice, VanKilled, law\n## date (1): month_beginning\n## \n## ℹ Use `spec()` to retrieve the full column specification for this data.\n## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n# Make a time-series chart of two continuous variables, coloured by a \n# categorical variable, then add a trend line\nroad_deaths +\n  ggplot(aes(x = month_beginning, y = drivers)) + \n  geom_point(aes(colour = law))\n## Error in `fortify()`:\n## ! `data` must be a <data.frame>, or an object coercible by `fortify()`,\n##   not an S3 object with class <uneval>.\n## ℹ Did you accidentally pass `aes()` to the `data` argument?\n\n\n7.6.3 Reproducible data\nOur shortened code would make a great reproducible example except for one thing: the data file road_deaths_data.csv only exists on our computer. This means the example is not actually reproducible, since anyone trying to run this code on their computer to identify the error would find that they instead got a different error saying that the file road_deaths_data.csv was not found.\nYou could deal with this by uploading your dataset to a website and then having the read_csv() function read it from that URL. But you might not want to share your data (perhaps it is sensitive in some way), or your dataset might be too large to post online. For this reason, many R packages come with toy datasets that can be used in learning or in testing for errors. You can see a list of all the toy datasets available in the packages you have loaded by typing data() in the R console. This will produce a file that gives the name and description of each available dataset.\nTo use one of these toy datasets, you just use the the name of the dataset as you would use any other R object (like the road_deaths object we created above). One commonly used toy dataset is the mpg dataset from the ggplot2 package, which contains fuel economy data for 38 models of car.\nThe data in this dataset are on a completely different topic to the data we were trying to use, but this does not matter as long as the data contains variables of the same type (numeric, character, etc.) as the original data. We can see what variables are in the mpg dataset using the head() function as usual.\n\nlibrary(ggplot2)\n\nhead(mpg)\n## # A tibble: 6 × 11\n##   manufacturer model displ  year   cyl trans      drv     cty   hwy fl    class \n##   <chr>        <chr> <dbl> <int> <int> <chr>      <chr> <int> <int> <chr> <chr> \n## 1 audi         a4      1.8  1999     4 auto(l5)   f        18    29 p     compa…\n## 2 audi         a4      1.8  1999     4 manual(m5) f        21    29 p     compa…\n## 3 audi         a4      2    2008     4 manual(m6) f        20    31 p     compa…\n## 4 audi         a4      2    2008     4 auto(av)   f        21    30 p     compa…\n## 5 audi         a4      2.8  1999     6 auto(l5)   f        16    26 p     compa…\n## 6 audi         a4      2.8  1999     6 manual(m5) f        18    26 p     compa…\n\nFrom this, we can see that there is a year variable that we can use as a substitute for the month_beginning variable in our original code, a variable called hwy that is numeric and so can be substituted for the drivers variable in our code, and a categorical variable called trans that we can substitute for the law variable in our data. This means we can use this data instead of our own data, knowing that anyone on any computer with the ggplot2 package installed can run the code and should get the same result.\n\nChange the code below so that it uses the mpg dataset rather than loads data from the road_deaths_data.csv file. You will need to change the variable names in the stack of ggplot() functions.\n\n# Load packages\nlibrary(ggplot2)\nlibrary(readr)\n\n# Load a dataset from your computer and wrangle it\nroad_deaths <- read_csv(\"road_deaths_data.csv\")\n## Rows: 192 Columns: 9\n## ── Column specification ────────────────────────────────────────────────────────\n## Delimiter: \",\"\n## dbl  (8): DriversKilled, drivers, front, rear, kms, PetrolPrice, VanKilled, law\n## date (1): month_beginning\n## \n## ℹ Use `spec()` to retrieve the full column specification for this data.\n## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n# Make a time-series chart of two continuous variables, coloured by a \n# categorical variable\nroad_deaths +\n  ggplot(aes(x = month_beginning, y = drivers)) + \n  geom_point(aes(colour = law))\n## Error in `fortify()`:\n## ! `data` must be a <data.frame>, or an object coercible by `fortify()`,\n##   not an S3 object with class <uneval>.\n## ℹ Did you accidentally pass `aes()` to the `data` argument?\n\n\n\nWe can change our existing reprex code to use the mpg dataset rather than load data from the road_deaths_data.csv file by changing the variable names in the ggplot() stack.\n\n\n# Load packages\nlibrary(ggplot2)\n\n# Make a time-series chart of two continuous variables, coloured by a \n# categorical variable\nmpg +\n  ggplot(aes(x = year, y = hwy)) + \n  geom_point(aes(colour = trans))\n## Error in `fortify()`:\n## ! `data` must be a <data.frame>, or an object coercible by `fortify()`,\n##   not an S3 object with class <uneval>.\n## ℹ Did you accidentally pass `aes()` to the `data` argument?\n\nWe have now managed to reduce our original 37 lines of code down to 8 lines, as well as making the example reproducible by using a widely available toy dataset. The shorter code still produces the same error while being much easier to read, so we are much more likely to get help quickly than if we had just sent someone our original code.\nMost of the time, the act of producing a reprex will be enough for us to find and fix the error without any external help. Can you see the problem with our code that is making this error happen? If not, we will reveal it at the end of this tutorial.\n\n7.6.4 Checking your reprex is reproducible\nNow that you have the minimum code needed to reproduce the error, it’s almost time to share it with people who can help you. But before you do that, it’s worth checking that the code is truly reproducible. To do this we will use the reprex package, which is part of the tidyverse suite of packages you already have installed.\n\n\n\nTo use the reprex package, first put your code in a separate R document in the Source panel in RStudio. Open a new R script in RStudio now and paste the code from the last exercise into it. Once you’ve done that, select all the code in that document. Now click the Addins button in RStudio and scroll down until you can choose Reprex selection.\nAfter a few seconds, some code should appear in the RStudio Viewer panel showing your code and the error message that it produces. This code has also been copied to your computer clipboard so that you can paste it into an email or web form when you are asking for help.\nIf the error message that you see along with the code in the Viewer panel is not the error message you were expecting, your example is not yet reproducible. For example if you tried to run the Reprex selection command on the original code that we started this section with, we would get an error message 'road_deaths_data.csv' does not exist in current working directory.\nOnce your reprex produces the same error as the code you originally had the issue with, you’re ready to share it to get help.\n\nStats Illustrations by Allison Horst licensed under the Creative Commons Attribution licence."
  },
  {
    "objectID": "07_handling_bugs/index.html#sources-of-help",
    "href": "07_handling_bugs/index.html#sources-of-help",
    "title": "\n7  Handling bugs in your code\n",
    "section": "\n7.7 Sources of help",
    "text": "7.7 Sources of help\nIf you are being taught R by a formal instructor, or you have friends or colleagues who can help, they will probably be the first people that you go to for help.\nIf this doesn’t work, or if you are the most proficient R user that you know, you might need another place to turn to. Fortunately, R has a large community of volunteers who will help you. Before you ask people online for help, it’s important to check that someone hasn’t already asked the same question and had it answered. Duplicate questions increase the workload of the volunteers who answer questions and slow everything down, so if your question has frequently been answered already it’s possible your question will just be ignored.\nTo find out if there is an answer to your question, the easiest thing to do is to search the error message online. Google, or another search engine of your choice, is definitely your friend. If you search online for the error message that was produced by our reprex code, you will see that there are over 100 pages discussing this error message and how to fix it.\n\n7.7.0.1 Stack Overflow\nLet’s imagine, though, that there were no relevant hits when we searched for the error message, or that none of the results was useful. In that case, we need to pose a new question to the R community. The place to find the largest slice of that community is probably the website Stack Overflow. This is a website for people who are writing code in any programming language imaginable to get help. It is part of the larger Stack Exchange Network of question-and-answer websites covering everything from travel to veganism.\nTo ask a new question on Stack Overflow, go to stackoverflow.com/questions/ask and create an account or log in. You will now be asked to complete a short form with your question. Questions are more likely to get an answer faster if you:\n\nGive the question a specific title. Over 20 million questions have been asked on Stack Overflow since it launched, so a generic title like ‘Help’, ‘R error’ or even ‘ggplot error’ will not help other people find your question. Look at some recent questions about R on Stack Overflow to get some ideas on what title to give for your question.\nIn the body of your question, briefly (2–3 lines should do it) explain what you were trying to do, then paste the reprex output that the Reprex selection addin copied to your clipboard into the question body box underneath your brief explanation. You will see that Stack Overflow recognises the format of your code and shows you a preview of it underneath the question box.\nFinally, add the tag r to the Tags box so that people know your question is about coding in R. This is crucial – if you do not tag your question as being about R, it is very unlikely that volunteers who know about R will be able to find your question.\n\nSubmit your question and wait for an answer. As soon as someone answers your question, or comments on it to ask for more detail, you will get an email alert. Many questions are typically answered within a few hours. Hopefully this will help you get to the final stage of the emotional roller coaster of debugging:\n\n\n\nStats Illustrations by Allison Horst licensed under the Creative Commons Attribution licence."
  },
  {
    "objectID": "07_handling_bugs/index.html#in-summary",
    "href": "07_handling_bugs/index.html#in-summary",
    "title": "\n7  Handling bugs in your code\n",
    "section": "\n7.8 In summary",
    "text": "7.8 In summary\n\n7.8.0.1 A workflow for handling errors in R\n\nWhen you run some code in R and it produces an error, follow the steps learned in this tutorial to identify and fix the problem.\n\nRead the error message and fix any obvious problems.\nMake sure your code follows the style guide introduced in the previous tutorials, which will make it easier to see where there might be problems, especially those caused by typos. In particular, make sure that each function is on a separate line so you can comment them out individually.\nComment out all the lines of your code. Uncomment each line in turn (starting with the first line), re-running the code each time until you see the error you’re trying to understand. Now you know which line is likely to be causing the problem.\nIf the error message is one of the common errors explained in this tutorial, follow the steps outlined in the previous section to fix it.\nIf that does not fix the problem, read the manual page for the function that you think is causing the problem. Check that the values you have passed to each argument are what the function needs in order to work.\nIf that does not fix the problem, create a reproducible example by removing all the parts of your code not needed to create the error. Often, the act of creating a reproducible example will help you fix the problem, but if not then you can use it to get help from others.\n\nYou will be able to fix almost all the errors in your code by following the early steps in this list. Only quite rarely will you need to post a reprex online to get help.\n\n\n7.8.0.2 What caused the error in our reproducible example?\nThe error in our reproducible example was very simple, but quite difficult to spot. On line 7 of the code below, we try to add the ggplot() function to the mpg object using the + operator when what we wanted to do was pass the mpg object to the ggplot() function using the |> operator. R does not know how to add a dataset to a function in this way so it produced an error message.\nIf you replaced + with |> on line 6 of the code below, the code would now run normally. Since we have removed almost all of our original code to make a reproducible example, the resulting plot looks nothing like what we wanted. This does not matter – when we are producing a reprex we only care about reliably producing the same error. Now that we have fixed the error, we could go back and fix the original code to produce the chart we wanted.\n# load packages\nlibrary(ggplot2)\n\n# make a time-series chart of two continuous variables, coloured by a \n# categorical variable\nmpg + # <---- THE `+` OPERATOR HERE SHOULD BE A `|>` OPERATOR INSTEAD\n  ggplot(aes(x = year, y = hwy)) + \n  geom_point(aes(colour = trans))\nThe mistake in this code is a very easy one to make, because what + does inside a ggplot() stack is so similar to what |> does in passing the result of one function to the next function in an data-wrangling pipeline. Remember that we only use + to combine functions inside a ggplot() stack, and use |> to combine functions everywhere else.\n\nIn this tutorial we have learned how to handle messages, warnings and errors in R. We have learned to take time to understand error messages, to isolate errors so that we can better understand them, to use manual pages for functions to check that every argument is correct, and how to write reproducible examples so that we can get help online. This will help you become a more independent coder.\n\n\nFor more information on writing reproducible examples, see:\n\nWatch the webinar Creating reproducible examples with reprex by Jenny Bryan.\nRead the Reprex do’s and don’ts on the reprex package website."
  },
  {
    "objectID": "appendices/read_functions.html",
    "href": "appendices/read_functions.html",
    "title": "Appendix A — Functions for reading data into R",
    "section": "",
    "text": "R can read data in many different formats. Different functions (often from different packages) are needed to read files of different formats. These are some of the functions needed to read common types of file used in data analysis, including spatial data formats.\n\n\n\n\n\n\nFunctions for reading data into R\n  \n  \nData file type\n      Package\n      Function\n      Can load compressed files?\n      Can load files from URL?\n    \n\n\n\nComma-separated values (.csv)\n\n\nreadr\n\n\nread_csv() (data from English-speaking countries) or read_csv2() (data from elsewhere)\n\n\nyes\n\n\nyes\n\n\n\n\nFixed-width files (usually .txt)\n\n\nreadr\n\n\nread_fwf()\n\n\nyes\n\n\nyes\n\n\n\n\nGeoJSON (.geojson)\n\n\nsf\n\n\nread_sf()\n\n\nno\n\n\nyes\n\n\n\n\nGeoPackage (.gpkg)\n\n\nsf\n\n\nread_sf()\n\n\nno\n\n\nyes\n\n\n\n\nGoogle Sheets\n\n\ngooglesheets4\n\n\nread_sheet()\n\n\nn/a\n\n\nyes\n\n\n\n\nHTML (.htm or .html)\n\n\nxml2\n\n\nread_html() (probably used with functions from the rvest package)\n\n\nn/a\n\n\nyes\n\n\n\n\nJSON (.json)\n\n\njsonlite\n\n\nread_json()\n\n\nyes\n\n\nyes\n\n\n\n\nMicrosoft Excel (.xlsx or .xls)\n\n\nreadxl\n\n\nread_excel()\n\n\nno\n\n\nno\n\n\n\n\nOpenDocument Spreadsheet (.ods)\n\n\nreadODS\n\n\nread_ods()\n\n\nno\n\n\nyes\n\n\n\n\nR Data (.rds)\n\n\nreadr\n\n\nread_rds()\n\n\nn/a\n\n\nyes\n\n\n\n\nSAS (.sas7bdat)\n\n\nhaven\n\n\nread_sas()\n\n\nyes\n\n\nyes\n\n\n\n\nShapefile (.shp)\n\n\nsf\n\n\nread_sf()\n\n\nno\n\n\nno\n\n\n\n\nSPSS Statistics (.sav)\n\n\nhaven\n\n\nread_sav()\n\n\nyes\n\n\nyes\n\n\n\n\nStata (.dta)\n\n\nhaven\n\n\nread_dta()\n\n\nyes\n\n\nyes\n\n\n\n\nTab-separated values (.tsv or .tab)\n\n\nreadr\n\n\nread_tsv()\n\n\nyes\n\n\nyes\n\n\n\n\nXML (usually .xml)\n\n\nxml2\n\n\nread_xml()\n\n\nyes\n\n\nyes"
  }
]