[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Learn crime mapping with R",
    "section": "",
    "text": "Welcome!\nThis book will give you the knowledge and skills to effectively communicate information about crime and related topics using maps. We will cover the principles of analysing geographic information and the strengths and weaknesses of different maps for communicating it.",
    "crumbs": [
      "Welcome!"
    ]
  },
  {
    "objectID": "index.html#who-is-this-book-for",
    "href": "index.html#who-is-this-book-for",
    "title": "Learn crime mapping with R",
    "section": "Who is this book for?",
    "text": "Who is this book for?\nThis book is for you if you are:\n\nSomeone who wants to learn how to understand patterns of crime using maps and related forms of data analysis.\nSomeone who already knows how to map crimes and would like to learn how to programme in a familiar context.",
    "crumbs": [
      "Welcome!"
    ]
  },
  {
    "objectID": "index.html#why-use-this-book",
    "href": "index.html#why-use-this-book",
    "title": "Learn crime mapping with R",
    "section": "Why use this book?",
    "text": "Why use this book?\nThere are several books available for learning crime mapping. The advantages of using this book are that:\n\nIt teaches up-to-date crime mapping techniques. Some of the most-popular books on crime mapping were written over a decade ago and do not reflect substantial developments in the field since then.\nIt teaches crime mapping using exclusively free software (R and RStudio). Some other books teach crime mapping in expensive proprietary software that most people who need to make crime maps do not have access to.\nIt is an online book, so includes videos to introduce theoretical concepts and walk you through the process of creating different maps.\nIt uses examples from across the world, so it is not only useful to readers from one country.\nIt’s completely free to read and redistribute!",
    "crumbs": [
      "Welcome!"
    ]
  },
  {
    "objectID": "index.html#why-learn-crime-mapping-in-r",
    "href": "index.html#why-learn-crime-mapping-in-r",
    "title": "Learn crime mapping with R",
    "section": "Why learn crime mapping in R?",
    "text": "Why learn crime mapping in R?\nWe could make crime maps in several different apps. This includes commercial geographic information systems such as ArcGIS or MapInfo, free software such as QGIS, and data analysis programmes such as Tableau. So why learn crime mapping in a programming language like R?\nThere are several reasons:\n\nMaking maps using a programming language makes your work much more efficient, especially if (as is common in crime analysis) you need to produce similar maps periodically using updated data, or need to produce multiple similar maps for different areas or crime types.\nR is free. This can be important for people working in agencies with very limited budgets for software for data analysis.\nR has extensive mapping capabilities, supported by a large team of friendly experts who provide online support.\nR is good for other types of data analysis, so everything you learn here can be used for analysing crime data using techniques that are unrelated to maps.\n\nLearning a programming language like R involves a little extra work at the start, but this book is written to make this as easy as possibly by gently introducing you to programming ideas a little at a time. Once you’ve got started, you’ll find mapping crime in this way makes it much easier to advance further in the future.",
    "crumbs": [
      "Welcome!"
    ]
  },
  {
    "objectID": "index.html#how-to-use-this-book",
    "href": "index.html#how-to-use-this-book",
    "title": "Learn crime mapping with R",
    "section": "How to use this book",
    "text": "How to use this book\nThere are two ways to use this book.\n\nYou can read it as you would any other technical how-to book, by reading through each chapter and working through each exercise in RStudio on your computer or online.\nYou can work through each chapter as an interactive tutorial in RStudio, getting immediate feedback on each exercise. The interactive tutorials also include quizzes to help you check your understanding.\n\nIf you want to use the interactive tutorials, there are a few extra set-up steps to work through at the bottom of this page. They will only take a few minutes.\nI recommend using the interactive tutorials if you can. The interactive tutorials are used for teaching crime mapping to BSc Crime and Security Science students at University College London so they have been tested by hundreds of people learning crime mapping before you.",
    "crumbs": [
      "Welcome!"
    ]
  },
  {
    "objectID": "00_setup/index.html#step-1-install-r",
    "href": "00_setup/index.html#step-1-install-r",
    "title": "Install the software needed for this book",
    "section": "Step 1: install R",
    "text": "Step 1: install R\nThe first step is to download and install R, a programming language designed for analysing and visualising data, including making maps. To install R, visit the R website and download R for either Windows or Mac, depending on what type of computer you are using. If you already have R installed on your computer, please update it to the latest release.\nThis video talks you through the process of installing R:",
    "crumbs": [
      "Install the software needed for this book"
    ]
  },
  {
    "objectID": "00_setup/index.html#step-2-install-rstudio",
    "href": "00_setup/index.html#step-2-install-rstudio",
    "title": "Install the software needed for this book",
    "section": "Step 2: install RStudio",
    "text": "Step 2: install RStudio\nThe next step is to download RStudio, an app that you can use to work with the R programming language more efficiently. Download RStudio Desktop for your computer from the Posit website (that’s the company that makes RStudio) and install. If you already have RStudio Desktop installed on your machine, please update it to the latest release.\nThis video talks you through the process of installing RStudio:",
    "crumbs": [
      "Install the software needed for this book"
    ]
  },
  {
    "objectID": "00_setup/index.html#step-3-install-rtools-windows-only",
    "href": "00_setup/index.html#step-3-install-rtools-windows-only",
    "title": "Install the software needed for this book",
    "section": "Step 3: install RTools (Windows only)",
    "text": "Step 3: install RTools (Windows only)\nIf you are using a Windows computer you should install Rtools, which will be needed by RStudio for some tutorials. If you are using a Mac or Linux computer, you do not need to install Rtools.\nTo install RTools:\n\nDownload the latest version from the R website and open the downloaded file.\nFollow the installation instructions (accept all the default options).\nOpen RStudio.\nFind the panel (in the bottom-left) marked Console.\nFind the &gt; symbol at the bottom of that panel.\nCopy and paste the following code to the right of the &gt; symbol:\n\nwrite('PATH=\"${RTOOLS40_HOME}\\\\usr\\\\bin;${PATH}\"', file = \"~/.Renviron\", append = TRUE)\n\nPress Enter.",
    "crumbs": [
      "Install the software needed for this book"
    ]
  },
  {
    "objectID": "00_setup/index.html#if-you-cannot-install-software-on-your-computer",
    "href": "00_setup/index.html#if-you-cannot-install-software-on-your-computer",
    "title": "Install the software needed for this book",
    "section": "If you cannot install software on your computer",
    "text": "If you cannot install software on your computer\nYou may not be able to install software on the computer you want to use for crime mapping, or you may prefer not to for various reasons. In that case, you can run RStudio online using Posit Cloud. Posit Cloud is free for a certain number of hours each month, after which you can pay to continue using it.\nUsing Posit Cloud allows you to avoid the installation steps for R, RStudio and RTools above, but remember that since Posit Cloud operates online, you should not use it work on confidential or personal data unless you have the necessary permission to do so.",
    "crumbs": [
      "Install the software needed for this book"
    ]
  },
  {
    "objectID": "00_setup/index.html#running-the-interactive-tutorials",
    "href": "00_setup/index.html#running-the-interactive-tutorials",
    "title": "Install the software needed for this book",
    "section": "Running the interactive tutorials",
    "text": "Running the interactive tutorials\nIf you are going to use this book as you would any other technical how-to book, you can move ahead to the next chapter now. If you are going to use the interactive tutorials that accompany this book, there is one final step to set them up:\n\nOpen RStudio.\nFind the panel (in the bottom-left) marked Console.\nFind the &gt; symbol at the bottom of that panel.\nCopy and paste the following code to the right of the &gt; symbol:\n\nsource(\"https://github.com/mpjashby/crimemapping/raw/main/inst/initialise.R\")\n\nPress Enter.\n\n\nIt will take a few minutes for the tutorials to be set up. Once the process is complete you will see a message telling you this.",
    "crumbs": [
      "Install the software needed for this book"
    ]
  },
  {
    "objectID": "01_getting_started/index.html#welcome",
    "href": "01_getting_started/index.html#welcome",
    "title": "\n1  Getting started\n",
    "section": "\n1.1 Welcome",
    "text": "1.1 Welcome\n\nWelcome to this course on crime mapping! This course uses interactive tutorials like this one to help you learn about using maps and spatial analysis techniques to understand crime. Watch this video to learn more about the course.\n\n\nWelcome to Learn Crime Mapping with R. This book will help you learn about using maps and spatial analysis techniques to understand crime. Watch this video to learn more.\n\n\n\nNow click Next Topic below to start this tutorial.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Getting started</span>"
    ]
  },
  {
    "objectID": "01_getting_started/index.html#why-put-crimes-on-maps",
    "href": "01_getting_started/index.html#why-put-crimes-on-maps",
    "title": "\n1  Getting started\n",
    "section": "\n1.2 Why put crimes on maps?",
    "text": "1.2 Why put crimes on maps?\nThis course is about how we can use maps and other spatial analysis tools to help understand, prevent and respond to crime. Watch this video to understand why spatial analysis is a useful tool for understanding crime.\n\n\nWeisburd, D. (2015). The law of crime concentration and the criminology of place. Criminology, 53(2), 133-157.\nJohnson, S. (2010). A brief history of the analysis of crime concentration. European Journal of Applied Mathematics, 21(4-5), 349.\nFarrell, G. (2015). Crime concentration theory. Crime Prevention and Community Safety, 17(4), 233-248.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Getting started</span>"
    ]
  },
  {
    "objectID": "01_getting_started/index.html#why-is-crime-concentrated-in-space",
    "href": "01_getting_started/index.html#why-is-crime-concentrated-in-space",
    "title": "\n1  Getting started\n",
    "section": "\n1.3 Why is crime concentrated in space?",
    "text": "1.3 Why is crime concentrated in space?\nWhy is crime concentrated in space? Watch this video to find out more about how our environment influences opportunities for crime and how that causes clusters of different crimes.\n\n\nSantos, R. B. (2015). Routine Activity Theory: A Cornerstone of Police Crime Analyst Work. In The Criminal Act: \nCohen, L. E., and Felson, M. (1979). Social Change and Crime Rate Trends: A Routine Activity Approach. American Sociological Review, 44(4), 588–608.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Getting started</span>"
    ]
  },
  {
    "objectID": "01_getting_started/index.html#finding-your-way-around-rstudio",
    "href": "01_getting_started/index.html#finding-your-way-around-rstudio",
    "title": "\n1  Getting started\n",
    "section": "\n1.4 Finding your way around RStudio",
    "text": "1.4 Finding your way around RStudio\nWe will use RStudio for almost all of this course. Watch this video to find your way around the different panels in the RStudio window.\n\n\n1.4.1 Slightly adjusting how RStudio works\nBefore we start using RStudio, we should make a few changes to how it is set up that will make it easier to fix any mistakes we make while coding. To do this, click on the Tools menu in RStudio and then on Global Options…. In the dialogue box that opens, click on General in the left-hand panel if General is not selected already.\n\n\nIn the “Workspace” section of the right-hand panel, find an option that says “Restore .RData into workspace at startup” and make sure the check box to the left of that option is not checked. On the next line down, click the drop-down menu labelled “Save workspace to .RData on exit:” and choose the option Never. Click Apply and then OK to close the dialogue box.\n\nThe RStudio IDE Cheat Sheet highlights some of the features available in RStudio and gives a list of available keyboard short-cuts.\nWriting Code in RStudio is a webinar that talks you through RStudio in more detail.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Getting started</span>"
    ]
  },
  {
    "objectID": "01_getting_started/index.html#navigating-these-tutorials",
    "href": "01_getting_started/index.html#navigating-these-tutorials",
    "title": "\n1  Getting started\n",
    "section": "\n1.5 Navigating these tutorials",
    "text": "1.5 Navigating these tutorials\nThe tutorials that make up this course include short chunks of R code that you can run directly in this window. To run the code in each chunk, just click the Run Code button in the top-right corner of each chunk. If you want to re-run a chunk of code, you can click the Start Over button in the top-left corner of each chunk. Some of the chunks will have pre-filled code for you to run, while for others you will be asked to type the code needed to complete a task based on what you have already learned.\nWhen you click Run Code, the output of the chunk will appear below it (there may be a delay of a few seconds for more-complicated chunks).\nTo try out using a code chunk, click Run Code below – you should see the message ‘Hello, world!’ printed below the box.\n\nmessage(\"Hello, world!\")\n\nHello, world!\n\n\n\nSome of the tutorials include boxes like this one that contain information that it is particularly important for you to know to avoid common mistakes in writing code. Pay special attention to these points and remember to ask questions if anything isn’t clear.\n\n\n\nMore information you might like to know &lt;- click here\n\n\nIn these tutorials you will also see lines marked ‘Extra detail’ that you can click on to find out more information about a particular issue. This is generally information that you do not need to know to complete the tutorial, but which might be useful in other circumstances or which might answer some questions that you have. You can skip these boxes if you want to, or come back to them later if you have a question.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Getting started</span>"
    ]
  },
  {
    "objectID": "01_getting_started/index.html#in-summary",
    "href": "01_getting_started/index.html#in-summary",
    "title": "\n1  Getting started\n",
    "section": "\n1.6 In summary",
    "text": "1.6 In summary\nNow that you know why crime mapping is useful for understanding crime, why crime is typically concentrated in space and how to find your way around RStudio, in the next tutorial we will produce our first crime map in R.\nIf you’re not feeling too confident at this point in the course, don’t worry – learning something new is always a bit of a roller coaster and there is lots of help available in subsequent tutorials.\n\n\n\nArtwork by @allison_horst",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Getting started</span>"
    ]
  },
  {
    "objectID": "02_your_first_crime_map/index.html#introduction",
    "href": "02_your_first_crime_map/index.html#introduction",
    "title": "\n2  Your first crime map\n",
    "section": "\n2.1 Introduction",
    "text": "2.1 Introduction\nIn this tutorial we will use R to produce a simple crime map. To do this we will skip over lots of the detail of how R works and what choices we should make in creating maps. We will return to all these details in future sessions, so for-now please don’t worry about understanding every single line of code. Everything will become clear as we work through the tutorials in this course.\nThe map we’re going to create shows the locations of four homicides in downtown Atlanta in 2019:\n\n\n\n\n\nTo start off with, watch this video that walks through the code needed to make this map. Don’t worry if there are things in the video that you don’t understand – the rest of this tutorial will explain each line of code in turn.\n\n\n2.1.1 Running code in this tutorial\nIn the video above you saw code being run in RStudio, but to save switching between this tutorial and the RStudio console, the rest of this tutorial includes short chunks of R code that you can run directly in this window. We will use these chunks of code to walk through the code we need to produce a map. To run the code in each chunk, just click the Run Code button in the top-right corner of each chunk. If you want to re-run a chunk of code, you can click the Start Over button in the top-left corner of each chunk.\nWhen you click Run Code, the output of the chunk will appear below it (there may be a delay of a few seconds for more-complicated chunks).\nTo try out using a code chunk, click Run Code below – you should see the message ‘Hello, world!’ printed below the box.\n\nmessage(\"Hello, world!\")\n\nHello, world!\n\n\nmessage() is a function – a piece of R code that does something. The thing that the messages() function does is simple: it prints a message on the screen.\nNow click Continue below to start this tutorial.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Your first crime map</span>"
    ]
  },
  {
    "objectID": "02_your_first_crime_map/index.html#loading-crime-data",
    "href": "02_your_first_crime_map/index.html#loading-crime-data",
    "title": "\n2  Your first crime map\n",
    "section": "\n2.2 Loading crime data",
    "text": "2.2 Loading crime data\n\n2.2.1 Loading packages\n\nBefore we can work with our data, we first load packages of functions for use in the analysis. To load these packages, click Run Code. This will produce various messages, all of which you can safely ignore for now.\n\n\nBefore we can work with our data, we first load packages of functions for use in the analysis. Loading packages will produce various messages, all of which you can safely ignore for now.\n\n\n# Load the R packages we need to analyse this data\nlibrary(ggspatial)\nlibrary(sf)\n\nLinking to GEOS 3.11.0, GDAL 3.5.3, PROJ 9.1.0; sf_use_s2() is TRUE\n\nlibrary(tidyverse)\n\n\n\nWhat do these messages mean?\n\n\nFor now you don’t need to worry about these messages, but if you really want to know what they mean …\n\n\n\n\nSome R packages make use of other apps and utilities on your computer. For example, the sf package makes use of a piece of software call GDAL that is used for managing spatial data. So that you know which version of GDAL is being used, sf prints a message telling you.\nThe tidyverse package itself loads several packages that are commonly used together for analysing data. When you load tidyverse, it will print a message telling you which packages it has loaded, along with the version number for each package. It also prints a message saying if any functions from the tidyverse packages have replaced (“masked”) any functions from packages that were previously loaded.\nIn general, R packages use start-up messages to remind you of information that is not likely to be critical to your work, but which it might be useful to know at some point in the future.\n\n\n\n2.2.2 Loading data\nThe first task in creating any crime map is to obtain the crime and other data necessary. In many cases preparing the data for analysis and mapping will be a substantial task, but in this case we are going to use some pre-prepared crime data together with a pre-drawn street map (which we will ask R to download automatically when it draws the final map).\nThe data we will use will be records of homicides in the Downtown neighbourhood of Atlanta, Georgia, in 2019. We can load the homicide data using the read_csv() function.\n\nClick the Run Code button to load the data.\n\n\n# Download the data directly from a URL and store it as an object\nhomicides &lt;- read_csv(\"https://mpjashby.github.io/crimemappingdata/downtown_homicides.csv\")\n\nRows: 4 Columns: 4\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): label\ndbl (3): report_number, longitude, latitude\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nThe read_csv() function loads data from a file and prints a message showing the name of each column in the data and the type of data (number, text etc.) in each column. Again, you can ignore this message for now.\n\nSometimes R has several functions that have similar names but do different things. That means it is very important to pay attention to which function you need in any particular set of circumstances. In this case we are using the read_csv() function, which does a slightly different thing from the similarly named read.csv() function (note the . instead of the _). Since read.csv() has several limitations that read_csv() does not have, we will always use read_csv() in this course.\n\nWe have stored the results of the read_csv() function in an R object called homicides. An object in R is anything that stores any type of data. There are many types of objects, but for this tutorial we don’t need to explore these in any more detail. All you need to remember for now is that objects store data and functions do things.\n\n2.2.3 Viewing the data\nTo check the data has been loaded correctly, we can view the loaded data using the head() function. By default, head() prints the first six rows of the data stored in an object.\n\nClick Run Code to view the data.\n\n\n# Display the data\nhead(homicides)\n\n# A tibble: 4 × 4\n  report_number label                                         longitude latitude\n          &lt;dbl&gt; &lt;chr&gt;                                             &lt;dbl&gt;    &lt;dbl&gt;\n1     190191530 \"400 W PEACHTREE ST NW\\n19 January @ 15:00\"       -84.4     33.8\n2     190570315 \"171 AUBURN AVE NE @CITY WALK APARTMENTS\\n26…     -84.4     33.8\n3     192160018 \"241 FORSYTH ST SW\\n 4 August @ 00:00\"            -84.4     33.7\n4     193302338 \"80 JESSE HILL JR DR SE @GRADY\\n26 November …     -84.4     33.8\n\n\nThe data contain four columns: a unique identifier for a homicide, a label describing when and where that homicide occurred, and the longitude and latitude of the homicide location. Depending on the width of your screen, you may need to click on the ‘▸’ symbol to view all the columns in the data. We can use this data to plot the homicides on a map.\n\nIn the code head(homicides), there are no quote marks around the word homicides.\nAlmost all programming languages will interpret words differently depending on whether they have quotes around them or not. In this case, if you type the code head(homicides) then R will print the first few rows of the data stored in the homicides object.\nOn the other hand, if you type the code head(\"homicides\") or head('homicides'), R will interpret this as an instruction to print the first few elements of the literal text ‘homicides’. Since the text ‘homicides’ contains only one element (more about that later), head(\"homicides\") will just print the word ‘homicides’.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Your first crime map</span>"
    ]
  },
  {
    "objectID": "02_your_first_crime_map/index.html#processing-the-data",
    "href": "02_your_first_crime_map/index.html#processing-the-data",
    "title": "\n2  Your first crime map\n",
    "section": "\n2.3 Processing the data",
    "text": "2.3 Processing the data\nBefore we can plot the data on a map, we have to complete some pre-processing steps. Having to process data before being able to analyse or visualise it is common in all types of data analysis, but spatial analysis often involves additional processing that takes account of the special features of spatial data.\n\n2.3.1 Converting the data into a spatial format\nTwo data-processing tasks are needed to produce this map. The first is to convert the data into a simple features or SF object, which is a special type of R object that can be used by functions that process spatial data. We will cover the details of the st_as_sf() function that converts our data into into an SF object later on.\n\nClick Run Code to convert the data into SF format.\n\n\n# Convert the data to a simple features object, which we can use in functions \n# that work on spatial data\nhomicides_sf &lt;- st_as_sf(\n  homicides, \n  coords = c(\"longitude\", \"latitude\"), \n  crs = \"EPSG:4326\"\n)\n\nWhen you run this code, it looks like nothing happened. This is because the results of the code are stored in the homicides_sf object. Do you remember what R code to use to view the first few rows of this object?\n\nType that code into the box below and click Run Code to view the data.\n\n\nIf you cannot remember how to view the contents of an object, you can click on the Solution button to get help.\nAs you go through these tutorials, try to avoid using the Solution button unless you have tried to input the correct code yourself. You will learn much more if you try to work out the answer by referring back to an earlier page in the tutorial or to your own notes.\nDon’t worry about getting the answer wrong – nothing bad will happen if you run the wrong code in this tutorial and you can have as many attempts as you like to get the answer right.\n\n\n# To view an object in R, use the `head()` function. To view the contents of the\n# homicides object, use this code:\nhead(homicides_sf)\n\nSimple feature collection with 4 features and 2 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: -84.39732 ymin: 33.74827 xmax: -84.38185 ymax: 33.76614\nGeodetic CRS:  WGS 84\n# A tibble: 4 × 3\n  report_number label                                              geometry\n          &lt;dbl&gt; &lt;chr&gt;                                           &lt;POINT [°]&gt;\n1     190191530 \"400 W PEACHTREE ST NW\\n19 January @ …  (-84.3876 33.76614)\n2     190570315 \"171 AUBURN AVE NE @CITY WALK APARTME… (-84.38185 33.75546)\n3     192160018 \"241 FORSYTH ST SW\\n 4 August @ 00:00\" (-84.39732 33.74827)\n4     193302338 \"80 JESSE HILL JR DR SE @GRADY\\n26 No… (-84.38198 33.75168)\n\n\nThe data looks identical to before running the function st_as_sf(), except that the two columns called longitude and latitude have disappeared and there is now an extra column called geometry. The geometry column is important because lots of functions in R can recognise that the geometry column represents a location on the surface of the earth that can be used to analyse and map data in space.\n\n2.3.2 Changing the data projection\nThe geometry column in the homicides_sf object represents locations on the surface of the earth using co-ordinates (pairs of numbers). In this case, the co-ordinates are expressed as longitudes and latitudes, but there are lots of other types of co-ordinates (known as co-ordinate reference systems).\nWe’ll learn more about co-ordinate reference systems in a future tutorial, but for now it’s enough to know that each different system has advantages and disadvantages. To make the homicide locations easier to add to a map, we are going to first transform the co-ordinates from longitudes and latitudes to a co-ordinate reference system that is specifically designed for mapping data for the US state of Georgia.\nTo do this, we will use the st_transform() function, together with a code representing the co-ordinate reference system we want to use (you don’t need to understand this code at this stage).\n\nhomicides_sf_trans &lt;- st_transform(homicides_sf, \"EPSG:26967\")\n\nhead(homicides_sf_trans)\n\nSimple feature collection with 4 features and 2 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 678630.6 ymin: 415608.5 xmax: 680065.5 ymax: 417588.4\nProjected CRS: NAD83 / Georgia West\n# A tibble: 4 × 3\n  report_number label                                             geometry\n          &lt;dbl&gt; &lt;chr&gt;                                          &lt;POINT [m]&gt;\n1     190191530 \"400 W PEACHTREE ST NW\\n19 January @ … (679535.4 417588.4)\n2     190570315 \"171 AUBURN AVE NE @CITY WALK APARTME… (680065.5 416402.8)\n3     192160018 \"241 FORSYTH ST SW\\n 4 August @ 00:00\" (678630.6 415608.5)\n4     193302338 \"80 JESSE HILL JR DR SE @GRADY\\n26 No… (680052.6 415983.6)\n\n\nAgain, the data looks almost identical, except that the values in the geometry column have changed (you don’t need to understand yet the details of how these numbers are different). Now that we’ve completed the data processing, we can go on to produce the map itself.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Your first crime map</span>"
    ]
  },
  {
    "objectID": "02_your_first_crime_map/index.html#draw-the-map",
    "href": "02_your_first_crime_map/index.html#draw-the-map",
    "title": "\n2  Your first crime map\n",
    "section": "\n2.4 Draw the map",
    "text": "2.4 Draw the map\nWe are now ready to produce our map of homicides in downtown Atlanta. So that people viewing the map will understand where the homicides occurred, we will plot the homicides on top of a base layer showing streets, parks and other geographic features obtained from an online web mapping service.\n\nClick Run Code to create the map.\n\n\nggplot(homicides_sf_trans) + \n  annotation_map_tile(type = \"osm\", zoom = 15, progress = \"none\") + \n  geom_sf_label(\n    aes(label = label), \n    lineheight = 1, \n    size = 2.5, \n    hjust = 1, \n    vjust = 0\n  ) + \n  geom_sf(colour = \"white\", fill = \"orangered1\", size = 4, shape = 21) + \n  scale_x_continuous(expand = expansion(mult = 0.5)) + \n  scale_y_continuous(expand = expansion(mult = 0.2)) + \n  labs(\n    title = \"Homicides in Downtown Atlanta, 2019\",\n    caption = \"Background map by OpenStreetMap\"\n  ) +\n  theme_void()\n\n\n\n\nYou can change the appearance of the map by changing various parts of the code above. For example, you could change the colour of the points that mark the homicides by changing the code fill = \"orangered1\" to fill = \"mediumblue\", or change the base map to a different style by changing the code type = \"osm\" to type = \"cartolight\".\n\nEach time you change part of the code, click Run Code to see what changes on the map.\nOnce you have finished experimenting with changing the appearance of the map, click Next Topic below.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Your first crime map</span>"
    ]
  },
  {
    "objectID": "02_your_first_crime_map/index.html#putting-the-code-together",
    "href": "02_your_first_crime_map/index.html#putting-the-code-together",
    "title": "\n2  Your first crime map\n",
    "section": "\n2.5 Putting the code together",
    "text": "2.5 Putting the code together\nNow we have walked through the different parts of the code, we can create a map from scratch in a single block of code. In this example, we will map homicides from in Glenrose Heights neighbourhood of Atlanta, and a different style of base map. Since the area covered by the map is derived from the data itself, the extent of the map will update automatically.\n\n# Load the R packages we need to analyse this data\nlibrary(ggspatial)\nlibrary(sf)\nlibrary(tidyverse)\n\n# Download the data directly from a URL and store it as an object\nhomicides &lt;- read_csv(\"https://mpjashby.github.io/crimemappingdata/glenrose_heights_homicides.csv\")\n\n# Convert the data to a simple features object, which we can use in functions \n# that work on spatial data\nhomicides_sf &lt;- st_as_sf(\n  homicides, \n  coords = c(\"longitude\", \"latitude\"), \n  crs = \"EPSG:4326\"\n)\n\n# Transform the data to a co-ordinate reference system for the state of Georgia\nhomicides_sf_trans &lt;- st_transform(homicides_sf, \"EPSG:26967\")\n\n# Plot the map\nggplot(homicides_sf_trans) + \n  annotation_map_tile(type = \"osm\", zoom = 15, progress = \"none\") + \n  geom_sf_label(\n    aes(label = label), \n    lineheight = 1, \n    size = 2.5, \n    hjust = 1, \n    vjust = 0\n  ) + \n  geom_sf(colour = \"white\", fill = \"mediumblue\", size = 4, shape = 21) + \n  scale_x_continuous(expand = expansion(mult = 1.5)) + \n  scale_y_continuous(expand = expansion(mult = 0.2)) + \n  labs(\n    title = \"Homicides in Glenrose Heights, 2019\",\n    caption = \"Background map by OpenStreetMap\"\n  ) +\n  theme_void()",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Your first crime map</span>"
    ]
  },
  {
    "objectID": "02_your_first_crime_map/index.html#in-summary",
    "href": "02_your_first_crime_map/index.html#in-summary",
    "title": "\n2  Your first crime map\n",
    "section": "\n2.6 In summary",
    "text": "2.6 In summary\n\nWell done – you have finished your first mapping tutorial. You may not have understood every line of code in this tutorial, but we will cover them all in more detail over the rest of this course. By the end of the tutorials, you will be able to write code like this to create many different types of crime map.\n\nIn this tutorial you have learned how to load data into R, prepare it for use in making a map and then used it to make your first crime map of this course.\nThe map we have produced in this tutorial is effective for showing the locations of just a few crimes, but is too limited to show more complicated patterns or larger datasets. In the following tutorials, we will learn how to produce more sophisticated maps and spatial analysis. We will also learn how each of the functions that we have used in this tutorial work.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Your first crime map</span>"
    ]
  },
  {
    "objectID": "03_data_wrangling/index.html#introduction",
    "href": "03_data_wrangling/index.html#introduction",
    "title": "\n3  Wrangling data\n",
    "section": "\n3.1 Introduction",
    "text": "3.1 Introduction\nA major step in using any data to make decisions or draw conclusions is data wrangling: the process of transforming data from the format in which we originally have it to the format needed to analyse and present it to our audience.\n\n\nStart off by watching this video that walks through the different steps in wrangling a dataset. We will cover all the steps in the video in more detail during the rest of this tutorial.\n\n\n3.1.1 Functions\nIn this tutorial we will learn how to wrangle data in R using functions – specialised pieces of code that do something to the data we give it. The code to use a function (sometimes called calling the function) has two parts: the function name followed by a pair of parentheses, inside which are zero or more arguments separated by commas. Arguments are a way of providing input that a function works on, or to fine-tune the way the function works (we will see many examples of this later). Remember that you can identify a function in R because the name will always have parentheses after it.\nOne basic R function is sqrt(), which calculates the square root of a number. The sqrt() function has only one argument: the number that we want to find the square root of.\n\nType sqrt(2) in the box below and click Run Code to calculate the square root of 2.\n\n\nsqrt(2)\n\n[1] 1.414214\n\n\nWhen you run code in R, by default R prints the output of your code – in this case, just the number 1.414214 (for now, you can ignore the number [1] in square brackets).\n\n3.1.2 Packages\nR contains thousands of different functions that do different things. A few functions are contained in the default installation of R that you have already installed (this is sometimes referred to as base R). But most functions are contained in packages, which are extensions to base R. Most packages focus on a particular type of data analysis, so that there are packages devoted to time-series analysis, testing whether events are clustered in particular places, network analysis and thousands of other tasks. Packages are often developed by experts in the field, and are typically updated to introduce new features.\nTo use a package in R, we must do two things:\n\n\ninstall the package, which we have to do just once on each computer we want to use, then\n\nload the package, which we have to do each time we restart R (which happens when we open RStudio or switch between projects).\n\nThe install.packages() function downloads and installs packages from the Comprehensive R Archive Network (universally known as CRAN), which contains about 20,300 different packages. Some packages that are still in the early stages of development are not available on CRAN, but all the packages we will use are there.\n\nSo to install (for example) the package called tidyverse, which we will use extensively in this tutorial, we would run the R code:\ninstall.packages(\"tidyverse\")\nWe only have to install a package once for each computer that we will use to run R, although we would have to do it again if we updated to a new version of R. Once a package is installed on our computer, we have to load it so that we can use it in our code. We load packages using the library() function, which should probably have been called load_package() but isn’t. So to load the tidyverse package, we run the code:\nlibrary(tidyverse)\nMany packages are focused on specialist tasks and so are only used occasionally, but a few packages are likely to be useful in almost all the code we write. Fortunately, packages can themselves load other packages, and all the main packages we need are themselves loaded by the tidyverse package. That is why you will often see library(tidyverse) at the top of R code in subsequent tutorials – that short line of code loads several packages containing hundreds of functions that we can use in data analysis.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Wrangling data</span>"
    ]
  },
  {
    "objectID": "03_data_wrangling/index.html#loading-data",
    "href": "03_data_wrangling/index.html#loading-data",
    "title": "\n3  Wrangling data\n",
    "section": "\n3.2 Loading data",
    "text": "3.2 Loading data\nBefore we can do anything with any data, we have to load it into R. In this course we will read tabular data in comma-separated values (CSV) and Excel formats, as well as spatial data in different formats (because there are lots of ways to store spatial data). We will learn how to read CSV and Excel data now, but leave loading spatial data until later.\nTabular data contains multiple columns where every column has the same number of rows. For example, crime data might have columns for the type of crime, date and address at which the crime occurred.\n\n\n\nCrime data in rectangular format\n\ntype\ndate\naddress\n\n\n\nhomicide\n11 Feb 2023\n274 Main St\n\n\nnon-residential burglary\n21 Sep 2023\n541 Station Rd\n\n\npersonal robbery\n24 Jan 2024\n10 North Av\n\n\n\n\n\n\n3.2.1 Loading CSV data\n\nData stored in CSV format is easy to load with the read_csv() function from the readr package. readr is one of the packages loaded by the tidyverse package, so all we need to do to use this package is include the code library(tidyverse) on the first line of our R script. We will use comments (lines of code beginning with #) to help explain as we go.\n# Load the tidyverse suite of packages, including the readr package \n# that contains the read_csv() function\nlibrary(tidyverse)\n\n# We can load data from a file in the same folder as our R script\nsan_fran_rob &lt;- read_csv(\"san_francisco_robbery.csv\")\n\n# Or another folder on your computer ('../' is short for the parent \n# folder of the current folder)\nsan_fran_rob &lt;- read_csv(\"../san_francisco_robbery.csv\")\n\n# Or directly from a file online\nsan_fran_rob &lt;- read_csv(\"http://example.com/san_francisco_robbery.csv\")\nIn each of these examples, the code stores the result of the read_csv() function in an object named san_fran_rob. Objects are places where we can store data. To create an object and store our data in it, we use the assignment operator &lt;- (a less-than sign followed by a dash). Continually typing &lt;- can be tedious, so in RStudio we can use the keyboard short cut Option+- (on Mac) or Alt+- (on Windows or Linux) to insert the complete operator.\n\nWhen choosing object names, it is important to remember that if you assign a value (such as the number 1 or the result of the function read_csv()) to an object name, R will overwrite any existing value of that object name. We can see this in a simple example:\none_to_ten &lt;- 1:10\none_to_ten &lt;- sqrt(2)\nIf we were to run this code, the object one_to_ten would not actually hold the numbers from one to ten, but instead the value 1.414214 (the square root of two). There is also no way to undo assignment of a value to an object, so once you have run the code one_to_ten &lt;- sqrt(2) it is not possible to recover any previous value that was assigned to the object one_to_ten.\n\nObjects come in several different types, with tabular data typically being stored as a data frame. The read_csv() function actually produces a modern variation on the data frame called (slightly strangely) a tibble, which makes use of some advances in how R handles data since the data-frame format was set 20 years ago. Tibbles behave just like data frames almost all of the time (so much so that people working with tibbles often call them data frames) except for a few occasions where they behave in a more-convenient way.\nWe can use read_csv() to load data from https://mpjashby.github.io/crimemappingdata/san_francisco_robbery.csv and store it in an object called san_fran_rob.\n\nUse the code above to help you, or click the Solution button to reveal the answer.\n\n\nsan_fran_rob &lt;- read_csv(\"https://mpjashby.github.io/crimemappingdata/san_francisco_robbery.csv\")\n\nRows: 951 Columns: 5\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (1): offense_type\ndbl  (3): uid, longitude, latitude\ndttm (1): date_time\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nIf the data are loaded successfully, R will list the columns in the data and the type of variable (numeric, date etc.) stored in each column. The format of this is somewhat esoteric, but if you are interested they are explained in the ‘Extra detail’ box below.\n\n\nWhat do the messages produced by read_csv() mean?\n\n\nBy default, the read_csv() function prints a message when it loads data to summarise the format of each data column. In the case of the san_fran_rob dataset, read_csv() tells us that:\n\nthere is one column called offense_type that contains character (chr) values,\nthere are three columns called uid, longitude and latitude containing numeric (dbl) values, and\nthere is one column called date_time that contains values stored as dates and times (dttm).\n\nThere are some other possible types of data, but we will learn about these later on. The numeric values are referred to as dbl values because they are stored in a format that can handle numbers that are not whole numbers (e.g. 123.456). This format for storing numbers is called the double-precision floating-point format, which is often known as the double format for short. Most numbers in R are stored in double format, so you can think of the format code dbl as meaning ‘numeric’.\n\n\nTo see the first few rows of data currently stored in an object, we can use the head() function.\n\nType the code needed to view the first few rows of the san_fran_rob object and click the Run Code button.\n\n\nhead(san_fran_rob)\n\n# A tibble: 6 × 5\n       uid offense_type     date_time           longitude latitude\n     &lt;dbl&gt; &lt;chr&gt;            &lt;dttm&gt;                  &lt;dbl&gt;    &lt;dbl&gt;\n1 24103841 personal robbery 2019-01-01 19:50:00     -122.     37.8\n2 24103948 personal robbery 2019-01-02 08:00:00     -122.     37.8\n3 24104162 personal robbery 2019-01-03 00:30:00     -122.     37.8\n4 24104203 personal robbery 2019-01-03 03:13:00     -122.     37.8\n5 24104237 personal robbery 2019-01-03 09:30:00     -122.     37.7\n6 24104238 personal robbery 2019-01-03 09:30:00     -122.     37.7\n\n\n\n3.2.2 Loading Excel data\n\nLoading data from Microsoft Excel files is very similar to loading CSV data, with a few important differences. Functions to load Excel data are contained in the readxl package, which was installed automatically when we installed the tidyverse package.\nThere are two main things we must do to import Excel data that are not required for importing CSV data. The first is that the readxl package cannot directly load files from a URL, instead only loading files that are present on your computer. To get round this, we will first download an Excel file and store it in a temporary directory (to avoid cluttering up our computers).\n\nUsing download.file() on Windows\nIf you are using a Windows computer, you may find that the download.file() function in the code below does not work as expected. This is because Windows handles files in a way that distinguishes between plain-text files such as .txt and .csv files and binary files, which includes most other file types (including compressed files). Since aggravated_assaults.xlsx is not a plain-text file, on Windows you need to specify that you want it to be downloaded as a binary file. To do this, add the argument mode = \"wb\" to the download.file() function so that it reads:\ndownload.file(\n  url = \"https://mpjashby.github.io/crimemappingdata/aggravated_assaults.xlsx\",\n  destfile = temp_file,\n  mode = \"wb\"\n)\nIf you are using a Mac or a Linux computer then you do not need to worry about this.\n\n\n# Specify the name of and location of our temporary file: it does not matter\n# what this file is called or where it is stored, so we use the tempfile()\n# function to create a file in the correct location automatically\ntemp_file &lt;- tempfile(fileext = \".xlsx\")\n\n# Download the Excel file and store it in the temporary location\ndownload.file(\n  url = \"https://mpjashby.github.io/crimemappingdata/aggravated_assaults.xlsx\",\n  destfile = temp_file,\n  mode = \"wb\"\n)\n\nThe download.file() function does not produce any output if the file has been successfully downloaded, so you will not see any output when you run this code.\nNow we have downloaded our data, we can load it into R. Since Excel files can contain multiple sheets, we need to specify which sheet we would like to load into a tibble. We can use the excel_sheets() function to get a list of sheets in an Excel file:\n\n# Load the readxl package\nlibrary(readxl)\n\n# Get a list of sheets in an Excel file\nexcel_sheets(temp_file)\n\n[1] \"Austin\"     \"Fort Worth\" \"Seattle\"   \n\n\nWe can now load the sheet containing data for Austin and view the first few rows of the resulting object:\n\nagg_assault_data &lt;- read_excel(temp_file, sheet = \"Austin\")\n\nhead(agg_assault_data)\n\n# A tibble: 6 × 5\n  date                longitude latitude location_type location_category\n  &lt;dttm&gt;                  &lt;dbl&gt;    &lt;dbl&gt; &lt;chr&gt;         &lt;chr&gt;            \n1 2019-01-01 00:00:00     -97.7     30.3 residence     residence        \n2 2019-01-01 00:00:00     -97.8     30.2 residence     residence        \n3 2019-01-01 00:01:00     -97.7     30.3 &lt;NA&gt;          &lt;NA&gt;             \n4 2019-01-01 00:15:00     -97.7     30.3 &lt;NA&gt;          &lt;NA&gt;             \n5 2019-01-01 00:27:00     -97.8     30.2 residence     residence        \n6 2019-01-01 00:30:00     -97.7     30.3 &lt;NA&gt;          &lt;NA&gt;             \n\n\nNow we have learned how to load our data into an object, we can use other R functions to work with that data in many different ways.\n\nDifferent types of data are loaded into R with different functions, e.g. CSV files are loaded with the read_csv() function from the readr package and Microsoft Excel files are loaded with the read_excel() function from the readxl package. You can download a table of functions for reading data into R to remind you of which function to use to load each type of file.\n\n\nLearn more about how to read data into R by reading this chapter of the free online book R for Data Science.\nExcel data can often be messy and the readxl package contains various other functions that can be used to deal with this. You can learn more about how to handle messy Excel data in this online tutorial.\n\n\n3.2.3 Check your understanding\nAnswer the following questions to check your understanding of what we’ve learned so far in this tutorial. If you get a question wrong, you can keep trying until you get the right answer.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Wrangling data</span>"
    ]
  },
  {
    "objectID": "03_data_wrangling/index.html#selecting-columns",
    "href": "03_data_wrangling/index.html#selecting-columns",
    "title": "\n3  Wrangling data\n",
    "section": "\n3.3 Selecting columns",
    "text": "3.3 Selecting columns\nIn this section we will learn how to reduce the size of our data by selecting only the columns we need and discarding the rest. This can be particularly useful if we are working with a very-large dataset, or if we want to produce a table containing only some columns.\n\n\nWe can use the select() function from the dplyr package (one of the packages that is loaded automatically when we call the library(tidyverse) function) to select columns.\nIf we wanted to select just the date and location_type columns from the agg_assault_data we loaded in the previous section:\n\nselect(agg_assault_data, date, location_type)\n\n# A tibble: 8,696 × 2\n   date                location_type\n   &lt;dttm&gt;              &lt;chr&gt;        \n 1 2019-01-01 00:00:00 residence    \n 2 2019-01-01 00:00:00 residence    \n 3 2019-01-01 00:01:00 &lt;NA&gt;         \n 4 2019-01-01 00:15:00 &lt;NA&gt;         \n 5 2019-01-01 00:27:00 residence    \n 6 2019-01-01 00:30:00 &lt;NA&gt;         \n 7 2019-01-01 00:51:00 &lt;NA&gt;         \n 8 2019-01-01 01:00:00 residence    \n 9 2019-01-01 01:00:00 &lt;NA&gt;         \n10 2019-01-01 01:12:00 residence    \n# ℹ 8,686 more rows\n\n\n\nIn a previous section, we mentioned that the code needed to run (or call) a function in R has two parts: the function name followed by a pair of parentheses, inside which are zero or more arguments separated by commas. The arguments in the select() function (and many other functions in the dplyr package) work in a slightly different way to many other functions. Here, the first argument is the name of the data object that we want to select from. All the remaining arguments (here, date and location_type) are the names of the columns we want to select from the data.\nWe can select as many columns as we want, by just adding the names of the columns separated by commas.\n\nWrite the code necessary to select the longitude and latitude columns from the agg_assault_data object:\n\n\nselect(agg_assault_data, longitude, latitude)\n\n# A tibble: 8,696 × 2\n   longitude latitude\n       &lt;dbl&gt;    &lt;dbl&gt;\n 1     -97.7     30.3\n 2     -97.8     30.2\n 3     -97.7     30.3\n 4     -97.7     30.3\n 5     -97.8     30.2\n 6     -97.7     30.3\n 7     -97.7     30.3\n 8     -97.7     30.4\n 9     -97.7     30.3\n10     -97.7     30.3\n# ℹ 8,686 more rows\n\n\nThe columns in our new dataset will appear in the order in which we specify them in the select() function.\nWe can also use select() to rename columns at the same time as selecting them. For example, to select the columns date and location_type while also renaming location_type to be called type:\n\nselect(agg_assault_data, date, type = location_type)\n\n# A tibble: 8,696 × 2\n   date                type     \n   &lt;dttm&gt;              &lt;chr&gt;    \n 1 2019-01-01 00:00:00 residence\n 2 2019-01-01 00:00:00 residence\n 3 2019-01-01 00:01:00 &lt;NA&gt;     \n 4 2019-01-01 00:15:00 &lt;NA&gt;     \n 5 2019-01-01 00:27:00 residence\n 6 2019-01-01 00:30:00 &lt;NA&gt;     \n 7 2019-01-01 00:51:00 &lt;NA&gt;     \n 8 2019-01-01 01:00:00 residence\n 9 2019-01-01 01:00:00 &lt;NA&gt;     \n10 2019-01-01 01:12:00 residence\n# ℹ 8,686 more rows\n\n\nIf we want to rename a column while keeping all the columns in the data, we can instead use the rename() function (also from the dplyr package):\n\nrename(agg_assault_data, type = location_type)\n\n# A tibble: 8,696 × 5\n   date                longitude latitude type      location_category\n   &lt;dttm&gt;                  &lt;dbl&gt;    &lt;dbl&gt; &lt;chr&gt;     &lt;chr&gt;            \n 1 2019-01-01 00:00:00     -97.7     30.3 residence residence        \n 2 2019-01-01 00:00:00     -97.8     30.2 residence residence        \n 3 2019-01-01 00:01:00     -97.7     30.3 &lt;NA&gt;      &lt;NA&gt;             \n 4 2019-01-01 00:15:00     -97.7     30.3 &lt;NA&gt;      &lt;NA&gt;             \n 5 2019-01-01 00:27:00     -97.8     30.2 residence residence        \n 6 2019-01-01 00:30:00     -97.7     30.3 &lt;NA&gt;      &lt;NA&gt;             \n 7 2019-01-01 00:51:00     -97.7     30.3 &lt;NA&gt;      &lt;NA&gt;             \n 8 2019-01-01 01:00:00     -97.7     30.4 residence residence        \n 9 2019-01-01 01:00:00     -97.7     30.3 &lt;NA&gt;      &lt;NA&gt;             \n10 2019-01-01 01:12:00     -97.7     30.3 residence residence        \n# ℹ 8,686 more rows\n\n\nRemember that functions in R generally do not change existing objects, but instead produce (or return) new ones. This means if we want to store the result of this function so we can use it later, we have to assign the value returned by the function to a new object (or overwrite the existing object):\n\nagg_assault_locations &lt;- select(agg_assault_data, lon = longitude, lat = latitude)\n\nhead(agg_assault_locations)\n\n# A tibble: 6 × 2\n    lon   lat\n  &lt;dbl&gt; &lt;dbl&gt;\n1 -97.7  30.3\n2 -97.8  30.2\n3 -97.7  30.3\n4 -97.7  30.3\n5 -97.8  30.2\n6 -97.7  30.3\n\n\n\nYou can learn more about selecting, filtering and arranging data using the functions in the dplyr package by reading this Introduction to dplyr tutorial.\n\n\n3.3.1 Check your understanding",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Wrangling data</span>"
    ]
  },
  {
    "objectID": "03_data_wrangling/index.html#filtering-rows",
    "href": "03_data_wrangling/index.html#filtering-rows",
    "title": "\n3  Wrangling data\n",
    "section": "\n3.4 Filtering rows",
    "text": "3.4 Filtering rows\nOften in crime mapping we will only be interested in part of a particular dataset. In the same way that we can select particular columns in our data, we can filter particular rows using the filter() function from the dplyr package.\n\n\nIf we were only interested in offences in the agg_assault_data dataset that occurred in residences, we could use filter():\n\nfilter(agg_assault_data, location_type == \"residence\")\n\n# A tibble: 4,385 × 5\n   date                longitude latitude location_type location_category\n   &lt;dttm&gt;                  &lt;dbl&gt;    &lt;dbl&gt; &lt;chr&gt;         &lt;chr&gt;            \n 1 2019-01-01 00:00:00     -97.7     30.3 residence     residence        \n 2 2019-01-01 00:00:00     -97.8     30.2 residence     residence        \n 3 2019-01-01 00:27:00     -97.8     30.2 residence     residence        \n 4 2019-01-01 01:00:00     -97.7     30.4 residence     residence        \n 5 2019-01-01 01:12:00     -97.7     30.3 residence     residence        \n 6 2019-01-01 01:20:00     -97.7     30.4 residence     residence        \n 7 2019-01-01 01:50:00     -97.7     30.3 residence     residence        \n 8 2019-01-01 02:21:00     -97.8     30.2 residence     residence        \n 9 2019-01-01 02:26:00     -97.8     30.2 residence     residence        \n10 2019-01-01 02:35:00     -97.7     30.4 residence     residence        \n# ℹ 4,375 more rows\n\n\nNote that:\n\nthe column name location_type is not surrounded by quotes but the column value \"residence\" is, and\nthe == (equal to) operator is used, since a single equals sign = has another meaning in R.\n\nWe can filter using the values of more than one column simultaneously. To filter offences in which the location_category is ‘retail’ and the location_type is ‘convenience store’:\n\nfilter(\n  agg_assault_data, \n  location_category == \"retail\", \n  location_type == \"convenience store\"\n)\n\n# A tibble: 90 × 5\n   date                longitude latitude location_type     location_category\n   &lt;dttm&gt;                  &lt;dbl&gt;    &lt;dbl&gt; &lt;chr&gt;             &lt;chr&gt;            \n 1 2019-01-06 16:58:00     -97.8     30.5 convenience store retail           \n 2 2019-01-08 17:56:00     -97.7     30.3 convenience store retail           \n 3 2019-01-10 01:25:00     -97.7     30.4 convenience store retail           \n 4 2019-01-12 00:49:00     -97.6     30.4 convenience store retail           \n 5 2019-01-12 05:53:00     -97.7     30.4 convenience store retail           \n 6 2019-01-12 17:01:00     -97.7     30.4 convenience store retail           \n 7 2019-01-12 19:12:00     -97.8     30.3 convenience store retail           \n 8 2019-01-12 19:14:00     -97.7     30.4 convenience store retail           \n 9 2019-01-13 04:10:00     -97.8     30.2 convenience store retail           \n10 2019-01-22 22:01:00     -97.7     30.3 convenience store retail           \n# ℹ 80 more rows\n\n\nAs well as filtering using the == operator, we can filter using the greater-than (&gt;), less-than (&lt;), greater-than-or-equal-to (&gt;=) and less-than-or-equal-to (&lt;=) operators. For example, we can choose offences that occurred in residences on or after 1 July 2019:\n\nfilter(\n  agg_assault_data, \n  location_type == \"residence\", \n  date &gt;= as.Date(\"2019-07-01\")\n)\n\n# A tibble: 2,286 × 5\n   date                longitude latitude location_type location_category\n   &lt;dttm&gt;                  &lt;dbl&gt;    &lt;dbl&gt; &lt;chr&gt;         &lt;chr&gt;            \n 1 2019-07-01 02:40:00     -97.8     30.4 residence     residence        \n 2 2019-07-01 07:26:00     -97.7     30.3 residence     residence        \n 3 2019-07-01 08:25:00     -97.8     30.2 residence     residence        \n 4 2019-07-01 09:39:00     -97.7     30.3 residence     residence        \n 5 2019-07-01 09:40:00     -97.7     30.4 residence     residence        \n 6 2019-07-01 16:24:00     -97.8     30.1 residence     residence        \n 7 2019-07-01 17:30:00     -97.8     30.2 residence     residence        \n 8 2019-07-01 17:41:00     -97.8     30.2 residence     residence        \n 9 2019-07-01 18:03:00     -97.8     30.2 residence     residence        \n10 2019-07-01 18:16:00     -97.8     30.1 residence     residence        \n# ℹ 2,276 more rows\n\n\nSometimes we will want to filter rows that are one thing or another. We can do this with the | (or) operator. For example, we can filter offences that occurred either in leisure facilities or shopping malls on or after 1 July 2019:\n\nfilter(\n  agg_assault_data, \n  location_category == \"leisure\" | location_type == \"mall\", \n  date &gt;= as.Date(\"2019-07-01\")\n)\n\n# A tibble: 10 × 5\n   date                longitude latitude location_type location_category\n   &lt;dttm&gt;                  &lt;dbl&gt;    &lt;dbl&gt; &lt;chr&gt;         &lt;chr&gt;            \n 1 2019-07-01 09:00:00     -97.7     30.3 entertainment leisure          \n 2 2019-07-15 14:06:00     -97.8     30.3 mall          retail           \n 3 2019-08-06 23:20:00     -97.7     30.4 mall          retail           \n 4 2019-08-18 13:31:00     -97.7     30.3 mall          retail           \n 5 2019-08-25 15:35:00     -97.8     30.3 mall          retail           \n 6 2019-09-06 15:58:00     -97.7     30.4 mall          retail           \n 7 2019-10-05 16:47:00     -97.7     30.2 mall          retail           \n 8 2019-11-01 22:23:00     -97.7     30.3 entertainment leisure          \n 9 2019-11-18 16:29:00     -97.7     30.4 mall          retail           \n10 2019-11-27 14:14:00     -97.8     30.3 mall          retail           \n\n\nIf we want to filter offences that have any one of several different values of the same column, we can use the %in% (in) operator. To filter offences that occurred in either streets or publicly accessible open spaces:\n\nfilter(agg_assault_data, location_category %in% c(\"open space\", \"street\"))\n\n# A tibble: 165 × 5\n   date                longitude latitude location_type location_category\n   &lt;dttm&gt;                  &lt;dbl&gt;    &lt;dbl&gt; &lt;chr&gt;         &lt;chr&gt;            \n 1 2019-01-07 21:37:00     -97.8     30.5 green space   open space       \n 2 2019-01-08 14:02:00     -97.7     30.3 green space   open space       \n 3 2019-01-10 01:59:00     -97.6     30.3 green space   open space       \n 4 2019-01-11 17:00:00     -97.7     30.3 green space   open space       \n 5 2019-01-12 15:50:00     -97.8     30.2 green space   open space       \n 6 2019-01-12 16:00:00     -97.8     30.4 green space   open space       \n 7 2019-01-15 07:20:00     -97.7     30.2 green space   open space       \n 8 2019-01-17 00:00:00     -97.7     30.4 green space   open space       \n 9 2019-01-22 22:00:00     -97.7     30.2 green space   open space       \n10 2019-01-29 01:40:00     -97.7     30.2 green space   open space       \n# ℹ 155 more rows\n\n\nThe code c(\"open space\", \"street\") produces what is referred to in R as a vector (sometimes referred to as an atomic vector, especially in error messages). A vector is a one-dimensional sequence of values of the same type (i.e. all numbers, all character strings etc.). For example, a vector might hold several strings of text (as in the vector c(\"open space\", \"street\")) or a series of numbers such as c(1, 2, 3). There is lots we could learn about vectors, but for now it’s only necessary to know that we can create vectors with the c() or combine function.\nIf we wanted to re-use a vector of values several times in our code, it might make sense to store the vector as an object. For example:\n\n# Create vector of location types we are interested in\nlocation_types &lt;- c(\"open space\", \"street\")\n\n# Filter the data\nfilter(agg_assault_data, location_category %in% location_types)\n\n# A tibble: 165 × 5\n   date                longitude latitude location_type location_category\n   &lt;dttm&gt;                  &lt;dbl&gt;    &lt;dbl&gt; &lt;chr&gt;         &lt;chr&gt;            \n 1 2019-01-07 21:37:00     -97.8     30.5 green space   open space       \n 2 2019-01-08 14:02:00     -97.7     30.3 green space   open space       \n 3 2019-01-10 01:59:00     -97.6     30.3 green space   open space       \n 4 2019-01-11 17:00:00     -97.7     30.3 green space   open space       \n 5 2019-01-12 15:50:00     -97.8     30.2 green space   open space       \n 6 2019-01-12 16:00:00     -97.8     30.4 green space   open space       \n 7 2019-01-15 07:20:00     -97.7     30.2 green space   open space       \n 8 2019-01-17 00:00:00     -97.7     30.4 green space   open space       \n 9 2019-01-22 22:00:00     -97.7     30.2 green space   open space       \n10 2019-01-29 01:40:00     -97.7     30.2 green space   open space       \n# ℹ 155 more rows\n\n\nFinally, you can filter based on the output of any R function that returns TRUE or FALSE. For example, missing values are represented in R as NA. We can test whether a value is missing using the is.na() function. If we wanted to remove rows from our data that had missing location types, we would filter for those rows that are not NA. We can do this by combining the is.na() function with the ! (not) operator:\n\nfilter(agg_assault_data, !is.na(location_type))\n\n# A tibble: 5,344 × 5\n   date                longitude latitude location_type location_category\n   &lt;dttm&gt;                  &lt;dbl&gt;    &lt;dbl&gt; &lt;chr&gt;         &lt;chr&gt;            \n 1 2019-01-01 00:00:00     -97.7     30.3 residence     residence        \n 2 2019-01-01 00:00:00     -97.8     30.2 residence     residence        \n 3 2019-01-01 00:27:00     -97.8     30.2 residence     residence        \n 4 2019-01-01 01:00:00     -97.7     30.4 residence     residence        \n 5 2019-01-01 01:12:00     -97.7     30.3 residence     residence        \n 6 2019-01-01 01:20:00     -97.7     30.4 residence     residence        \n 7 2019-01-01 01:35:00     -97.7     30.3 hotel         hotel            \n 8 2019-01-01 01:50:00     -97.7     30.3 residence     residence        \n 9 2019-01-01 02:21:00     -97.8     30.2 residence     residence        \n10 2019-01-01 02:26:00     -97.8     30.2 residence     residence        \n# ℹ 5,334 more rows\n\n\nWe will see lots more examples of how to use filter() in future tutorials.\n\n3.4.1 Check your understanding",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Wrangling data</span>"
    ]
  },
  {
    "objectID": "03_data_wrangling/index.html#transforming-values",
    "href": "03_data_wrangling/index.html#transforming-values",
    "title": "\n3  Wrangling data\n",
    "section": "\n3.5 Transforming values",
    "text": "3.5 Transforming values\nIt is often useful to create new columns in our data, or change the values of existing columns. The mutate() function in the dplyr package gives us a way to transform existing columns in our dataset using almost any R function.\n\n\n\nFor example, say we wanted to create a new column in our aggravated-assault dataset specifying the day of the week on which each crime occurred. We can do this using the wday() function from the lubridate package (using the label = TRUE argument to produce weekday names, rather than numbers):\n\nlibrary(lubridate)\n\nmutate(agg_assault_data, weekday = wday(date, label = TRUE))\n\n# A tibble: 8,696 × 6\n   date                longitude latitude location_type location_category\n   &lt;dttm&gt;                  &lt;dbl&gt;    &lt;dbl&gt; &lt;chr&gt;         &lt;chr&gt;            \n 1 2019-01-01 00:00:00     -97.7     30.3 residence     residence        \n 2 2019-01-01 00:00:00     -97.8     30.2 residence     residence        \n 3 2019-01-01 00:01:00     -97.7     30.3 &lt;NA&gt;          &lt;NA&gt;             \n 4 2019-01-01 00:15:00     -97.7     30.3 &lt;NA&gt;          &lt;NA&gt;             \n 5 2019-01-01 00:27:00     -97.8     30.2 residence     residence        \n 6 2019-01-01 00:30:00     -97.7     30.3 &lt;NA&gt;          &lt;NA&gt;             \n 7 2019-01-01 00:51:00     -97.7     30.3 &lt;NA&gt;          &lt;NA&gt;             \n 8 2019-01-01 01:00:00     -97.7     30.4 residence     residence        \n 9 2019-01-01 01:00:00     -97.7     30.3 &lt;NA&gt;          &lt;NA&gt;             \n10 2019-01-01 01:12:00     -97.7     30.3 residence     residence        \n# ℹ 8,686 more rows\n# ℹ 1 more variable: weekday &lt;ord&gt;\n\n\nDepending on the width of your screen, you might need to click the ▸ button to see the new variable.\nWe can also categorise an existing variable, for example creating a variable to show whether an offence occurred in the northern or southern half of the city:\n\nmutate(\n  agg_assault_data, \n  region = if_else(latitude &gt; median(latitude), \"northern\", \"southern\")\n)\n\n# A tibble: 8,696 × 6\n   date                longitude latitude location_type location_category region\n   &lt;dttm&gt;                  &lt;dbl&gt;    &lt;dbl&gt; &lt;chr&gt;         &lt;chr&gt;             &lt;chr&gt; \n 1 2019-01-01 00:00:00     -97.7     30.3 residence     residence         north…\n 2 2019-01-01 00:00:00     -97.8     30.2 residence     residence         south…\n 3 2019-01-01 00:01:00     -97.7     30.3 &lt;NA&gt;          &lt;NA&gt;              south…\n 4 2019-01-01 00:15:00     -97.7     30.3 &lt;NA&gt;          &lt;NA&gt;              south…\n 5 2019-01-01 00:27:00     -97.8     30.2 residence     residence         south…\n 6 2019-01-01 00:30:00     -97.7     30.3 &lt;NA&gt;          &lt;NA&gt;              south…\n 7 2019-01-01 00:51:00     -97.7     30.3 &lt;NA&gt;          &lt;NA&gt;              north…\n 8 2019-01-01 01:00:00     -97.7     30.4 residence     residence         north…\n 9 2019-01-01 01:00:00     -97.7     30.3 &lt;NA&gt;          &lt;NA&gt;              north…\n10 2019-01-01 01:12:00     -97.7     30.3 residence     residence         north…\n# ℹ 8,686 more rows\n\n\nWe can change existing columns, although (as with objects) there is no way to undo this so you should only replace columns if you are sure you will not need them. For example, if we wanted to remove the time portion of the date variable (which may sometimes be useful, as shown in the next section) using the as_date() function (also from the lubridate package) and at the same time create the weekday variable:\n\nmutate(\n  agg_assault_data, \n  date = as_date(date),\n  weekday = wday(date, label = TRUE)\n)\n\n# A tibble: 8,696 × 6\n   date       longitude latitude location_type location_category weekday\n   &lt;date&gt;         &lt;dbl&gt;    &lt;dbl&gt; &lt;chr&gt;         &lt;chr&gt;             &lt;ord&gt;  \n 1 2019-01-01     -97.7     30.3 residence     residence         Tue    \n 2 2019-01-01     -97.8     30.2 residence     residence         Tue    \n 3 2019-01-01     -97.7     30.3 &lt;NA&gt;          &lt;NA&gt;              Tue    \n 4 2019-01-01     -97.7     30.3 &lt;NA&gt;          &lt;NA&gt;              Tue    \n 5 2019-01-01     -97.8     30.2 residence     residence         Tue    \n 6 2019-01-01     -97.7     30.3 &lt;NA&gt;          &lt;NA&gt;              Tue    \n 7 2019-01-01     -97.7     30.3 &lt;NA&gt;          &lt;NA&gt;              Tue    \n 8 2019-01-01     -97.7     30.4 residence     residence         Tue    \n 9 2019-01-01     -97.7     30.3 &lt;NA&gt;          &lt;NA&gt;              Tue    \n10 2019-01-01     -97.7     30.3 residence     residence         Tue    \n# ℹ 8,686 more rows\n\n\nYou may sometimes want to change only some values in a column. We can do this in various ways, depending on which values we want to change:\n\nmutate(\n  agg_assault_data,\n  # Change a single value with a new value (and otherwise keep the existing \n  # value) using the if_else() function\n  location_type = if_else(location_type == \"street\", \"road\", location_type),\n  # Change multiple values in a categorical variable using the recode() \n  # function, in which values are changed using arguments in the format\n  # old_value = new_value\n  location_category = recode(\n    location_category, \n    \"open space\" = \"public open space\",\n    \"street\" = \"street or road\"\n  )\n)\n\n# A tibble: 8,696 × 5\n   date                longitude latitude location_type location_category\n   &lt;dttm&gt;                  &lt;dbl&gt;    &lt;dbl&gt; &lt;chr&gt;         &lt;chr&gt;            \n 1 2019-01-01 00:00:00     -97.7     30.3 residence     residence        \n 2 2019-01-01 00:00:00     -97.8     30.2 residence     residence        \n 3 2019-01-01 00:01:00     -97.7     30.3 &lt;NA&gt;          &lt;NA&gt;             \n 4 2019-01-01 00:15:00     -97.7     30.3 &lt;NA&gt;          &lt;NA&gt;             \n 5 2019-01-01 00:27:00     -97.8     30.2 residence     residence        \n 6 2019-01-01 00:30:00     -97.7     30.3 &lt;NA&gt;          &lt;NA&gt;             \n 7 2019-01-01 00:51:00     -97.7     30.3 &lt;NA&gt;          &lt;NA&gt;             \n 8 2019-01-01 01:00:00     -97.7     30.4 residence     residence        \n 9 2019-01-01 01:00:00     -97.7     30.3 &lt;NA&gt;          &lt;NA&gt;             \n10 2019-01-01 01:12:00     -97.7     30.3 residence     residence        \n# ℹ 8,686 more rows\n\n\nWe could also make changes based on more-complicated sets of criteria using the case_when() function, but we will return to that in a future tutorial.\nThe R functions that you use inside mutate() must return the same number of values as there are rows in the dataset. This is true for most R functions (which are referred to as vectorised functions), but there are some – such as mean() and max() – that return a single value. These summarising functions cannot be used inside mutate() (you will see an error message if you try) but are instead used with the next data-wrangling function we will learn about: summarise().",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Wrangling data</span>"
    ]
  },
  {
    "objectID": "03_data_wrangling/index.html#summarising-rows",
    "href": "03_data_wrangling/index.html#summarising-rows",
    "title": "\n3  Wrangling data\n",
    "section": "\n3.6 Summarising rows",
    "text": "3.6 Summarising rows\nSummarising data is often useful in crime analysis. We can use the summarise() function from the dplyr package to produce summaries of different columns in our data. There is an identical function called summarize() so that you do not have to remember whether to use the US or British spelling.\nBy default, summarise() collapses data into a single row, with each column summarised using a function that you specify. For example, suppose you want to find out which police station a specialist squad should be based at to most easily respond to reports of serious assaults. You might do this by working out the weighted centre of all the offence locations, i.e. the means of the longitudes and latitudes for all the crimes. You could then base the squad at the police station that was closest to the weighted centre.\n\nsummarise(\n  agg_assault_data, \n  mean_lng = mean(longitude, na.rm = TRUE),\n  mean_lat = mean(latitude, na.rm = TRUE)\n)\n\n# A tibble: 1 × 2\n  mean_lng mean_lat\n     &lt;dbl&gt;    &lt;dbl&gt;\n1    -97.7     30.3\n\n\n\n\nWhat does the argument na.rm = TRUE do?\n\n\nLots of functions in R have an argument called na.rm that can be set to either TRUE or FALSE. Setting na.rm = TRUE in this case specifies that the mean() function should remove (rm) any missing (NA) values before calculating the mean.\nIf we do not specify this and our data contain any missing values, the mean() function will return NA. Functions in R do this because it is not possible to completely answer the question ‘what is the mean of these values?’ if some of the values are missing.\nThis logic applies in lots of cases. For example, if you create an R object called value with the code value &lt;- 2 and then run the R code value &gt; 1, you will get the answer TRUE. But if you set the object value to be NA using the code value &lt;- NA, when you run the R code value &gt; 1 you will get the answer NA. This is because there is no way to know if the missing value represented by NA is greater than 1 or not. This is why it is often useful to calculate statistics such as a mean value after removing any missing values using the na.rm = TRUE argument.\n\n\nsummarise() becomes more useful if we first divide our data into groups, since we then get a summary for each group separately. We can use the .by argument of the summarise() function to specify that we want separate summaries for each unique value of one or more columns in the data. For example, to produce a separate summary for each unique value of location_category, we can use this code:\n\nsummarise(\n  agg_assault_data,\n  mean_lng = mean(longitude, na.rm = TRUE),\n  mean_lat = mean(latitude, na.rm = TRUE),\n  .by = location_category\n)\n\n# A tibble: 12 × 3\n   location_category mean_lng mean_lat\n   &lt;chr&gt;                &lt;dbl&gt;    &lt;dbl&gt;\n 1 residence            -97.7     30.3\n 2 &lt;NA&gt;                 -97.7     30.3\n 3 hotel                -97.7     30.3\n 4 other                -97.7     30.3\n 5 retail               -97.7     30.3\n 6 healthcare           -97.7     30.3\n 7 open space           -97.7     30.3\n 8 commercial           -97.7     30.3\n 9 education            -97.7     30.3\n10 government           -97.7     30.3\n11 leisure              -97.7     30.4\n12 transportation       -97.7     30.2\n\n\nYou can add multiple grouping variables using the c() (combine) function if you want to generate summary values for groups within groups:\n\nsummarise(\n  agg_assault_data,\n  mean_lng = mean(longitude, na.rm = TRUE),\n  mean_lat = mean(latitude, na.rm = TRUE),\n  .by = c(location_category, location_type)\n)\n\n# A tibble: 24 × 4\n   location_category location_type     mean_lng mean_lat\n   &lt;chr&gt;             &lt;chr&gt;                &lt;dbl&gt;    &lt;dbl&gt;\n 1 residence         residence            -97.7     30.3\n 2 &lt;NA&gt;              &lt;NA&gt;                 -97.7     30.3\n 3 hotel             hotel                -97.7     30.3\n 4 other             other                -97.7     30.3\n 5 retail            other retail         -97.7     30.3\n 6 healthcare        healthcare           -97.7     30.3\n 7 retail            mall                 -97.8     30.3\n 8 retail            convenience store    -97.7     30.3\n 9 open space        green space          -97.7     30.3\n10 commercial        office               -97.7     30.3\n# ℹ 14 more rows\n\n\n\n3.6.1 Counting rows\nOne very common way of summarising data is to count the number of rows in a dataset that have each unique value of one or more columns. For example, if we have a dataset of crimes in which each row represents a single crime, we might want to count how many crimes happened on each day of the week, or how many crimes of each type are in the dataset. We can use summarise() to do that, together with the n() function (from the same dplyr package as summarise()). For example, if we wanted to count how many rows in the agg_assault_data dataset had each unique combination of location_category and location_type:\n\nsummarise(agg_assault_data, n = n(), .by = c(location_category, location_type))\n\n# A tibble: 24 × 3\n   location_category location_type         n\n   &lt;chr&gt;             &lt;chr&gt;             &lt;int&gt;\n 1 residence         residence          4385\n 2 &lt;NA&gt;              &lt;NA&gt;               3352\n 3 hotel             hotel               248\n 4 other             other               205\n 5 retail            other retail         30\n 6 healthcare        healthcare           36\n 7 retail            mall                 11\n 8 retail            convenience store    90\n 9 open space        green space         165\n10 commercial        office               28\n# ℹ 14 more rows\n\n\nIn this code, the n() function simply returns the number of rows of data in each group, i.e. the number of rows with each unique combination of values of location_category and location_type.\nSince counting the number of rows in each group a dataset is a very common task, dplyr includes another function called count() that allows you to do the same thing as in the code above, but with slightly less typing:\nSo if you wanted to know how many aggravated assaults had occurred in each location category and type:\n\ncount(agg_assault_data, location_category, location_type)\n\n# A tibble: 24 × 3\n   location_category location_type         n\n   &lt;chr&gt;             &lt;chr&gt;             &lt;int&gt;\n 1 commercial        construction         11\n 2 commercial        factory/warehouse     2\n 3 commercial        finance               1\n 4 commercial        office               28\n 5 commercial        storage               4\n 6 education         child care           11\n 7 education         college               5\n 8 education         school               25\n 9 government        government           18\n10 healthcare        healthcare           36\n# ℹ 14 more rows",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Wrangling data</span>"
    ]
  },
  {
    "objectID": "03_data_wrangling/index.html#arranging-rows",
    "href": "03_data_wrangling/index.html#arranging-rows",
    "title": "\n3  Wrangling data\n",
    "section": "\n3.7 Arranging rows",
    "text": "3.7 Arranging rows\nIt is sometimes useful to be able to place rows in a dataset into a particular order. We can do this using the arrange() function from the dplyr package. For example, we can sort the aggravated-assault data by date:\n\narrange(agg_assault_data, date)\n\n# A tibble: 8,696 × 5\n   date                longitude latitude location_type location_category\n   &lt;dttm&gt;                  &lt;dbl&gt;    &lt;dbl&gt; &lt;chr&gt;         &lt;chr&gt;            \n 1 2019-01-01 00:00:00     -97.7     30.3 residence     residence        \n 2 2019-01-01 00:00:00     -97.8     30.2 residence     residence        \n 3 2019-01-01 00:01:00     -97.7     30.3 &lt;NA&gt;          &lt;NA&gt;             \n 4 2019-01-01 00:15:00     -97.7     30.3 &lt;NA&gt;          &lt;NA&gt;             \n 5 2019-01-01 00:27:00     -97.8     30.2 residence     residence        \n 6 2019-01-01 00:30:00     -97.7     30.3 &lt;NA&gt;          &lt;NA&gt;             \n 7 2019-01-01 00:51:00     -97.7     30.3 &lt;NA&gt;          &lt;NA&gt;             \n 8 2019-01-01 01:00:00     -97.7     30.4 residence     residence        \n 9 2019-01-01 01:00:00     -97.7     30.3 &lt;NA&gt;          &lt;NA&gt;             \n10 2019-01-01 01:12:00     -97.7     30.3 residence     residence        \n# ℹ 8,686 more rows\n\n\nBy default, arrange() sorts rows in ascending order, i.e. it sorts numeric values from the smallest to the largest, dates from earliest to latest and character values alphabetically. We can instead sort values in descending order by wrapping the name of a column in the desc() function:\n\narrange(agg_assault_data, desc(date))\n\n# A tibble: 8,696 × 5\n   date                longitude latitude location_type location_category\n   &lt;dttm&gt;                  &lt;dbl&gt;    &lt;dbl&gt; &lt;chr&gt;         &lt;chr&gt;            \n 1 2019-12-31 23:33:00     -97.7     30.4 residence     residence        \n 2 2019-12-31 23:26:00     -97.7     30.4 residence     residence        \n 3 2019-12-31 23:20:00     -97.7     30.3 &lt;NA&gt;          &lt;NA&gt;             \n 4 2019-12-31 23:19:00     -97.7     30.3 &lt;NA&gt;          &lt;NA&gt;             \n 5 2019-12-31 22:39:00     -97.7     30.3 other         other            \n 6 2019-12-31 22:29:00     -97.7     30.3 residence     residence        \n 7 2019-12-31 21:13:00     -97.8     30.2 residence     residence        \n 8 2019-12-31 20:52:00     -97.7     30.3 residence     residence        \n 9 2019-12-31 20:44:00     -97.7     30.3 residence     residence        \n10 2019-12-31 20:19:00     -97.7     30.3 &lt;NA&gt;          &lt;NA&gt;             \n# ℹ 8,686 more rows\n\n\nWe can also sort the data based on multiple columns – the data are sorted first on the first column that you specify, with tied rows then sorted on the subsequent columns in order.\n\narrange(agg_assault_data, date, desc(location_type), location_category)\n\n# A tibble: 8,696 × 5\n   date                longitude latitude location_type location_category\n   &lt;dttm&gt;                  &lt;dbl&gt;    &lt;dbl&gt; &lt;chr&gt;         &lt;chr&gt;            \n 1 2019-01-01 00:00:00     -97.7     30.3 residence     residence        \n 2 2019-01-01 00:00:00     -97.8     30.2 residence     residence        \n 3 2019-01-01 00:01:00     -97.7     30.3 &lt;NA&gt;          &lt;NA&gt;             \n 4 2019-01-01 00:15:00     -97.7     30.3 &lt;NA&gt;          &lt;NA&gt;             \n 5 2019-01-01 00:27:00     -97.8     30.2 residence     residence        \n 6 2019-01-01 00:30:00     -97.7     30.3 &lt;NA&gt;          &lt;NA&gt;             \n 7 2019-01-01 00:51:00     -97.7     30.3 &lt;NA&gt;          &lt;NA&gt;             \n 8 2019-01-01 01:00:00     -97.7     30.4 residence     residence        \n 9 2019-01-01 01:00:00     -97.7     30.3 &lt;NA&gt;          &lt;NA&gt;             \n10 2019-01-01 01:12:00     -97.7     30.3 residence     residence        \n# ℹ 8,686 more rows\n\n\n\n3.7.1 Check your understanding\nType the code necessary to arrange agg_assault_data in order of latitude, in descending order (from largest to smallest)\nRun your code using the Run Code button, then (if necessary) correct your code and run it again. Once you are happy that your code does what it is intended to do, click the Solution button to check.\n\narrange(agg_assault_data, desc(latitude))\n\n# A tibble: 8,696 × 5\n   date                longitude latitude location_type location_category\n   &lt;dttm&gt;                  &lt;dbl&gt;    &lt;dbl&gt; &lt;chr&gt;         &lt;chr&gt;            \n 1 2019-07-05 10:33:00     -97.7     30.5 residence     residence        \n 2 2019-09-28 19:31:00     -97.8     30.5 residence     residence        \n 3 2019-09-20 13:30:00     -97.8     30.5 residence     residence        \n 4 2019-08-25 22:04:00     -97.8     30.5 &lt;NA&gt;          &lt;NA&gt;             \n 5 2019-11-19 12:00:00     -97.8     30.5 school        education        \n 6 2019-01-26 19:52:00     -97.8     30.5 residence     residence        \n 7 2019-07-15 13:37:00     -97.8     30.5 &lt;NA&gt;          &lt;NA&gt;             \n 8 2019-01-28 21:14:00     -97.8     30.5 residence     residence        \n 9 2019-02-03 00:53:00     -97.8     30.5 residence     residence        \n10 2019-07-31 01:37:00     -97.8     30.5 residence     residence        \n# ℹ 8,686 more rows",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Wrangling data</span>"
    ]
  },
  {
    "objectID": "03_data_wrangling/index.html#saving-data",
    "href": "03_data_wrangling/index.html#saving-data",
    "title": "\n3  Wrangling data\n",
    "section": "\n3.8 Saving data",
    "text": "3.8 Saving data\nOnce we have finished wrangling a particular dataset, it is often useful to save it to a file so that we can use it again in future without going through all the steps of data wrangling again.\nMost R functions that begin with read_ (like read_csv() and read_excel()) have equivalent functions that begin write_ and which save data into a particular file format. In this example, we will use the write_csv() function from the readr package, which is loaded when we load the tidyverse package.\n# We can write data to a file in the same folder as our R script\nwrite_csv(agg_assault_data, \"fort_worth_agg_assault.csv\")\n\n# Or another folder on your computer ('../../' is short for the parent folder of\n# the parent folder of the current folder)\nwrite_csv(agg_assault_data, \"../../fort_worth_agg_assault.csv\")\nFor very large datasets, we can save a compressed version of the file by adding .gz (for gzip) to the end of the file name, which tells R to compress the file after creating it.\nwrite_csv(agg_assault_data, \"fort_worth_agg_assault.csv.gz\")\nread_csv() can read gzipped CSV files, but some other programs (such as Excel) cannot, so only use this option if you are sure you will only need to open the file in software that can handle it.\nThere are corresponding write functions for other types of data (which we will come back to when we learn how to handle spatial data), but in this course we will store all non-spatial data in CSV format because it can be read by many different programs.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Wrangling data</span>"
    ]
  },
  {
    "objectID": "03_data_wrangling/index.html#stringing-functions-together",
    "href": "03_data_wrangling/index.html#stringing-functions-together",
    "title": "\n3  Wrangling data\n",
    "section": "\n3.9 Stringing functions together",
    "text": "3.9 Stringing functions together\nIn this tutorial we have learned how to use the dplyr functions select(), filter(), mutate(), summarise() and arrange() to wrangle data from one format to another. Data wrangling is part of almost all data analysis, so these are skills we will use frequently.\nData wrangling often involves multiple steps. For example, we might want to load some data, select certain columns, filter some rows, mutate some of the variables, summarise the dataset and save the result. We can do each of these steps separately, assigning the result of each step to a new object.\n\n# Load the lubridate package, since we will need it below\nlibrary(lubridate)\n\n# Read San Francisco robbery data\nrobbery1 &lt;- read_csv(\"https://mpjashby.github.io/crimemappingdata/san_francisco_robbery.csv\")\n\nRows: 951 Columns: 5\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (1): offense_type\ndbl  (3): uid, longitude, latitude\ndttm (1): date_time\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n# Select only the columns we need\nrobbery2 &lt;- select(robbery1, date_time)\n\n# Filter only those offences that occurred in the first quarter of 2019\nrobbery3 &lt;- filter(robbery2, as.Date(date_time) &lt;= as.Date(\"2019-03-31\"))\n\n# Create a new weekday variable\nrobbery4 &lt;- mutate(robbery3, weekday = wday(date_time, label = TRUE))\n\n# Count how many offences occurred on each weekday\nq1_weekday_counts &lt;- count(robbery4, weekday)\n\n# Print the first few rows of the result\nhead(q1_weekday_counts, n = 7)\n\n# A tibble: 7 × 2\n  weekday     n\n  &lt;ord&gt;   &lt;int&gt;\n1 Sun        34\n2 Mon        27\n3 Tue        28\n4 Wed        20\n5 Thu        34\n6 Fri        27\n7 Sat        26\n\n\nThis code works, but involves creating six new objects, even though we only need the final object for our analysis. You may notice that the first argument expected by select(), filter(), mutate() and count() is always the data tibble produced by the previous step. This means we can skip saving the result of each step as a new object, and just run the previous function inside the first function. This approach produces the following code, which produces exactly the same result as the code above.\n\nq1_weekday_counts &lt;- count(\n  mutate(\n    filter(\n      select(\n        read_csv(\"https://mpjashby.github.io/crimemappingdata/san_francisco_robbery.csv\"),\n        -offense_type\n      ), \n      as.Date(date_time) &lt;= as.Date(\"2019-03-31\")\n    ),\n    weekday = wday(date_time, label = TRUE)\n  ),\n  weekday\n)\n\nRows: 951 Columns: 5\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (1): offense_type\ndbl  (3): uid, longitude, latitude\ndttm (1): date_time\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nhead(q1_weekday_counts, n = 7)\n\n# A tibble: 7 × 2\n  weekday     n\n  &lt;ord&gt;   &lt;int&gt;\n1 Sun        34\n2 Mon        27\n3 Tue        28\n4 Wed        20\n5 Thu        34\n6 Fri        27\n7 Sat        26\n\n\nThis code works and takes up less space, but it’s quite difficult to read – which can be a problem for finding and fixing problems with your code. For example, it’s quite hard (without counting pairs of parentheses) to work out that the reference to the column weekday on line 12 of this code belongs to the count() function on line 1.\nIt’s possible to write this code so that it is readable and does not require us to create multiple different objects to store the result of each step in our code. This method uses the |&gt; (or pipe) operator. The pipe operator works by using the result of the code on the left-hand side of the pipe as the first argument to a function on the right-hand side. So the code x  |&gt; fun1() |&gt; fun2(y) is the same as the code fun2(fun1(x), y), but it is much easier to see that fun1() is run before fun2(). It may be useful to read the pipe operator as ‘and then’, since piped code does the first thing and then the second thing with the result and then the third thing with the result of that, and so on. Piped code (sometimes called a pipeline) is a lot like the series of steps in a recipe.\nSince each function we are using returns the new data, and the first argument to all of those functions is the name of the input data object, the pipe means we can just omit the first argument to all except the first function.\n\nsan_fran_rob &lt;- read_csv(\"https://mpjashby.github.io/crimemappingdata/san_francisco_robbery.csv\")\n\nRows: 951 Columns: 5\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (1): offense_type\ndbl  (3): uid, longitude, latitude\ndttm (1): date_time\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nq1_weekday_counts &lt;- san_fran_rob |&gt; \n  select(-offense_type) |&gt; \n  filter(as.Date(date_time) &lt;= as.Date(\"2019-03-31\")) |&gt; \n  mutate(weekday = wday(date_time, label = TRUE)) |&gt; \n  count(weekday)\n\nhead(q1_weekday_counts, n = 7)\n\n# A tibble: 7 × 2\n  weekday     n\n  &lt;ord&gt;   &lt;int&gt;\n1 Sun        34\n2 Mon        27\n3 Tue        28\n4 Wed        20\n5 Thu        34\n6 Fri        27\n7 Sat        26\n\n\nThis code strikes a good balance between being easy to read and not requiring us to manage lots of intermediate variables. You might not find the pipe operator completely intuitive at the moment, but it will become easier as you see more examples in future tutorials.\n\n\nWhat about the %&gt;% pipe operator?\n\n\nIf you have learned any R coding before, you might have learned to use the %&gt;% pipe operator from the magrittr package. The %&gt;% pipe operator was introduced several years ago to allow people to construct pipelines of code in R. The %&gt;% operator was so widely used that the team that writes the R programming language decided to provide a pipe operator in R itself, to avoid the need to load the magrittr package.\nYou will still see the %&gt;% pipe operator used in lots of R code examples online. In almost all cases, when you see %&gt;% you can replace it with the R pipe operator |&gt;, since they both work in very similar ways.\n\n\n\nYou can find out more about how to use the pipe operator in the Introducing magrittr online tutorial.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Wrangling data</span>"
    ]
  },
  {
    "objectID": "03_data_wrangling/index.html#in-summary",
    "href": "03_data_wrangling/index.html#in-summary",
    "title": "\n3  Wrangling data\n",
    "section": "\n3.10 In summary",
    "text": "3.10 In summary\n\nIn this tutorial, you have learned how to wrangle data in R using functions from packages in the tidyverse suite of packages. You can now construct a complete pipeline of R code to take raw data and transform it into the format(s) we need to effectively map crimes.\n\n\nDeveloping your data wrangling skills will help you to produce better, faster analysis of crime (and other) data. If you would like to develop your skills further, you might be interested in:\n\n\nData Wrangling with R by Claudia Engel, a free online book that explains the functions introduced in this tutorial (and some others) in more detail.\n\nData transformation with dplyr cheat sheet by the team that makes RStudio, which provides a handy two-page guide to the main functions in the dplyr package, which is very useful for reminding you of the code needed to run each of the functions we have used in this tutorial.\n\nData wrangling with R and RStudio by Garrett Grolemund, a recording of a webinar covering the data-wrangling functions introduced in this tutorial and some other very-useful related functions.\n\nR for Data Science by Hadley Wickham and Garrett Grolemund, a free online book that is the bible for wrangling data in R.\n\n\n\nArtwork by @allison_horst",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Wrangling data</span>"
    ]
  },
  {
    "objectID": "04_code_with_style/index.html#introduction",
    "href": "04_code_with_style/index.html#introduction",
    "title": "\n4  Code with style\n",
    "section": "\n4.1 Introduction",
    "text": "4.1 Introduction\nNow that you’re beginning to write code in R, it’s time to introduce a few conventions for how to write code so that it’s easier to read. This is important because “good coding style is like correct punctuation: you can manage without it, butitsuremakesthingseasiertoread”.\nWriting readable code is particularly important because it is much easier to find mistakes in well-written code, and your code will inevitably contain mistakes (everyone’s code contains mistakes!).\nWriting code has been compared to performing in a band:\n\nI liken the situation to a group of musicians trying to form a band. Each one comes in believing that their way of doing things is best (their “method” or “process”). The band will struggle so long as everyone is trying to do their own thing. It’s impossible to create good music unless everyone in the band agrees on the tempo, the style and who should take lead during a song. Anyone who has ever heard a high school band perform knows this to be true. Unless everyone is on the same page, you aren’t going to accomplish much.\n\nYou might be relaxing at this point, thinking “that isn’t a problem for me, because I’m the only person who is going to be working on my code”. If so, think again. It’s been said that in data science that there are always at least two people working on a project: the you who is working on the code now, and the past you who has worked on the same code previously. The problem is that past you does not answer emails. So you can save future you a lot of hassle later by writing readable code.\nThis tutorial introduces some basic guidelines on formatting your code. This is a condensed version of the The tidyverse style guide, which provides lots more detail. All the code you see in the tutorials in this course was written following this style guide.\n\n\n\nOn Perl from Three Panel Soul. Some content on this page contains public sector information licensed under the Open Government Licence v3.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Code with style</span>"
    ]
  },
  {
    "objectID": "04_code_with_style/index.html#organising-your-code",
    "href": "04_code_with_style/index.html#organising-your-code",
    "title": "\n4  Code with style\n",
    "section": "\n4.2 Organising your code",
    "text": "4.2 Organising your code\nUp until now we have only written code in the code boxes within these interactive tutorials, but when you move on to make maps using your own R code you will typically write that code in the RStudio source panel. Your code will usually be in one or more files with the .R file extension. Whether you keep all your code for a specific project in one file or split it into multiple files is up to you. Generally you should do whatever makes it easier to understand how your code is structured.\n\n4.2.1 Leaving notes for future you\nWithin each .R file, you can make your code easier to understand in several ways. First, add a comment (one or more lines beginning with # followed by a space) at the top of the file to explain what the code in that file does. This will make it easier for you to know that you’ve found the right file if you are looking for it in a few weeks when you’ve forgotten (which you will) what file contains what code.\n# This code produces a density map of bike thefts in Vancouver in 2020\nComments should usually start with a capital letter and follow normal English rules of punctuation, spacing, etc.\n\n4.2.2 Letting your code breathe\nUnless your code is very simple, it will probably consist of several separate tasks that are completed one after another. For example your code might download some data, wrangle it and then plot it on a map. In that case, it can be useful to split your code up into blocks by leaving a blank line between the code needed for each task. For example, if we take the code:\nlibrary(lubridate)\nlibrary(tidyverse)\ncrimes &lt;- read_csv(\"crime_data.csv\")\ncrimes &lt;- janitor::clean_names(crimes)\nburglaries &lt;- filter(crimes, type == \"burglary\")\nburglaries &lt;- mutate(burglaries, month = month(date_time))\nggplot() + \n  geom_point(aes(x = lon, y = lat, colour = month)) +\n  theme_void()\nit becomes easier to read if we split the code up into four tasks: loading the necessary packages, reading the data, wrangling the data and plotting the data.\nlibrary(lubridate)\nlibrary(tidyverse)\n\ncrimes &lt;- read_csv(\"crime_data.csv\")\n\ncrimes &lt;- janitor::clean_names(crimes)\nburglaries &lt;- filter(crimes, type == \"burglary\")\nburglaries &lt;- mutate(burglaries, month = month(date_time))\n\nggplot(burglaries) + \n  geom_point(aes(x = lon, y = lat, colour = month)) +\n  theme_void()\nSince data wrangling involves several steps and each function uses the result of the previous step, we could use the pipe operator |&gt; to make that code a bit cleaner:\nlibrary(lubridate)\nlibrary(tidyverse)\n\ncrimes &lt;- read_csv(\"crime_data.csv\")\n\nburglaries &lt;- crimes |&gt; \n  janitor::clean_names() |&gt; \n  filter(type == \"burglary\") |&gt; \n  mutate(month = month(date_time))\n\nggplot(burglaries) + \n  geom_point(aes(x = lon, y = lat, colour = month)) +\n  theme_void()\n\n4.2.3 Header comments\nIf your code includes very long tasks (e.g. where the code takes up more than a full screen on your computer), you might want to use header comments to divide your code into sections. You can do this by writing a comment that is followed by four of more hyphens (----):\n# Load data ----\n\n… some code …\n\n\n# Wrangle data ----\n\n… some code …\n\n\n# Plot data ----\n\n… some code …\nRStudio will recognise lines that end in four or more hyphens as being headings, and will create a table of contents for your code. You can use this to move between headings by clicking on the Jump To menu at the bottom of the Source panel in RStudio:\n\n\n\n\n\nIn general, writing code that is readable is more important than writing the shortest code possible, so don’t be afraid to let your code breathe by using space to separate your code into meaningful chunks.\n\n\nquiz(\n  caption = \"\",\n  \n  question(\n    \"How do you write a comment in R?\",\n    answer(\"A `#` symbol followed by a space. Comments should start with a capital leter. e.g. `# Load data`\", correct = TRUE),\n    answer(\n      \"A `#` symbol, followed by no spaces. Comments should start with a capital letter. e.g. `#Load data`\",\n      message = \"Not quite right: comments should start with a capital letter but should also have a space after the initial `#` symbol.\"\n    ),\n    answer(\n      \"A `#` symbol followed by a space. Comments should be in lower case unless the comment is long enough to contain more than one sentence. e.g. `# load data`\",\n      message = \"Not quite right: comments should start with a space after the initial `#` symbol, but the comment text should start with a capital letter.\"\n    ),\n    answer(\n      \"A `#` symbol, followed by no spaces. Comments should be in lower case unless the comment is long enough to contain more than one sentence. e.g. `#load data`\",\n      message = \"Not quite right: comments should start with a space after the initial `#` symbol and the comment text should start with a capital letter.\"\n    ),\n    correct = random_praise(),\n    allow_retry = TRUE,\n    random_answer_order = TRUE\n  )\n  \n)",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Code with style</span>"
    ]
  },
  {
    "objectID": "04_code_with_style/index.html#naming-objects",
    "href": "04_code_with_style/index.html#naming-objects",
    "title": "\n4  Code with style\n",
    "section": "\n4.3 Naming objects",
    "text": "4.3 Naming objects\nR objects can have any name you like, as long as the name starts with a letter and contains only letters, numbers, dots (.) and underscores (_). That said, you will find coding easier if you follow a few conventions.\n\nUse only lower-case letters in the names of objects, which avoids you having to remember whether a particular letter was upper- or lower-case.\nUse snake case (object_name, with words separated by underscores) for object names rather than camel case (objectName) or kebab case (object-name).\nDon’t use dots in object names.\nDon’t give objects the same names as R functions, because re-using function names makes reading your code more difficult.\n\n\n\n\n\n\nJust as crime_data_atlanta_2020.csv is a more-useful file name than data_file_23.csv, you will find it easier to read your code if you give your objects meaningful names. So when you load data into R (e.g. with read_csv()) don’t just call it data (not least because there is a function named data()) but instead give it a name like atlanta_crimes if it contains (for example) crime data from Atlanta.\n\n\nquiz(\n  caption = \"\",\n  \n  question(\n    \"Which of these would be a good name for an object containing data on homicides in Abu Dhabi?\",\n    answer(\"`abu_dhabi_homicides`\", correct = TRUE),\n    answer(\n      \"`homicide_data`\",\n      message = \"Not quite right: this name is in the right case, but could better describe the data it contains.\"\n    ),\n    answer(\n      \"`Abu_Dhabi_homicides`\",\n      message = \"Not quite right: it's best to only use lower-case characters in variables, so you don't need to remember whether you've used upper- or lower-case characters.\"\n    ),\n    answer(\n      \"`abu-dhabi-homicides`\",\n      message = \"This will cause an error because the `-` indicates R should subtract one value from another.\"\n    ),\n    correct = random_praise(),\n    allow_retry = TRUE,\n    random_answer_order = TRUE\n  )\n  \n)",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Code with style</span>"
    ]
  },
  {
    "objectID": "04_code_with_style/index.html#spacing",
    "href": "04_code_with_style/index.html#spacing",
    "title": "\n4  Code with style\n",
    "section": "\n4.4 Spacing",
    "text": "4.4 Spacing\nSpacing out code makes it much easier to read, but (just as in any language) code is easiest to read if spaces are used where people expect them to be by convention.\nMostly in R, we use spaces where we would expect them in English: after commas but not before, outside parentheses but not inside, etc.\n# Good\nread_csv(\"crime_data.csv\", skip = 4)\n\n# Bad\nread_csv(\"crime_data.csv\",skip = 4)\nread_csv(\"crime_data.csv\" ,skip = 4)\nread_csv(\"crime_data.csv\" , skip = 4)\nDon’t put spaces inside parentheses, or between the names of functions and the parentheses:\n# Good\nmean(x, na.rm = TRUE)\n\n# Bad\nmean (x, na.rm = TRUE)\nmean( x, na.rm = TRUE )\nDo put spaces around most operators (==, +, -, &lt;-, etc.), including either side of = when specifying the values of function arguments:\n# Good\nheight &lt;- (feet * 12) + inches\nmean(x, na.rm = TRUE)\n\n# Bad\nheight&lt;-feet*12+inches\nmean(x, na.rm=TRUE)\nAlthough there are some operators that shouldn’t have spaces around them: $, @, [, [[, ^, : and ?.\n\n\nquestion(\n  \"Which of these lines of code has the optimal spacing?\",\n  answer('`burglary &lt;- filter(crimes, type == \"burglary\")`', correct = TRUE),\n  answer('`burglary&lt;-filter(crimes,type==\"burglary\")`'),\n  answer('`burglary &lt;- filter(crimes, type==\"burglary\")`'),\n  answer('`burglary &lt;- filter ( crimes , type == \"burglary\" )`'),\n  correct = random_praise(),\n  incorrect = random_encouragement(),\n  allow_retry = TRUE,\n  random_answer_order = FALSE\n)",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Code with style</span>"
    ]
  },
  {
    "objectID": "04_code_with_style/index.html#functions",
    "href": "04_code_with_style/index.html#functions",
    "title": "\n4  Code with style\n",
    "section": "\n4.5 Functions",
    "text": "4.5 Functions\nWe’ve now got used to calling functions to do things in R, like calling read_csv() to load data from a CSV file or filter() to choose certain rows from a dataset. We know that we can change the behaviour of functions by using arguments. For example, we can wrap a a string of text into shorter lines using the str_wrap() function from the stringr package. str_wrap() needs two arguments: the text to be wrapped into multiple lines and the maximum length of a line of text before the next word is wrapped onto a new line. These arguments are called string and width, so we can call the function as:\nstr_wrap(string = \"some text to be wrapped\", width = 10)\nThe string argument provides the data that the str_wrap() function will work on, while the width argument provides the details of how that work should be done. Since the data argument to a function is typically required (the function makes no sense without it) and is often the first argument, you can omit the name of data arguments to functions. For all other arguments, it is best to give the argument name. So to use str_wrap(), you can write:\nstr_wrap(\"some text to be wrapped\", width = 10)\nIn general, you should keep lines of code to a maximum of 80 characters long, since they can easily fit on most screens and are easy to read. When calling a function, put all of the parameters on a single line if they will fit into 80 characters or less:\ndo_something_simple(\"something\", with = \"only\", short, \"arguments\")\nBut if the function call is longer than 80 characters, use one line each for the function name, each argument, and the closing ), with the arguments indented by two spaces. This makes the code much easier to read.\n# Good\ndo_something_very_complicated(\n  something = \"that\",\n  requires = many,\n  arguments = \"some of which may be long\"\n)\n\n# Bad\ndo_something_very_complicated(\"that\", requires, many, arguments,\n                              \"some of which may be long\"\n                              )\n\nOne mistake that people often make when splitting a function across multiple lines is to leave the closing parenthesis ) at the end of the last line of the function, like this:\n# Bad\ndo_something_very_complicated(\n  something = \"that\",\n  requires = many,\n  arguments = \"some of which may be long\")\nThe problem with this is that it makes it harder to see where a particular function ends. Instead, put the closing parenthesis on a line on its own, indented with the same number of spaces as the first line of the function:\n# Good\ndo_something_very_complicated(\n  something = \"that\",\n  requires = many,\n  arguments = \"some of which may be long\"\n)\nThis makes it much easier to see where a function call starts and ends.\n\nWhen combining multiple functions using the pipe operator (|&gt;), put each function on a single line, with all but the first line indented by two spaces:\na_function() |&gt; \n  another_function() |&gt; \n  and_a_third_function()\nOnce you learn about other types of R code you will need to know how best to style it, but we will learn about those when we need to.\n\n\nquiz(\n  caption = \"\",\n  \n  question(\n    \"If a function is written across several lines of code because it will not fit on one line, where should the closing parenthesis `)` at the end of the function be placed?\",\n    answer(\"On a separate line, indented as much as the function name.\", correct = TRUE),\n    answer(\n      \"On a separate line, indented as much as the arguments on the lines above it.\",\n      message = \"Not quite right: the closing parenthesis should be a separate line, but indented as much as the function name so that you can easily see where the function begins and ends.\"\n    ),\n    answer(\"At the end of the final function argument, on the same line.\"),\n    answer(\n      \"It doesn't matter where the closing parenthesis is placed as long as there is one.\",\n      message = \"While your code will wherever you place the closing parenthesis, the coding style guide introduced in this chapter is designed to make your code easier to read. This is important because if your code is easier to read that will make it easier to find and fix any mistakes in it.\"\n    ),\n    correct = random_praise(),\n    allow_retry = TRUE,\n    random_answer_order = TRUE\n  )\n  \n)",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Code with style</span>"
    ]
  },
  {
    "objectID": "04_code_with_style/index.html#styling-your-code-automatically",
    "href": "04_code_with_style/index.html#styling-your-code-automatically",
    "title": "\n4  Code with style\n",
    "section": "\n4.6 Styling your code automatically",
    "text": "4.6 Styling your code automatically\nYou can get help on styling your R code using the styler package, which can automatically format your code for you. After you install the styler package with the code install.packages(\"styler\"), you can style your code by:\n\nselecting the code you want to style,\nopening the Addins menu at the top of the Source panel in RStudio,\nclicking ‘Style selection’ in the ‘Styler’ section of the list of addins.\n\n\n\n\n\n\nRStudio will also try to help style your code as you type, for example by automatically indenting lines.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Code with style</span>"
    ]
  },
  {
    "objectID": "04_code_with_style/index.html#in-summary",
    "href": "04_code_with_style/index.html#in-summary",
    "title": "\n4  Code with style\n",
    "section": "\n4.7 In summary",
    "text": "4.7 In summary\n\nYou now know how to write your R code so that it is easy to read, which makes it much easier to understand. Understanding code when you read it is important because it allows you to work out what the code is trying to achieve and because it makes it much easier to find and fix problems when your code is not behaving as you want it to.\n\n\nWriting readable, understandable code is important. To find out more about this, read some of these articles:\n\n\nWhy coding style matters by Nicholas Zakas.\n\nThe tidyverse style guide by Hadley Wickham, which is the basis for the rules outlined in this tutorial.\n\n\n\nThe tidyverse style guide licensed under the Creative Commons Attribution-ShareAlike licence. Artwork by @allison_horst",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Code with style</span>"
    ]
  },
  {
    "objectID": "05_your_second_crime_map/index.html#introduction",
    "href": "05_your_second_crime_map/index.html#introduction",
    "title": "\n5  Your second crime map\n",
    "section": "\n5.1 Introduction",
    "text": "5.1 Introduction\nWe’ve already produced a simple map of crime in a neighbourhood, but we skipped over a lot of the details of how to do it. In this tutorial we will make another crime map, this time focusing more on each step in the process.\nIn this tutorial we will make a map of bicycle thefts in Vancouver in 2020. At the end of this tutorial, the final map will look something like this. We will then improve it further in a future tutorial on what makes a good map.\n\n\n\n\n\nBefore we get into the detail of how to make this map, watch this video that goes over the main points of the code we will use:",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Your second crime map</span>"
    ]
  },
  {
    "objectID": "05_your_second_crime_map/index.html#handling-spatial-data",
    "href": "05_your_second_crime_map/index.html#handling-spatial-data",
    "title": "\n5  Your second crime map\n",
    "section": "\n5.2 Handling spatial data",
    "text": "5.2 Handling spatial data\nMaps are visual representations of spatial data. Spatial data is special because each row in the data is associated with some geographic feature such as a building, a street or the boundary of a neighbourhood. This adds some quirks that we have to understand to work with spatial data successfully.\nMaps are made up of multiple layers of spatial data that are styled to represent features of interest and then stacked on top of one another to make the finished map. Watch this video to learn more about spatial layers and the different types of data that we can use in maps.\n\nPoints, lines and polygons in spatial data are known as geometric objects or simply geometries. Spatial data is data that has a geometric object (e.g. a pair of co-ordinates representing a crime location) associated with each row.\n\n5.2.1 Representing places on the earth\nWith any spatial data, we need a way of describing where on the earth a particular point (such as the location of a crime or the corner of a building) is located. Watch this video to find out about the different co-ordinate systems we can use to do this.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Your second crime map</span>"
    ]
  },
  {
    "objectID": "05_your_second_crime_map/index.html#spatial-data-in-r",
    "href": "05_your_second_crime_map/index.html#spatial-data-in-r",
    "title": "\n5  Your second crime map\n",
    "section": "\n5.3 Spatial data in R",
    "text": "5.3 Spatial data in R\n\n\nThere are several packages that handle raster map data from different sources – one of them is the ggspatial package that we have already used to load the base map of Atlanta for the homicide map we made in one of the earlier tutorials.\n\nVector data can be handled in R using functions from the sf package. SF stands for ‘simple features’, which is a standard for storing spatial data. SF objects are data frames that have a special column to hold the geometry (point, line or polygon) associated with each row in the data. SF objects also understand what co-ordinate system the geometry are described in. This means SF objects can be transformed between co-ordinate systems and combined together in layers on a map.\nThere a lots of functions in the sf package for handling spatial data. Almost all of these functions begin with the letters st_ (e.g. st_read()), which makes it easy to identify that those functions are designed to be used on SF objects.\n\n5.3.1 Reading spatial data\nThe special features of spatial data – needing to store geometries, details of the projection used etc. – mean that spatial data is often stored in special file formats. There are lots of spatial-data formats, but fortunately almost all of them can be read by the st_read() function. This means we do not need to learn a different function for each spatial-data format.\nWhile datasets with line or polygon geometries must almost always be stored in specific spatial-data formats, point data can also be stored in common data formats such as Excel and CSV files. The data for this tutorial is provided by the Vancouver Police Department in a CSV file (gzipped to reduce the file size). The file is located at:\nhttps://mpjashby.github.io/crimemappingdata/vancouver_thefts.csv.gz\n\nThinking back to the tutorial on data wrangling, what R code is needed to load this data into a tibble called thefts? Type the R code into the box below and click Run Code to see the result. If you need help, click the Solution button, but try to remember the code (or look up your notes from the data-wrangling tutorial) before revealing the solution – you’ll learn a lot more that way. Remember to load any necessary packages using the library() function!\n\n# Since the data are stored in a regular CSV file, we can use the `read_csv()`\n# function from the readr package to read the file, and the assignment operator\n# `&lt;-` to store the data in the object `thefts`. `read_csv()` can read directly\n# from a URL, so there is no need to download the data first.\nlibrary(tidyverse)\n\nthefts &lt;- read_csv(\"https://mpjashby.github.io/crimemappingdata/vancouver_thefts.csv.gz\")\n\nRows: 21918 Columns: 10\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (3): TYPE, HUNDRED_BLOCK, NEIGHBOURHOOD\ndbl (7): YEAR, MONTH, DAY, HOUR, MINUTE, X, Y\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nNow that you have stored the data in the thefts object, what code is needed to view the first few rows of data? Type the code into the box and click Run Code to check the result.\n\nhead(thefts)\n\n# A tibble: 6 × 10\n  TYPE   YEAR MONTH   DAY  HOUR MINUTE HUNDRED_BLOCK NEIGHBOURHOOD      X      Y\n  &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;         &lt;chr&gt;          &lt;dbl&gt;  &lt;dbl&gt;\n1 Othe…  2020     1     1     0      0 11XX BURNABY… West End      4.90e5 5.46e6\n2 Othe…  2020     1     1     0      0 13XX W 71ST … Marpole       4.90e5 5.45e6\n3 Othe…  2020     1     1     0      1 18XX E GEORG… Grandview-Wo… 4.95e5 5.46e6\n4 Othe…  2020     1     1     0      0 2X ALEXANDER… Central Busi… 4.92e5 5.46e6\n5 Thef…  2020     1     1     0      0 11XX SKEENA … Hastings-Sun… 4.98e5 5.46e6\n6 Thef…  2020     1     1     0     10 14XX LABURNU… Kitsilano     4.89e5 5.46e6\n\n\n\nThe data consists of 21,918 rows, each representing one theft. Before we can map this data, we will need to do some minor data wrangling to get it into the format we want.\n\n5.3.2 Reading data files from your computer\nSo far we have created datasets by loading data directly from URLs, such as the URL for the Vancouver thefts data we loaded in the previous section. But we can also load datasets that are already stored on our own computers. To do this, we need to know the file path that specifies where a particular file is stored.\nYou might have encountered file paths before, but you may not. A typical file path looks like this on a Mac or Linux computer:\n/Users/john_smith/Documents/Crime Mapping/vancouver_thefts.csv\nor like this on Windows:\nC:\\Users\\john_smith\\Documents\\Crime Mapping\\vancouver_thefts.csv\nThe important thing to note here is that computers store files such as vancouver_thefts.R in folders (also called directories), which are themselves often stored inside larger folders, etc. A file path tells a computer where to find a particular file. The file paths above can be read as telling the computer to open the Users directory, then the matt_ashby directory, then the Documents directory, then the Crime Mapping directory, and finally the file vancouver_thefts.csv.\nYou can find the file path of a particular file by typing file.choose() in the R console. This will open a new window that allows you to search or browse for a particular file. When you choose the file, R will print the file path in the console.\nThe two file paths shown above are called absolute file paths, because they show the full location of a particular file on the computer. But there are two problems with absolute paths: they can be very long so they clutter up your code, and (more importantly) they are only correct for a specific computer. If you write an R script that includes either of the file paths above, then you give that file for me to run, the code will immediately produce an error because there is no directory /Users/john_smith on my computer.\nWe can deal with that problem in a few ways. The first is to use a relative path. This specifies the location of a file not on the computer as a whole, but relative to the directory we are currently working in. This working directory will depend on how your version of RStudio is configured, but you can find out the working directory by typing getwd() in the R console. This will print the absolute path of the working directory for whatever you are currently working on in RStudio.\nImagine our working directory is /Users/john_smith/Documents/Crime Mapping/. If we wanted to open the file /Users/john_smith/Documents/Crime Mapping/vancouver_thefts.csv, we could use the absolute file path:\nread_csv(\"/Users/john_smith/Documents/Crime Mapping/vancouver_thefts.csv\")\nHowever, since the file is stored in our current working directory, we could also open the file like this:\nread_csv(\"vancouver_thefts.csv\")\nThis works because when we provide a relative path (e.g. one that does not begin with \\ on Mac or C:\\ on Windows), R treats the path as being relative to the current working directory. Since the file vancouver_thefts.csv is in the working directory, we don’t need to specify anything other than the file name.\nIf we wanted to read from a file that was in a directory within the working directory – e.g. /Users/john_smith/Documents/Crime Mapping/original_data/canada/vancouver_crime.gpkg – then we just need to specify where the file is relative to the working directory:\nread_sf(\"original_data/canada/vancouver_crime.gpkg\")\n\nFile paths in your code can make it harder to make sure your code is portable, i.e. that it can be used by other people. This is because the file structure on someone else’s computer is likely to be different from the file structure on your computer.\nYou might think code portability doesn’t matter, because you are not planning to share your code with anyone. But it’s quite possible that you will want to:\n\nRe-use code yourself on a different computer, e.g. if you replace your current computer with a new one.\nSend the code to someone else to get help with a problem.\nSend the code to someone else who has asked for help from you.\n\nTo help keep your code portable:\n\nWhere possible, include in your code any code needed to download the necessary data from the internet. This is what we most-often do in this course.\nIf the data is not available online, you will need to distribute the data along with your code. In that case, tell anyone you send you code to that they should keep the code file and data files in the same directory, then you can just refer to each data file by its name (e.g.  read_csv(\"vancouver_thefts.csv\")).\nAvoid using absolute file paths in your code, since these will almost certainly produce an error if anyone tries to run your code on another computer.\n\n\n\n5.3.3 Cleaning column names\n\n\nIn a previous tutorial I recommended choosing snake_case object names, e.g. calling a data object atlanta_robberies_2020 rather than atlantarobberies2020, AtlantaRobberies2020 or ATLANTAROBBERIES2020. This makes your code easier to read and means you don’t have to remember whether you named a variable using upper- or lower-case letters, since you know that you only use lower case.\nThe same recommendation applies to variable names in datasets, for the same reasons. Doing this makes it much easier to refer to objects and columns in your code, without having to worry about whether a particular letter was upper- or lower-case, or whether it had an accent etc.\n\nAt the moment, the column names in the thefts dataset are upper-case letters. Rather than having to remember this, we can easily convert them to snake case using the clean_names() function from the janitor package. To use a function from a package, we usually first load the package using the library() function. In this case, we probably won’t want to use any other functions from the janitor package, so instead of loading the whole package we will use this one function directly. To do this, we write the function name with the package name added to the front, separated by two colons ::.\n\nthefts &lt;- janitor::clean_names(thefts)\n\nhead(thefts)\n\n# A tibble: 6 × 10\n  type   year month   day  hour minute hundred_block neighbourhood      x      y\n  &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;         &lt;chr&gt;          &lt;dbl&gt;  &lt;dbl&gt;\n1 Othe…  2020     1     1     0      0 11XX BURNABY… West End      4.90e5 5.46e6\n2 Othe…  2020     1     1     0      0 13XX W 71ST … Marpole       4.90e5 5.45e6\n3 Othe…  2020     1     1     0      1 18XX E GEORG… Grandview-Wo… 4.95e5 5.46e6\n4 Othe…  2020     1     1     0      0 2X ALEXANDER… Central Busi… 4.92e5 5.46e6\n5 Thef…  2020     1     1     0      0 11XX SKEENA … Hastings-Sun… 4.98e5 5.46e6\n6 Thef…  2020     1     1     0     10 14XX LABURNU… Kitsilano     4.89e5 5.46e6\n\n\nYou can see that the data has stayed the same but all the column names are now in snake case. clean_names() would also have replaced any spaces with underscores, tried to separate words in the variable names and cleaned up several other potential problems. For this reason it is common to call janitor::clean_names() straight away after loading a dataset so that you can be confident that the column names will be in the format you expect.\nIf we wanted to use the clean_names() function again, we would have to include the package name and :: each time, so if our code was going to make repeated use of the function then it would probably be easier to load the package using the library() function as we have done in previous tutorials.\n\n5.3.4 Converting our data to an SF object\nAt present, the data in the thefts object is just a regular tibble. We could not use it to make a map because R does not know which columns represent the geometry, or what co-ordinate system the locations are recorded in. We can deal with this by converting the data to an SF object using the st_as_sf() function from the sf package.\nThe data provided by the Vancouver Police use the UTM zone 10N co-ordinate system. UTM is a system for assigning co-ordinates to any location on earth relative to a local origin point for the UTM zone covering that part of the planet. It is therefore similar to the British National Grid that we have already learned about, but for any part of the globe. The ‘N’ at the end of the zone name refers to the northern hemisphere.\n\nIn almost all cases, co-ordinate reference systems only work for the part of the world that they were designed for. So we should not use the UTM zone 10N co-ordinate system to map data outside the area for which it was designed (broadly speaking, the west coast of North America from Los Angeles to Vancouver, and the part of Canada directly north of Vancouver extending as far as the north pole). If we were to use the UTM zone 10N co-ordinate system for data from another part of the world, we would be very likely to get error messages or strange results.\n\n\n\n\n\n\nWe can convert the thefts tibble to an SF object using the st_as_sf() function (remember, all functions in the sf package start with st_, which can sometimes make the function names a little confusing). We specify which columns in the data represent the geometry (in this case, the x and y columns), and what co-ordinate system the data uses.\nCo-ordinate systems can be specified in lots of ways (some very complicated), but the easiest is to specify the EPSG code for the relevant system. An EPSG code is a unique reference number for a particular co-ordinate system that R can look up in a database to get the information needed to display the data on a map. The EPSG code for the UTM zone 10N is EPSG:32610.\n\nlibrary(sf)\n\nthefts_sf &lt;- st_as_sf(thefts, coords = c(\"x\", \"y\"), crs = \"EPSG:32610\")\n\nhead(thefts_sf)\n\nSimple feature collection with 6 features and 8 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 488991.7 ymin: 5450474 xmax: 497938.7 ymax: 5458985\nProjected CRS: WGS 84 / UTM zone 10N\n# A tibble: 6 × 9\n  type                year month   day  hour minute hundred_block  neighbourhood\n  &lt;chr&gt;              &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;          &lt;chr&gt;        \n1 Other Theft         2020     1     1     0      0 11XX BURNABY … West End     \n2 Other Theft         2020     1     1     0      0 13XX W 71ST A… Marpole      \n3 Other Theft         2020     1     1     0      1 18XX E GEORGI… Grandview-Wo…\n4 Other Theft         2020     1     1     0      0 2X ALEXANDER … Central Busi…\n5 Theft from Vehicle  2020     1     1     0      0 11XX SKEENA ST Hastings-Sun…\n6 Theft from Vehicle  2020     1     1     0     10 14XX LABURNUM… Kitsilano    \n# ℹ 1 more variable: geometry &lt;POINT [m]&gt;\n\n\nIf you look at the contents of the thefts_sf object, you’ll see that there is a new column called geometry (you may need to use the ▸ button to see it). This column contains the co-ordinates of each bike theft in a format that R recognises represent locations on the surface of the earth, which means they can be used to make maps.\n\nIt is important to remember that we should only use st_as_sf() to convert a non-spatial dataset (such as a tibble) into a spatial dataset (an SF object). If we use st_as_sf() on an object that is already an SF object, this can have unexpected results and lead to errors in your code.\nThe easy way to think about this is that if you have loaded a dataset with read_sf() or st_read() then you have already created an SF object, so you don’t need st_as_sf(). If you have loaded a dataset with any other function that reads data (such as read_csv() or read_excel()) then you will need to use st_as_sf() if you want to plot the data on a map. Most importantly, do not use st_as_sf() if you loaded a dataset with read_sf() or st_read().\n\n\n\n5.3.5 Finding bike thefts in our data\n\nIf you look through the contents of the thefts_sf object, you will see that not all of the rows relate to bicycle thefts. The type column shows that the dataset also includes thefts from vehicles, for example. To choose only those rows containing bicycle thefts, which function from the dplyr package would we used? If you need help, you can think back to the data-wrangling tutorial or have a look at the Data transformation with dplyr cheat sheet.\n\nquestion(\n  \"Which function from the `dplyr` package should we use to remove all the rows from our dataset except those for bicycle thefts?\",\n  answer(\"`filter()`\", correct = TRUE),\n  answer(\"`select()`\", message = \"Nearly right. The `select()` function allows us to choose particular *columns* from our dataset -- in this case, we want to choose particular *rows*.\"),\n  answer(\"`mutate()`\", message = \"Not quite. The `mutate()` function allows us to change the values in existing columns in our dataset, or add new columns.\"),\n  answer(\"`summarise()`\", message = \"Not quite. The `summarsise()` function allows us to create summaries of the values of columns, either for the whole dataset or groups of rows.\"),\n  correct = random_praise(),\n  incorrect = random_encouragement(),\n  allow_retry = TRUE,\n  random_answer_order = TRUE\n)\n\nType the code needed to choose only the rows of data that relate to bicycle thefts and store it in a new object called bike_thefts. If you get stuck, you can click the Hint buttons to get help, but try to find the answer on your own first!\n\n# Use the `filter()` function to choose particular rows in a dataset. The syntax \n# for `filter()` is `filter(dataset, column_name == \"value\")`\n\n\n# In the code `filter(dataset, column_name == \"value\")`, replace `dataset` with\n# the name of the SF object you have already created from the `thefts` tibble,\n# `column_name` with the name of the column containing the offence type and\n# `value` with the offence type for bicycle theft.\n\n\n# The correct code to store only bicycle thefts in a new object is:\nbike_thefts &lt;- filter(thefts_sf, type == \"Theft of Bicycle\")\n\nOur data is now ready for us to make our crime map!\n\n\nIf you look through the contents of the thefts_sf object, you will see that not all of the rows relate to bicycle thefts. The type column shows that the dataset also includes thefts from vehicles, for example. To choose only those rows containing bicycle thefts, we can use the filter() function from the dplyr package.\nbike_thefts &lt;- filter(thefts_sf, type == \"Theft of Bicycle\")\n\nSo far in this tutorial we have used several functions – read_csv(), clean_names(), st_as_sf() and filter() to produce an SF object representing the locations of bike thefts. Since we have done this step by step, we have created a different object to store the result produced by each function. But since we only need the final dataset, our code would be a lot easier to read if we used the pipe operator (|&gt;) to run all these functions in one go:\nbike_thefts &lt;- read_csv(\"https://mpjashby.github.io/crimemappingdata/vancouver_thefts.csv.gz\") |&gt;\n  janitor::clean_names() |&gt; \n  st_as_sf(coords = c(\"x\", \"y\"), crs = \"EPSG:32610\") |&gt; \n  filter(type == \"Theft of Bicycle\")",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Your second crime map</span>"
    ]
  },
  {
    "objectID": "05_your_second_crime_map/index.html#producing-maps-in-r",
    "href": "05_your_second_crime_map/index.html#producing-maps-in-r",
    "title": "\n5  Your second crime map\n",
    "section": "\n5.4 Producing maps in R",
    "text": "5.4 Producing maps in R\nNow that we have our data, we can use it to create a map of bicycle theft in Vancouver. Before we start, let’s take another look at our dataset so that we know which columns contain which data.\n\nType the code needed to view the first few rows of the bike_thefts dataset.\n\nhead(bike_thefts)\n\nSimple feature collection with 6 features and 8 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 488371.3 ymin: 5452696 xmax: 494295.1 ymax: 5458232\nProjected CRS: WGS 84 / UTM zone 10N\n# A tibble: 6 × 9\n  type              year month   day  hour minute hundred_block    neighbourhood\n  &lt;chr&gt;            &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;            &lt;chr&gt;        \n1 Theft of Bicycle  2020     1     1     0      0 12XX VENABLES ST Strathcona   \n2 Theft of Bicycle  2020     1     1     0      0 20XX MAPLE ST    Kitsilano    \n3 Theft of Bicycle  2020     1     1    13      0 7XX PACIFIC BLVD Central Busi…\n4 Theft of Bicycle  2020     1     1    20      0 53XX VINE ST     Arbutus Ridge\n5 Theft of Bicycle  2020     1     3    11     55 65XX ANGUS DR    Kerrisdale   \n6 Theft of Bicycle  2020     1     3    14      0 4XX E 10TH AVE   Mount Pleasa…\n# ℹ 1 more variable: geometry &lt;POINT [m]&gt;\n\n\n\n\n\nhead(bike_thefts)\n\nSimple feature collection with 6 features and 8 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 488371.3 ymin: 5452696 xmax: 494295.1 ymax: 5458232\nProjected CRS: WGS 84 / UTM zone 10N\n# A tibble: 6 × 9\n  type              year month   day  hour minute hundred_block    neighbourhood\n  &lt;chr&gt;            &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;            &lt;chr&gt;        \n1 Theft of Bicycle  2020     1     1     0      0 12XX VENABLES ST Strathcona   \n2 Theft of Bicycle  2020     1     1     0      0 20XX MAPLE ST    Kitsilano    \n3 Theft of Bicycle  2020     1     1    13      0 7XX PACIFIC BLVD Central Busi…\n4 Theft of Bicycle  2020     1     1    20      0 53XX VINE ST     Arbutus Ridge\n5 Theft of Bicycle  2020     1     3    11     55 65XX ANGUS DR    Kerrisdale   \n6 Theft of Bicycle  2020     1     3    14      0 4XX E 10TH AVE   Mount Pleasa…\n# ℹ 1 more variable: geometry &lt;POINT [m]&gt;\n\n\n\n\n5.4.1 Introduction to ggplot2\n\n\nA map is a specialised type of chart, so we can make maps using the ggplot2 package that is widely used to create other types of chart in R. ggplot2 charts are made up of layers, so they’re well suited to making maps.\n\nThe most-basic map that we can make simply plots the locations of crimes with no context. This almost never makes a good crime map, but we can use this type of map as the foundation around which we can build a better map.\nggplot2 plots work by building up a chart using different functions, each of which adds or modifies some part of the chart. Building a plot starts with calling the ggplot() function, with each subsequent function being added to the plot definition using the + operator. Note that while the package is called ggplot2, the function in that package used to create plots is called ggplot(), not ggplot2().\nThe most-important of the ggplot2 functions are those beginning with geom_, which add graphical elements to the chart. If you want to add a layer to your chart showing a scatter plot, you use the geom_point() function, while if you want to make a line chart you use geom_line().\n\n\n\n\n\nThere are lots of geom_ functions available for representing data on charts in different ways. For maps, the SF package includes the geom_sf() function that is designed to add spatial data (in the form of an SF object such as our bike_thefts data) to a chart, making it into a map. So to simply plot the points in our bicycle-theft data, we can use the code:\n\nlibrary(ggplot2)\n\nggplot() +\n  geom_sf(data = bike_thefts)\n\n\n\n\n\ngeom_sf() only works on SF objects, which is why we needed to convert the original tibble of data to an SF object using st_as_sf(). If you try to use geom_sf() on a dataset that is not stored as an SF object, R will produce an error.\n\nBy convention, each function that we add to ggplot() to change the appearance of our map goes on a new line (this makes the code easier to read) and all but the first line is indented by two spaces. RStudio does this indenting automatically if the previous line ends with a + symbol, since RStudio then understands that there is more code to come on the next line.\nThis map shows the bike-theft data, but it is obviously not a very useful map. Fortunately, we can use the features of the ggplot2 package to build on this basic map.\n\nUnless we want to produce a map of only a very small number of crimes (like the Atlanta downtown homicides map we produced in a previous tutorial), it is unlikely that a point map will be very useful.\nIn fact, if you find yourself making map with each crime represented by a separate point, you should probably stop and ask yourself if that is really the best way to achieve your goal – it will almost always be better to map the data in another way.\n\n\n5.4.2 Controlling aesthetics\nWe can change the appearance of the points by specifying various arguments to the geom_sf() function. These arguments are called aesthetics, because they control the aesthetic appearance of the geometric objects (points, lines etc.) that are produced by a geom_ function. There are lots of aesthetics, but some of the most common are:\n\n\ncolour controls the colour of points and lines (for polygons, it controls the colour of the border around the polygon edge) – you can also use the spelling color for this argument and get an identical result,\n\nfill controls the colour used to fill polygons or points that use a shape capable of having different colours in the centre and around the edge (fill has no meaning for lines),\n\nshape controls the shape (circle, triangle, square etc.) of points (it has no meaning for lines or polygons),\n\nsize controls the size of points and text,\n\nlinewidth controls the width of lines, including the borders around the edges of polygons, and\n\nalpha controls the transparency of a layer (alpha = 1 equals fully opaque, alpha = 0 means fully transparent).\n\ncolour and fill can be specified using any one of 657 R colour names or using a hexidecimal (‘hex’) colour code. Values of size don’t relate to any unit of size (e.g. millimetres or points), so it’s easiest to set the size of points and text by trial and error.\nThere are 25 built-in shapes for points in R (shape 16 is the default):\n\n\n\n\n\n\nChange the code below so that the points on our map are red squares instead of black circles (red is one of the 657 R colour names) and click Run Code to see the result. Use the hints if you need help, but try to work it out on your own first.\n\nggplot() +\n  geom_sf(data = bike_thefts)\n\n\n\n\n\n# Use the `shape` aesthetic to change the points to squares and the `colour`\n# aesthetic to change the point colour to red\n\n\n# Add the arguments `shape = 15` and `colour = \"red\"` to the `geom_sf()`\n# function, remembering that arguments are separated by commas\n\n\nggplot() +\n  geom_sf(data = bike_thefts, shape = 15, colour = \"red\")\n\n\n\n\n\n\nFor example, we could change the points on our map to be red squares rather than the default black circles:\nggplot() +\n  geom_sf(data = bike_thefts, shape = 15, colour = \"red\")\n\nAs we have said, this basic map is not very useful. We can see that there seems to be a cluster of bike thefts towards the top (north) of the map, but it is difficult to see how important this cluster is because so many of the points overlap. Overlapping points are a particular problem in maps, because if there are multiple crimes at the same location then the points representing those crimes will be exactly on top of one another and it will be impossible to see whether there is one crime at a particular location or 100.\n\nOne way to deal with this problem is to make the points semi-transparent so that overlapping points appear darker. This often works better if we also make the points slightly smaller at the same time. Use the alpha and size aesthetics to make the points smaller (relative to the default for points of size = 1) and semi-transparent. Keep changing the values of the two aesthetics until you are happy that the map makes it as easy as possible to see the distribution of bike thefts in Vancouver.\n\nggplot() +\n  geom_sf(data = bike_thefts)\n\n\n\n\n\n# Which values you've chosen will depend on your personal aesthetic preferences,\n# but these values produce a map that makes it easier to see the distribution of\n# points\nggplot() +\n  geom_sf(data = bike_thefts, size = 0.75, alpha = 0.1)\n\n\n\n\n\n\nOne way to deal with this problem is to make the points semi-transparent so that overlapping points appear darker. This often works better if we also make the points slightly smaller at the same time. We can use the alpha and size aesthetics to make the points smaller (relative to the default for points of size = 1) and semi-transparent.\nggplot() +\n  geom_sf(data = bike_thefts, size = 0.75, alpha = 0.1)\n\nMaking the points semi-transparent goes some way to making it easier to see where bike theft is most common in Vancouver, but the pattern is not clear and it is not possible to tell which darker points represent a handful of crimes at the same location and which represent hundreds of crimes at the same location. To make our map useful, we need to use a different technique.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Your second crime map</span>"
    ]
  },
  {
    "objectID": "05_your_second_crime_map/index.html#mapping-crime-density",
    "href": "05_your_second_crime_map/index.html#mapping-crime-density",
    "title": "\n5  Your second crime map\n",
    "section": "\n5.5 Mapping crime density",
    "text": "5.5 Mapping crime density\nA better way to show where crime is concentrated on a map is to work out the density of crime in each area and then map that density. By density in this context, we mean the relative concentration of points in each part of the area we are studying, i.e. how many points (representing bike thefts) are there in each part of the map relative to all the other areas of the map.\nTo estimate the density of points in different areas of the map, R uses a technique called kernel density estimation (KDE). To do this, R must:\n\ndivide the map into a grid of cells, each the same size,\ncount the number of points in each cell,\nfor each cell, count the number of points in nearby cells, but give less weight to (i.e. systematically undercount) those cells that are further away,\nfor each cell, total up the count of points in that cell and the (weighted) count of points in nearby cells – this is the estimate of the density of points in that cell.\n\nThis procedure has the effect of producing a smooth surface representing crime density.\nYou might recognise this procedure as another example of simplifying a complicated spatial dataset by converting it from individual points to a raster that summarises those points as values associated with a grid of cells. In this case, the value associated with each grid cell represents the relative density of points in that cell.\n\n\n\n\n\nWe don’t need to worry at this point about the details of how the counts are used to estimate the density of crimes – we will return to this in a later tutorial.\n\nThere are several ways we can make density maps in R. In this course we will use the sfhotspot package because it makes reasonable default decisions about how or density maps should look, while still giving us control over their appearance if we want it. sfhotspot also has other useful functions that we will use in future tutorials.\nTo create a density map using sfhotspot, we first use the hotspot_kde() function to convert a dataset of offence locations to an estimate of the density of offences for each cell in a grid. hotspot_kde() automatically chooses how big the cells in the grid should be (but we can set this ourselves if we want to).\n\nlibrary(sfhotspot)\n\nbike_theft_density &lt;- hotspot_kde(bike_thefts, quiet = TRUE)\n\nhead(bike_theft_density)\n\nSimple feature collection with 6 features and 2 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 490447.2 ymin: 5450107 xmax: 491647.2 ymax: 5450307\nProjected CRS: WGS 84 / UTM zone 10N\n# A tibble: 6 × 3\n      n   kde                                                           geometry\n  &lt;dbl&gt; &lt;dbl&gt;                                                      &lt;POLYGON [m]&gt;\n1     0 11.1  ((490447.2 5450107, 490447.2 5450307, 490647.2 5450307, 490647.2 …\n2     0 11.1  ((490647.2 5450107, 490647.2 5450307, 490847.2 5450307, 490847.2 …\n3     0 11.0  ((490847.2 5450107, 490847.2 5450307, 491047.2 5450307, 491047.2 …\n4     0 10.8  ((491047.2 5450107, 491047.2 5450307, 491247.2 5450307, 491247.2 …\n5     0 10.5  ((491247.2 5450107, 491247.2 5450307, 491447.2 5450307, 491447.2 …\n6     0  9.96 ((491447.2 5450107, 491447.2 5450307, 491647.2 5450307, 491647.2 …\n\n\nThe bike_theft_density object created by hotspot_kde() contains three columns: n contains the count of bike thefts in each cell, kde contains the estimate of the density of thefts in each cell, and geometry contains the outline of each grid cell.\nSince hotspot_kde() produces an SF object, we can add it to a map using the geom_sf() function. We can also use the fill aesthetic to specify that the fill colour of each grid cell should be determined based on the values of the kde column in the bike_theft_density object.\n\nggplot() +\n  geom_sf(aes(fill = kde), data = bike_theft_density, colour = NA)\n\n\n\n\nWe have already seen that we can set aesthetics such as colour and shape manually, but the aes() function allows us to specify that the values of different aesthetics should be controlled by columns in the data. The aes() function takes as its arguments pairs of values (combined with an = symbol) where the first value is an aesthetic and the second value is the name of a column in the data. For example, to use the colour of points on a map to represent different types of crime that were stored in a column in the data called type, we could use aes(colour = type).\n\nWhen should you specify the values of aesthetics inside aes() and when should you do it outside aes()?\n\nIf you want an aesthetic to have a constant value for all the points, lines or other shapes in a layer, control the aesthetic outside aes(). For example, you could use geom_sf(bike_thefts, colour = \"mediumblue\") to make all the shapes in that layer blue.\nIf you want to vary the appearance of shapes according to values in the data, you should control the aesthetic inside aes(). For example, you could use geom_sf(aes(colour = month), bike_thefts) to vary the colour of shapes in a layer according to values of the month column in the data.\n\nIf you specify a constant value for an aesthetic (e.g. colour = \"mediumblue\") this will over-ride any mapping for that aesthetic provided by the aes() function (e.g. aes(colour = month)). If you have used aes() to specify that an aesthetic should be controlled based on a column in the data but find that the aesthetic is not changing based on the data, check you have not also specified a constant value for that aesthetic.\naes() must be the first argument in the geom_*() function.\n\nIn this map, instead of seeing each crime as a separate point, we see the density of crime as the filled colour of cells in a grid. By comparing this density map to the point map we produced before, we can see that the density map makes the areas with the highest frequency of thefts easier to identify.\nYou can also see that our map now has a legend, showing that higher densities of bike thefts are shown on the map in dark blue and lower densities are shown in light blue. The exact values shown in the legend are not particularly meaningful, so we can ignore these for now (we will come back to this in a future tutorial).\n\n5.5.1 Fine-tuning density maps\nWe can control the appearance of KDE maps in several ways. For example, we can vary the number of cells in the grid and the definition of what cells the kernel density estimation process should consider to be ‘nearby’ for the purposes of calculating weighted counts. Cells are considered to be ‘nearby’ to a particular cell if they are closer to that cell than a distance known as the KDE bandwidth.\nBy default, hotspot_kde() chooses the cell size and the bandwidth automatically. The maps below show how changing these defaults changes the appearance of our map (with the legend and axes removed to make the small maps clearer).\n\n\n\n\n\nBy looking at the maps on the right-hand side, you can see that reducing the number of grid cells leads to a map that looks blocky and lacks information. Looking at maps higher up, you can see that increasing the bandwidth relative to the default makes the density surface smoother. The smoother the surface, the less detail we can see about where crime is most concentrated, until on the top row we can see almost no information at all. On the other hand, if we reduce the bandwidth too much (the bottom row of maps) then almost no ‘nearby’ cells are included in the count and so it becomes more difficult to identify patterns.\nIn most cases, you will not need to change the cell size used in calculating the density of points on a map, but if you do then you can do this using the cell_size argument to hotspot_kde().\nAlthough you can set the bandwidth manually using the bandwidth argument to hotspot_kde(), you will almost never want to do this. Instead, you can vary the bandwidth relative to the automatically chosen default bandwidth by using the bandwidth_adjust argument. For example, if you wanted to see more detail in your map by using a smaller bandwidth, you could use bandwidth_adjust = 0.75 or bandwidth_adjust = 3/4 to set the bandwidth to be three-quarters of the default bandwidth.\n\nChange the code below so that the bandwidth is half of the default bandwidth.\n\nhotspot_kde(bike_thefts, quiet = TRUE) |&gt; \n  ggplot() +\n  geom_sf(aes(fill = kde), colour = NA)\n\n\n\n\n\n# You can set either `bandwidth_adjust = 1/2` or `bandwidth_adjust = 0.5` to \n# get the same result\nhotspot_kde(bike_thefts, quiet = TRUE, bandwidth_adjust = 2/3) |&gt; \n  ggplot() +\n  geom_sf(aes(fill = kde), colour = NA)\n\n\n\n\nComparing this map to the first density map we produced, we can see that the slightly smaller bandwidth means we can see slightly more detail in the patterns of bike thefts.\n\n\nI recommend using a slightly smaller bandwidth than the default, so that your maps show a bit more detail. Try setting bandwidth_adjust = 0.5 whenever you produce a density layer using hotspot_kde(), but remember to look at the map to see if you are happy with the result.\n\nThe final way we can control the appearance of our density layer is to change the colour scheme used to represent density. To do this, we can use another type of ggplot2 function: scales. There are lots of scales available, but the scale_fill_distiller() function produces several different colour scales that are specifically designed to be effective on maps.\nAll the available colour schemes are on the Color Brewer website. Colour schemes can be divided into three types:\n\n\nsequential colour schemes are useful for showing values from low to high,\n\ndiverging colour schemes are useful for showing values relative to a meaningful central point, and\n\nqualitative colour schemes are useful for showing separate categories that can appear in any order and still be meaningful.\n\nIn crime mapping we’re usually interested in showing how crime varies from low to high, so we need to use a sequential colour palette. There are 18 sequential colour schemes (or palettes) available in scales_fill_distiller(), each with a name:\n\n\n\n\n\n\nIt is important to only use the right type of colour scale in the right circumstances, since using the wrong type of scale could end up misleading people reading your map. For example, a diverging colour scale gives the strong impression that the central point in the scale is meaningful.\nIn some circumstances this might be useful, for example if you wanted to show areas in which crime had increased in shades of one colour and areas in which crime had decreased in shades of another colour. In that case, a diverging scale would be appropriate because the central point represents something meaningful: no change in crime. If the central point is not meaningful, use a sequential colour scheme instead.\nIf you want to represent a categorical variable, you should use a categorical colour scale unless the categories have a natural order. For example, if you wanted to show ethnic groups on a map you would use a categorical colour scale since there is no one order of ethnic groups that is any more meaningful than any other. If you wanted to represent days of the week with colour, then you might want to use a sequential colour scheme since the days of the week have a meaningful order.\n\nBy default, scale_fill_distiller() sets the lowest values to have the darkest colour. This often does not work well, but we can change this by setting the argument direction = 1. I recommend doing this in all cases.\n\n\n\n\n\nYou can think of all the functions that we can add to ggplot() as being like a stack of pancakes, with each new function being placed on the top of the stack. To change the colour of our map, we just add scale_fill_distiller() to the existing stack.\n\nggplot() +\n  geom_sf(aes(fill = kde), data = bike_theft_density, colour = NA) +\n  scale_fill_distiller(palette = \"Oranges\", direction = 1)\n\n\n\n\n\nTry changing the code in this box to use one of the other Color Brewer palettes above. Which palette do you like best?\n\n5.5.2 Check your understanding\nAnswer the following questions to check your understanding of what we’ve learned in this section of this tutorial. If you get a question wrong, you can keep trying until you get the right answer.\n\nquiz(\n  caption = \"\",\n  \n  question(\n    \"What R package do we use to make density maps using the `hotspot_kde()` function?\",\n    answer(\"`sfhotspot`\", correct = TRUE),\n    answer(\n      \"`ggplot2`\",\n      message = \"`hotspot_kde()` produces SF objects that can be plotted using `ggplot()`, but the `hotspot_kde()` function itself does not come from the `ggplot2` package.\"\n    ),\n    answer(\n      \"`sf`\",\n      message = \"The `sf` package contains lots of functions for manipulating spatial data, but not for making maps.\"\n    ),\n    answer(\n      \"`dplyr`\",\n      message = \"The `dplyr` package contains functions for data wrangling, which we learned about in a previous tutorial.\"\n    ),\n    correct = random_praise(),\n    allow_retry = TRUE,\n    random_answer_order = TRUE\n  ),\n  \n  question(\n    \"What is the `bandwidth_adjust` argument of the function `hotspot_kde()` used for?\",\n    answer(\n      \"Adjusting the bandwidth of the density layer, relative to the default bandwidth.\",\n      correct = TRUE\n    ),\n    answer(\n      \"Adjusting the cell size of the density layer, relative to the default cell size.\",\n      message = \"We use the `cell_size` argument of `hotspot_kde()` to change the default cell size.\"\n    ),\n    answer(\n      \"Adjusting the colour scheme used to represent density on the map.\",\n      message = \"We use the `scale_fill_distiller()` function to adjust the colour scheme used to represent density on the map.\"\n    ),\n    correct = random_praise(),\n    allow_retry = TRUE,\n    random_answer_order = TRUE\n  ),\n  \n  question(\n    \"In what circumstances is it appropriate to use a diverging colour scale, e.g. one that goes from blue to white to red?\",\n    answer(\"To represent variation in a variable above and below a meaningful mid-point.\", correct = TRUE),\n    answer(\n      \"To represent variation in a variable from low to high.\",\n      message = \"In this case, you should use a sequential scale unless there is a mid-point in the scale that is meaningful.\"\n    ),\n    answer(\n      \"To represent variation in a variable from high to low.\",\n      message = \"In this case, you should use a sequential scale unless there is a mid-point in the scale that is meaningful.\"\n    ),\n    answer(\n      \"To represent categories in a categorical variable.\",\n      message = \"In this case, you should use a categorical colour scheme if the categories have no natural order and a sequential colour scheme if the categories do have a natural order.\"\n    ),\n    correct = random_praise(),\n    allow_retry = TRUE,\n    random_answer_order = TRUE\n  )\n  \n)\n\nYou now have the skills to make a density map of bike theft in Vancouver. Using your notes and the code in this section, complete the code needed to generate a map with:\n\nkernel density calculated using the default number of grid cells and half the default band width, and\nthe density shown using the Color Brewer ‘Purples’ colour scheme.\n\nRemember the data are stored in an object called bike_thefts.\n\nbike_theft_density_bw_half &lt;- hotspot_kde(\n  bike_thefts, \n  quiet = TRUE,\n  bandwidth_adjust = 0.5\n)\n\nggplot() +\n  geom_sf(aes(fill = kde), data = bike_theft_density_bw_half, colour = NA) +\n  scale_fill_distiller(palette = \"Purples\", direction = 1)",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Your second crime map</span>"
    ]
  },
  {
    "objectID": "05_your_second_crime_map/index.html#clipping-map-layers",
    "href": "05_your_second_crime_map/index.html#clipping-map-layers",
    "title": "\n5  Your second crime map\n",
    "section": "\n5.6 Clipping map layers",
    "text": "5.6 Clipping map layers\nThere is one limitation of the KDE layer on our map that we need to deal with. The area covered by the KDE layer is determined by the area covered by the point data that we provided to hotspot_kde(). More specifically, hotspot_kde() will calculate density values for every cell in the convex hull around the point data, i.e. for the smallest polygon that contains all the points in the data.\nThis can be a problem in some circumstances, because we do not necessarily have crime data for all the areas within the convex hull of the data, even though KDE values will be calculated for those areas. This could be misleading, since it will look like such areas have low crime density, when in fact we do not know what the density of crime in such areas is.\nFortunately, we can easily deal with this problem by clipping the KDE layer to the boundary of the area for which we have crime data. This means we will only show densities for cells for which we actually have data.\n\n\nA. We only have data on bike thefts from the City of Vancouver, so all the bike thefts in the data necessarily occurred within the city. We do not know what the density of crime outside the city is.\n\n\n\nB. The KDE function only knows the theft locations, not the area in which thefts could have occurred. So the convex hull created by the KDE layer will not necessarily match the area of the data.\n\n\n\n\n\nC. In this case, that means some areas (shaded) will be included in the KDE layer even though they happened outside the area covered by the data, which could be misleading.\n\n\n\nD. To avoid suggesting we know the density of crimes in areas for which we do not have data, we should clip the KDE layer to the boundary of the area for which we have data.\n\n\n\nWe can clip the KDE layer produced by hotspot_kde() to the boundary of the City of Vancouver using the st_intersection() function from the sf package. st_intersection() removes any rows from the dataset provided as the first argument that do not fall within the area covered by the dataset provided as the second argument. If we have the boundary of the City of Vancouver stored in an object called vancouver_boundary and the a KDE layer showing the density of bike thefts stored in the bike_theft_density object, we can use st_intersection() to remove any cells in bike_theft_density that are outside vancouver_boundary.\n\n# Before clipping the KDE layer, we can use the `nrow()` function to check how\n# many cells there are in the KDE grid (each grid cell is one row in the data)\nnrow(bike_theft_density)\n\n[1] 3318\n\n# Clip the density layer\nbike_theft_density_clip &lt;- st_intersection(\n  bike_theft_density, \n  vancouver_boundary\n)\n\nWarning: attribute variables are assumed to be spatially constant throughout\nall geometries\n\n# Now we can check the number of rows in the clipped layer, which will be\n# lower than in the original KDE layer\nnrow(bike_theft_density_clip)\n\n[1] 2927\n\n\nst_intersection() produces a warning message:\nWarning: attribute variables are assumed to be spatially constant throughout all\ngeometries\nAs long as you are simply using st_intersection() to remove parts of the data outside a boundary, you can ignore this message.\nIn most cases, we can use the pipe operator to clip a KDE layer just after we produce it with hotspot_kde():\nbike_theft_density_clip &lt;- bike_thefts |&gt; \n  hotspot_kde(bandwidth_adjust = 0.5, quiet = TRUE) |&gt; \n  st_intersection(vancouver_boundary)\n\nThe st_intersection() function requires that both spatial layers have the same co-ordinate system. If the two layers use different co-ordinate systems, you will need to transform one of the layers using st_transform() so that it uses the same co-ordinate system as the other layer.\nIf you do not know which co-ordinate systems the layers use, you can use the st_crs() function to extract the co-ordinate system from one layer and pass that value as the second argument to st_transform(). For example:\nst_transform(bike_theft_density, crs = st_crs(vancouver_boundary))",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Your second crime map</span>"
    ]
  },
  {
    "objectID": "05_your_second_crime_map/index.html#adding-a-base-map",
    "href": "05_your_second_crime_map/index.html#adding-a-base-map",
    "title": "\n5  Your second crime map\n",
    "section": "\n5.7 Adding a base map",
    "text": "5.7 Adding a base map\nThe density map we have made is much more effective than a point map at allowing us to identify where the highest number of bike thefts in Vancouver occur. However, it’s still quite difficult to know where those places are, because we cannot easily work out where in the city these places are. We can make this much easier by adding a base map underneath the density layer.\nWe can add a base map using annotation_map_tile() function from the ggspatial package. We can add annotation_map_tile() to a ggplot() stack in the same way that we would add geom_*() functions.\n\nggplot() +\n  # We add the base map to the `ggplot()` stack *before* the density layer \n  # because we want the base map to appear below the density layer\n  annotation_map_tile(zoomin = 0, progress = \"none\") +\n  # When adding a base map, it is useful to make any filled layers (such as the\n  # density layer) semi-transparent so that readers can see the base map\n  geom_sf(\n    aes(fill = kde), \n    data = bike_theft_density_bw_half, \n    alpha = 0.67, \n    colour = NA\n  ) +\n  scale_fill_distiller(palette = \"Purples\", direction = 1) +\n  theme_void() +\n  # Suppress the map legend -- we will learn more about these next lines of code\n  # in a future tutorial\n  theme_void() + \n  theme(legend.position = \"none\")\n\n\n\n\nThe base maps returned by annotation_map_tile() are available at many different zoom levels, from level 1 that is useful for mapping the whole world in one map, to level 20 that can be used to map a single building. By default, annotation_map_tile() downloads tiles with slightly less detail than we might want, but we can fix this by using the argument zoomin = 0. We could also set a specific zoom level using the zoom argument.\nFor example, these maps show the same area around the UCL Jill Dando Institute with base maps at different zoom levels.\n\n\n\n\n\nChoosing the right zoom level is a matter of balancing the level of detail in the map and the clarity of the image. In the maps above, zoom levels less than 12 tend to have pixelated images because they do not contain enough detail, while zoom levels greater than 12 contain too much detail and so the information is hard to read. But if this map covered a smaller or larger area, a different zoom level might be better. In general, setting zoomin = 0 and not setting any value for the zoom argument – so that annotation_map_tile() chooses the zoom level automatically – will produce an acceptable map.\nannotation_map_tile() also gives us access to several different types of base map. The default style (seen in the maps above) is called ‘osm’ because it is the default style used by Open Street Map, the organisation that provides the map data. We can specify which style of base map we want using the type argument to annotation_map_tile().\n\n\n\n\n\nYou may want to experiment with different base map styles by using the type argument to annotation_map_tile() in the map above, e.g. using annotation_map_tile(type = \"cartolight\", zoomin = 0, progress = \"none\").\nOne final note about the annotation_map_tile() function: you might have noticed that when we have used it above we have always set the argument progress = \"none\". This stops the function from printing a progress bar while it is downloading the map tiles. The progress bar can sometimes be useful, but when you include the output from an R function in a report (as you will learn to do in a future tutorial), the output from the progress bar is likely to interfere with the formatting of the report. To prevent that, we use the progress = \"none\" argument.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Your second crime map</span>"
    ]
  },
  {
    "objectID": "05_your_second_crime_map/index.html#adding-more-layers",
    "href": "05_your_second_crime_map/index.html#adding-more-layers",
    "title": "\n5  Your second crime map\n",
    "section": "\n5.8 Adding more layers",
    "text": "5.8 Adding more layers\n\nAdding a base map underneath our density layer makes it much easier to understand where the highest densities of bike theft in Vancouver are. But our map could make it easier still to see where clusters of thefts occur. We could, for example, add the names of different neighbourhoods in the city, and show the city limits so that we can tell which areas have no crime because crimes in those areas are not included in our data.\nIn an earlier section I suggested we can think of ggplot charts, including maps, as being like stacks of pancakes – each function we use to amend the appearance of our chart is added to the top of the stack. So to add another layer to our map, we just add another geom_ function to our plot.\nThe City of Vancouver provides boundary data for city neighbourhoods on its website in GeoJSON format. This is a spatial data format, so it can be read by st_read(). We can then add the layer to our map using geom_sf() in the same way as for the point layer in our first map.\n\n# This is a version of the data saved from the City of Vancouver website, so \n# that this tutorial continues to work if the original data is ever removed\nnbhds &lt;- st_read(\"https://mpjashby.github.io/crimemappingdata/vancouver_neighbourhoods.geojson\")\n\nReading layer `vancouver_neighbourhoods' from data source \n  `https://mpjashby.github.io/crimemappingdata/vancouver_neighbourhoods.geojson' \n  using driver `GeoJSON'\nSimple feature collection with 22 features and 3 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: -123.2248 ymin: 49.19894 xmax: -123.0232 ymax: 49.29581\nGeodetic CRS:  WGS 84\n\n\n\nAt the same time, we can also add labels to the plot at the centre of each neighbourhood using the geom_sf_label() function. We use the aes() function to specify which column in the nbhds dataset we want to use for the label text. Normally, we would use the code aes(label = name) to do this, but in this case we want to wrap the labels so that they don’t overlap adjacent neighbourhoods, To do this we can use the str_wrap() from the stringr package (part of the tidyverse), so that our code instead becomes:\naes(label = str_wrap(name, width = 10))\nThe geom_sf_label() function in the map below uses quite a lot of arguments to control the appearance of the labels:\n\n\nalpha = 0.5 to make the label background semi-transparent so that we can see the density layer underneath it,\n\ncolour = \"seagreen3\" to slightly reduce the prominence of the label text to avoid distracting attention from the density layer,\n\nlineheight = 1 to reduce the gap between lines in each label,\n\nsize = 2.5 to slightly reduce the size of the label text,\n\nlabel.size = NA to remove the default border around the label background.\n\nPutting all this together, we get our final map:\n\n# Load packages\nlibrary(ggspatial)\nlibrary(sf)\nlibrary(sfhotspot)\nlibrary(tidyverse)\n\n# Load data\nbike_thefts &lt;- read_csv(\"https://mpjashby.github.io/crimemappingdata/vancouver_thefts.csv.gz\") |&gt; \n  janitor::clean_names() |&gt; \n  st_as_sf(coords = c(\"x\", \"y\"), crs = \"EPSG:32610\") |&gt; \n  filter(type == \"Theft of Bicycle\")\nnbhds &lt;- st_read(\"https://mpjashby.github.io/crimemappingdata/vancouver_neighbourhoods.geojson\")\n\nReading layer `vancouver_neighbourhoods' from data source \n  `https://mpjashby.github.io/crimemappingdata/vancouver_neighbourhoods.geojson' \n  using driver `GeoJSON'\nSimple feature collection with 22 features and 3 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: -123.2248 ymin: 49.19894 xmax: -123.0232 ymax: 49.29581\nGeodetic CRS:  WGS 84\n\n# Create KDE layer\nbike_theft_density_clip &lt;- bike_thefts |&gt; \n  # Calculate density\n  hotspot_kde(bandwidth_adjust = 0.5, quiet = TRUE) |&gt; \n  # Transform the density data to use the same CRS as the neighbourhoods layer\n  st_transform(\"EPSG:4326\") |&gt; \n  # Clip the density layer to the area for which we have data\n  st_intersection(nbhds)\n\n# Plot map\nggplot() +\n  # Add base map\n  annotation_map_tile(type = \"cartolight\", zoomin = 0, progress = \"none\") +\n  # Add density layer\n  geom_sf(\n    aes(fill = kde), \n    data = bike_theft_density_clip, \n    alpha = 0.75, \n    colour = NA\n  ) +\n  # Add neighbourhood boundaries (note `fill = NA` stops the neighbourhood\n  # shapes being filled with a colour, which would obscure the density layer\n  # underneath)\n  geom_sf(data = nbhds, colour = \"seagreen3\", fill = NA) +\n  # Add neighbourhood names\n  geom_sf_label(\n    aes(label = str_wrap(name, 10)), \n    data = nbhds, \n    alpha = 0.5,\n    colour = \"seagreen\", \n    lineheight = 1, \n    size = 2.5,\n    label.size = NA\n  ) +\n  # Set the colour scale\n  scale_fill_distiller(direction = 1) +\n  # Remove the axes, legend and other elements from the map that we don't need\n  theme_void()\n\n\n\n\nNow we can see that bike theft in Vancouver is heavily concentrated in a handful of neighbourhoods, particularly the Downtown and West End neighbourhoods. This map is much more useful than the first map that we produced in this tutorial showing only the point location of each crime, since in this latest map we can see not only the greatest concentrations of bike thefts but how they relate to the different areas of the city.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Your second crime map</span>"
    ]
  },
  {
    "objectID": "05_your_second_crime_map/index.html#in-summary",
    "href": "05_your_second_crime_map/index.html#in-summary",
    "title": "\n5  Your second crime map\n",
    "section": "\n5.9 In summary",
    "text": "5.9 In summary\n\nIn this tutorial we have learned to produce a density map of crime. This type of map can be very useful in identifying where practitioners should focus efforts to respond to crime. For example, a map like this might help local police to decide where to send officers to carry out extra patrols, while a crime-prevention charity might decide to run events in particular areas to educate people on how best to protect their bikes.\n\nIn the next tutorial, we will learn how to improve this map further.\n\n5.9.0.1 Tips for producing effective density maps\n\nDensity layers on maps (e.g. a layer added using geom_sf() to display the result produced by hotspot_kde()) should be made semi-transparent so that readers can see the base map underneath. If a density layer is not semi-transparent then it is likely to be very difficult for readers to see exactly where areas of high density are in the real world. Try setting the argument alpha = 0.7 in the call to the geom_sf() function, then change that value until you are happy with the visibility of both the density layer and the base map underneath.\nDensity layers should almost always only show a single type of crime – avoid calculating a single KDE layer based on data that includes more than one type of crime. There are two problems with combining data for multiple crime types to produce a single density layer. First, different crimes often concentrate in different places, so a combined map might end up showing concentrations inaccurately. Second, since the KDE process is based on the number of points, a density layer produced by combining data for multiple crimes will inevitably be more influenced by whichever crime type is more numerous. Since more minor crimes tend to be more common, this could mean that your density map points people towards areas with lots of minor crimes and away from places where more-serious crimes happen.\nAvoid mapping ‘intangible’ crimes. These are crimes that are only ever recorded by police when officers happen to identify a crime while on patrol, rather than the crime usually being reported by the victim or a third-party. You should avoid mapping these types of crime because they generally reflect geographic concentrations of police patrol more than geographic concentrations of the crimes themselves. The most common intangible offences are drugs possession and weapons possession, which are detected incidentally by officers on patrol much more than they are reported to the police by the public.\n\nYou can find out more about some of the things we have covered in this tutorial using these resources:\n\nUnderstand more about the history of trying to develop accurate map projections in this short video: Why all world maps are wrong.\nFind out more about making all sorts of charts (not just maps) with the ggplot2 package in the Data Visualisation chapter of R for Data Science by Hadley Wickham and Garrett Grolemund.\nLearn more about making maps using simple features in Chapter 1 of Spatial Data Science by Edzer Pebesma and Roger Bivand.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Your second crime map</span>"
    ]
  },
  {
    "objectID": "06_map_context/index.html#introduction",
    "href": "06_map_context/index.html#introduction",
    "title": "\n6  Giving a map context\n",
    "section": "\n6.1 Introduction",
    "text": "6.1 Introduction\nIn this tutorial we will create this map of shootings in the Bronx borough of New York City in 2019. You’ll see that unlike the maps we have made so far, this map includes contextual elements such as a title, a legend and a scale bar.\n\nshootings_map_final\n\n\n\n\nIn this tutorial we will learn to add important context to our maps using these elements.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Giving a map context</span>"
    ]
  },
  {
    "objectID": "06_map_context/index.html#map-choices",
    "href": "06_map_context/index.html#map-choices",
    "title": "\n6  Giving a map context\n",
    "section": "\n6.2 Map choices",
    "text": "6.2 Map choices\nAmong the most important decisions you make when you are creating a map is what information to include and what to leave out. Watch this video to learn more about why this is important and how you can make those decisions.\n\nThe most important thing to remember when designing a map is to keep in mind the purpose that the map will be used for. Research on how people use maps has repeatedly shown that “the nature of the task or function to be performed by the map user is the single most important factor in determining how [someone] processes the information on the map”.\nAs explained in the video, when you create a crime map you should ask yourself:\n\n\nWhat will readers be using this map for? .\nWhat information do readers need?\nWhat do readers already know?\nIn what context are people going to use this map?\nHow could this map lead the audience to a wrong conclusion?\n\nMaps are powerful communication tools, which can sometimes knowingly or unknowingly mislead the reader. Watch this video to learn more about how maps can be misleading.\n\nWhenever you make a map, think about your own biases – are your own views on a topic likely to influence the results of your analysis? One way to test your own assumptions about a topic is to test them against other potential assumptions using an approach to crime analysis called hypothesis testing. To find out more about the hypothesis testing approach, read the paper Improving the explanatory content of analysis products using hypothesis testing.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Giving a map context</span>"
    ]
  },
  {
    "objectID": "06_map_context/index.html#visual-hierarchy",
    "href": "06_map_context/index.html#visual-hierarchy",
    "title": "\n6  Giving a map context\n",
    "section": "\n6.3 Visual hierarchy",
    "text": "6.3 Visual hierarchy\nMaps are among the most complex types of data visualisation. Even if we have chosen wisely what to include and what to leave out, there is likely to be lots of information on our map. For all but the simplest maps, there is a risk of readers – especially those in a hurry – might be overwhelmed or mislead by competing pieces of information (such as different layers of data) on a map.\nTo help readers understand what parts of a map they should focus most of their attention on and which are of less importance, we can establish a visual hierarchy. Watch this video to learn more about visual hierarchies in mapping.\n\nWe have used some of the principles of visual hierarchy in the maps we have already made. For example, in the density map of bike thefts in Vancouver, we used strong colours to represent the data and shades of grey for the base map. This helped readers intuitively understand that they should focus most attention on the data.\n\n\nquiz(\n  caption = \"\",\n  \n  question(\n    \"Which one of these is not a visual element that we can use to establish the visual hierarchy of different elements on our map?\",\n    answer(\"Brand\", correct = TRUE),\n    answer(\"Colour\"),\n    answer(\"Size\"),\n    answer(\"Isolation\"),\n    correct = random_praise(),\n    allow_retry = TRUE,\n    random_answer_order = TRUE\n  ),\n  \n  question(\n    \"Which types of colour are more likely to move an element up the visual hierarchy?\",\n    answer(\"Colours that are more saturated\", correct = TRUE),\n    answer(\"Colours that are less saturated\"),\n    correct = random_praise(),\n    allow_retry = TRUE,\n    random_answer_order = TRUE\n  )\n  \n)",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Giving a map context</span>"
    ]
  },
  {
    "objectID": "06_map_context/index.html#supporting-elements",
    "href": "06_map_context/index.html#supporting-elements",
    "title": "\n6  Giving a map context\n",
    "section": "\n6.4 Supporting elements",
    "text": "6.4 Supporting elements\nWe can often make our maps much more useful by adding supporting elements that explain the map content, give context or provide extra information. Watch this video to find out what elements you can add to your maps to make them more useful.\n\nWe will not need to include every supporting element mentioned in the video in all the maps we make. The visual hierarchy that you establish in your map by use of size, colour, etc. should make it clear which elements are most important. The data should always come first in the visual hierarchy, usually followed by the title and then the legend. Other elements should be lower down the hierarchy. In every map, the supporting elements should be designed so that they do not distract from the data.\n\n\n\nVisual hierarchy of elements in a crime map\n\nplace in hierarchy\nmap element\nhow often needed\n\n\n\n1st\ndata layers\nalways\n\n\n2nd\ntitle\nvirtually always\n\n\n3rd\nlegend\nusually\n\n\n4th\nbase map\nalmost always\n\n\n5th\nauthor and date\nvirtually always\n\n\n=6th\nscale\nsometimes\n\n\n=6th\nnorth arrow\nsometimes\n\n\n7th\ngrid\nrarely\n\n\n\n\n\nElements that are almost always needed on a crime map are not necessarily highest on the visual hierarchy. For example, the author name is virtually always needed but is relatively low on the visual hierarchy. This is because it is important information for readers who need it to judge the reliability of a map, or to get in touch to ask questions, but should not distract from the data for those readers who do not need it.\n\n\nquiz(\n  caption = \"\",\n  \n  question(\n    \"Which of these elements should be lowest down the visual hierarchy on most maps?\",\n    answer(\"North arrow\", correct = TRUE),\n    answer(\"Author and date\"),\n    answer(\"Title\"),\n    answer(\"Data layers\"),\n    correct = random_praise(),\n    allow_retry = TRUE,\n    random_answer_order = TRUE\n  ),\n  \n  question(\n    \"How likely is it that our maps will need to include a base map?\",\n    answer(\"Almost always\", correct = TRUE),\n    answer(\"Usually\"),\n    answer(\"Occasionally\"),\n    answer(\"Never\"),\n    correct = random_praise(),\n    allow_retry = TRUE,\n    random_answer_order = TRUE\n  )\n  \n)\n\n\nBefore we start learning the code needed to add titles and legends to our maps, watch this video walk-though of the main steps.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Giving a map context</span>"
    ]
  },
  {
    "objectID": "06_map_context/index.html#creating-and-storing-a-map",
    "href": "06_map_context/index.html#creating-and-storing-a-map",
    "title": "\n6  Giving a map context\n",
    "section": "\n6.5 Creating and storing a map",
    "text": "6.5 Creating and storing a map\nSince we will be adding various elements to a map in this tutorial, we will first create a map and save it as an R object. Any map or chart produced using the ggplot() function can be saved as an object using the assignment operator &lt;-. Just as for the result of any other R function, if we save it to an object the result will not be printed to the screen, but we can easily see the plot by simply typing the object name in the R console.\n\n\n\n\n\n\n# Load packages\nlibrary(ggspatial)\nlibrary(sf)\nlibrary(sfhotspot)\nlibrary(tidyverse)\n\n# Load shootings data\nshootings &lt;- read_csv(\n  \"https://mpjashby.github.io/crimemappingdata/bronx_shootings.csv\", \n  show_col_types = FALSE\n) |&gt; \n  st_as_sf(coords = c(\"longitude\", \"latitude\"), crs = \"EPSG:4326\") |&gt; \n  st_transform(\"EPSG:6538\")\n\n# Load NYC police precincts data\nprecincts &lt;- read_sf(\"https://mpjashby.github.io/crimemappingdata/nyc_precincts.gpkg\") |&gt; \n  janitor::clean_names() |&gt; \n  # Filter just those precincts that are in the Bronx (40th to 52nd)\n  filter(precinct %in% 40:52) |&gt; \n  st_transform(\"EPSG:6538\")\n\n# Calculate KDE\nshootings_kde &lt;- shootings |&gt; \n  hotspot_kde(\n    grid = hotspot_grid(precincts, cell_size = 100), \n    bandwidth_adjust = 0.33,\n    quiet = TRUE\n  ) |&gt; \n  st_intersection(precincts)\n\nWarning: attribute variables are assumed to be spatially constant throughout\nall geometries\n\n# Create map object\nshootings_map &lt;- ggplot() +\n  annotation_map_tile(type = \"cartolight\", zoomin = 0, progress = \"none\") +\n  geom_sf(aes(fill = kde), data = shootings_kde, alpha = 0.75, colour = NA) +\n  geom_sf(data = precincts, colour = \"grey33\", fill = NA) +\n  geom_sf_label(\n    aes(label = scales::ordinal(precinct)), \n    data = precincts,\n    alpha = 0.5, \n    colour = \"grey33\", \n    size = 2.5, \n    label.size = NA\n  ) +\n  scale_fill_distiller(palette = \"PuBu\", direction = 1) +\n  theme_void() +\n  theme(legend.position = \"none\")\n\n# Display map\nshootings_map\n\n\n\n\nThe code used to create this map is very similar to the code we used in one of the previous tutorials to make a map of bike theft in Vancouver, although there are a few differences.\nLooking at the code above, we have used the grid argument to the hotspot_kde() function, specifying hotspot_grid(precincts, cell_size = 100). By default, hotspot_kde() calculates density estimates (KDE values) for every grid cell within the area covered by the crime data. This is fine when at least a few crimes have occurred in all the parts of the area for which we have data. But if crime is heavily concentrated in a few places and there are large areas with no crimes, the density layer will not cover the whole area for which we have data. In this case, it is important to extend the density layer manually to make it clear that the apparent low density of crime in some places is due to a genuine lack of crime there, rather than because we do not have data for those places.\nWe can do this by creating a grid of cells for which KDE values should be calculated, rather than letting hotspot_kde() do this automatically. In the code above, we have used the hotspot_grid() function to create a grid that covers all the precincts in the Bronx (the area we have data for), and then passed the resulting grid to the grid argument of hotspot_kde(). This ensures that KDE values are calculated for every part of the Bronx.\n\nThe only other difference from the Vancouver map is that we have used the ordinal() function from the scales package to convert the precinct numbers to ordinal numbers (1st, 2nd, 3rd, etc.) for the map labels. This is because police precincts in New York City are usually referred to using ordinal numbers (e.g. “the 1st Precinct” rather than “Precinct 1”) and it will be easier for people to read the map if it uses terms they are familiar with.\nThere are many other functions in the scales package that format numbers in different ways, including comma() to add thousands separators to numbers and dollar() to format numbers as values in dollars or other currencies. There is a full list of scales functions on the package website.\nWe now have a basic map of shootings in the Bronx. This map isn’t good enough on its own, but we can use it to learn how to add supporting elements to a map.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Giving a map context</span>"
    ]
  },
  {
    "objectID": "06_map_context/index.html#titles",
    "href": "06_map_context/index.html#titles",
    "title": "\n6  Giving a map context\n",
    "section": "\n6.6 Titles",
    "text": "6.6 Titles\nA map title is one of the most important ways to add context to a map. Titles can either be descriptive or declarative. Descriptive titles simply state what data are shown on the map. For example, we might give our map the title “Shootings in the Bronx, 2019”. Declarative titles, on the other hand, state what you think the main conclusion should be that readers remember about the map. For example, we might use the title “Shootings are focused in the South Bronx”.\nDeclarative titles are usually more useful than descriptive titles because they help the reader to interpret the map. But writing a good declarative title is harder than writing a descriptive title, because it requires you to think about what is the main point that you want to make with the map. To help you come up with a good declarative title, you might want to try several different titles so that you can choose the one that communicates your message most clearly.\nWe can add a title to our map using the labs() (short for ‘labels’) function from the ggplot2 package. We can use labs() to add labels to various different parts of a map or plot, but for now we will just use the argument title to set the title.\n\nshootings_map + \n  labs(title = \"Shootings are focused in the South Bronx\")\n\n\n\n\nSometimes our preferred title might be too long to fit on a map. In this case, we can break the title across two or more lines. We can do this manually by adding the characters \\n (the character code for a new line) at the point where we want the text to start a new line. Alternatively, we can use the str_wrap() function from the stringr package to wrap the text automatically into lines of a given maximum length (specified using the wrap argument).\nWhen you use a declarative title for your map, it is often useful to provide a subtitle containing descriptive information. Adding a subtitle is very easy using the subtitle argument to the labs() function. Use the code above as a template to add a subtitle to your map explaining that the map shows fatal and non-fatal shootings in 2019.\n\n# Add a `subtitle` argument to the labs() function\nshootings_map + \n  labs(title = \"Shootings are focused in the South Bronx\")\n\n\n\n\n\nshootings_map + \n  labs(\n    title = \"Shootings are focused in the South Bronx\",\n    subtitle = \"Fatal and non-fatal shootings recorded by NYC Police, 2019\"\n  )\n\n\n\n\n\n6.6.1 Using captions to add author and other information\nThe labs() function has another argument that we can use to add text to our map for context. The caption argument is used to add information such as the author, date and source of the data to a map or chart. We can put any information we like into the caption, using str_wrap() or the new-line character \\n if necessary to stop the text overflowing the map.\n\nshootings_map + \n  labs(\n    title = \"Shootings are focused in the South Bronx\",\n    subtitle = \"Fatal and non-fatal shootings recorded by NYC Police, 2019\",\n    caption = str_glue(\n      \"Author: Joe Bloggs, Date produced: {lubridate::today()},\\n\",\n      \"Data: https://data.cityofnewyork.us/d/833y-fsy8\\n\",\n      \"Map data from OpenStreetMap\"\n    )\n  )\n\n\n\n\nThis code uses the str_glue() function from the stringr package to incorporate some automatically updated information (in this case, the current date) into the caption. str_glue() glues together any number of character strings separated by commas – in this case, the code includes two strings so that the lines of code do not become too long to easily read.\nstr_glue() can also include the values of R objects and the results of R functions that are placed inside braces {}. So the code {lubridate::today()} runs the function today() from the lubridate package and glues the result (the current date) into the text. If we had already loaded the lubridate library we could have just used the code {today()}, but since we do not need any other functions from lubridate for this analysis, we instead do not load the package but call the function by specifying which package it comes from using the :: operator as in the previous tutorial.\n\nWhen you use data to make maps, the data provider will often require you to acknowledge the source of that data. This is a legal requirement so it is important that you do this when required.\nThe base maps we use in this course use data from OpenStreetMap, which requires people using its data to acknowledge that they have done so. The easiest way to do this is to add the text “Map data from OpenStreetMap” to the caption of your maps if they use base maps from OpenStreetMap.\n\n\n6.6.2 Changing the appearance of titles and captions\nWe have added a title, subtitle and caption to our map, but you might not be happy with their appearance. You might want, for example, to move the caption down the visual hierarchy by making the text smaller and/or a lighter colour, or add some space between the subtitle and the map itself.\nWe can exercise almost complete control over the supporting elements of maps or charts made with ggplot() using the theme() function. One important thing to remember about theme() is that it only controls the non-data elements of a map – nothing you do with the theme() function will have any effect on the data elements of a map (in this case, the layer showing the density of shootings). To change the appearance of data layers within ggplot() maps, use the geom_, and scale_ families of functions as we have learned in previous tutorials.\nThe theme() function has a lot of potential arguments. If you need help using the theme() function (or any function in R) you can view a manual page (including a list of arguments) for the function by:\n\ntyping a question mark followed by the function name without parentheses (e.g. ?theme) into the R console,\ntyping the function name without parentheses into the search box in the Help panel in RStudio, or\nclicking on the function name anywhere in your R code to place the cursor on the function name, then pressing F1 on your keyboard.\n\nTry opening the manual page for theme() now to see the list of possible arguments it can take. Fortunately, we will not need most of these arguments most of the time – ggplot() has default values built in for every value that can be changed using theme(), and these defaults will be reasonable in almost all cases.\nTo reduce the visual prominence of the map caption, we can change the value of the plot.caption argument to theme(). Since the caption is a text element (rather than a polygon, line, etc.), we can use the helper function element_text() to do this. The following code changes the colour of the caption text to a lighter grey and makes the text smaller relative to the default using the helper function rel() (for relative sizing) – 0.7 means the text will be 70% as big as it would have been by default.\n\nshootings_map +\n  labs(\n    title = \"Shootings are focused in the South Bronx\",\n    subtitle = \"Fatal and non-fatal shootings recorded by NYC Police, 2019\",\n    caption = str_glue(\n      \"Author: Joe Bloggs, Date produced: {lubridate::today()},\\n\",\n      \"Data: https://data.cityofnewyork.us/d/833y-fsy8\\n\",\n      \"Map data from OpenStreetMap\"\n    )\n  ) +\n  theme(\n    plot.caption = element_text(colour = \"grey67\", size = rel(0.7))\n  )\n\n\n\n\nThe helper function element_text() has arguments to control the appearance of text in different ways. As well as colour (or color, either is fine) and size, there are:\n\n\nfamily controls the font used, e.g. Times New Roman or Helvetica,\n\nface controls the style of the font, i.e. ‘plain’, ‘italic’, ‘bold’ or ‘bold.italic’,\n\nhjust controls the horizontal justification of the text, where 0 means left aligned, 0.5 means centred and 1 means right aligned,\n\nvjust controls the vertical justification, and\n\nangle controls the angle (in degrees) of the text (0 means horizontal),\n\nlineheight controls the space between lines if you have created a value that has more than one line (e.g. using \\n or str_wrap()).\n\nThe margin argument controls the space around the text. It is easiest to specify the value of margin using the helper function margin() designed for that purpose. You specify the top, right, bottom and left margin separately in that order – to remember the order, think ‘trouble’.\n\nThe following code changes the margin around the map subtitle. Change this code so that it also makes the subtitle 80% of the default size and changes the caption so that it is left aligned.\n\nshootings_map +\n  labs(\n    title = \"Shootings are focused in the South Bronx\",\n    subtitle = \"Fatal and non-fatal shootings recorded by NYC Police, 2019\",\n    caption = str_glue(\n      \"Author: Joe Bloggs, Date produced: {lubridate::today()},\\n\",\n      \"Data: https://data.cityofnewyork.us/d/833y-fsy8\\n\",\n      \"Map data from OpenStreetMap\"\n    )\n  ) +\n  theme(\n    plot.subtitle = element_text(margin = margin(3, 0, 6, 0, unit = \"pt\")),\n    plot.caption = element_text(colour = \"grey50\", size = rel(0.7))\n  )\n\n\n\n\n\n# Change the line `plot.subtitle = element_text(margin = margin(3, 0, 6, 0)),`\n# so that it also specifies the text size\n\n\n# You can change the size relative to the default using the `rel()` function\n\n\n# Change the line `plot.caption = element_text(colour = \"grey50\", size = rel(0.7))`\n# so that it also specifies the horizontal alignment of the text\n\n\n# You can change the horizontal justification of the caption using the `hjust`\n# argument to the `element_text()` function\n\n\n\nshootings_map +\n  labs(\n    title = \"Shootings are focused in the South Bronx\",\n    subtitle = \"Fatal and non-fatal shootings recorded by NYC Police, 2019\",\n    caption = str_glue(\n      \"Author: Joe Bloggs, Date produced: {lubridate::today()},\\n\",\n      \"Data: https://data.cityofnewyork.us/d/833y-fsy8\\n\",\n      \"Map data from OpenStreetMap\"\n    )\n  ) +\n  theme(\n    # Make the plot subtitle smaller and adjust the margin around it\n    plot.subtitle = element_text(size = rel(0.8), margin = margin(3, 0, 6, 0)),\n    # Make the legend caption smaller, left-aligned and a lighter shade of grey\n    plot.caption = element_text(colour = \"grey67\", size = rel(0.7), hjust = 0)\n  )\n\n\n\n\nNow that we have finished setting the text elements for our map, we can save it as a new object that we can use as the basis for the other objects we want to add.\n\nshootings_map_titled &lt;- shootings_map +\n  labs(\n    title = \"Shootings are focused in the South Bronx\",\n    subtitle = \"Fatal and non-fatal shootings recorded by NYC Police, 2019\",\n    caption = str_glue(\n      \"Author: Joe Bloggs, Date produced: {lubridate::today()},\\n\",\n      \"Data: https://data.cityofnewyork.us/d/833y-fsy8\\n\",\n      \"Map data from OpenStreetMap\"\n    )\n  ) +\n  theme(\n    # Make the plot subtitle smaller and adjust the margin around it\n    plot.subtitle = element_text(size = rel(0.8), margin = margin(3, 0, 6, 0)),\n    # Make the legend caption smaller, left-aligned and a lighter shade of grey\n    plot.caption = element_text(colour = \"grey67\", size = rel(0.7), hjust = 0)\n  )\n\n\nIn this tutorial we will store the map we are creating in an object several times as we go through the process of explaining how to add context to a map. When you write your own code, you should not do this. Instead, you should create the map from start to finish in a single block of code. Doing that will make sure your code is easy to read and that you don’t have to keep track of more objects than necessary. For an example of this, see the final page of this tutorial.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Giving a map context</span>"
    ]
  },
  {
    "objectID": "06_map_context/index.html#legends",
    "href": "06_map_context/index.html#legends",
    "title": "\n6  Giving a map context\n",
    "section": "\n6.7 Legends",
    "text": "6.7 Legends\nLegends are important for all but the simplest crime maps because they help readers to interpret the points, lines and polygons used to represent data on a particular map. Except for point maps containing only a small number of crimes (such as the map of homicide in downtown Atlanta that we produced in the first tutorial), crime maps will almost always need a legend to help users interpret them.\nProducing a legend manually could be quite complicated, but fortunately ggplot() produces legends automatically. The reason the maps we have produced up to now haven’t included legends is that we have been suppressing the legends using the argument legend.position = \"none\" to the theme() function – look back at the code on the page ‘Creating and storing a map’ in this tutorial to see this code on the final line of the stack of functions added to ggplot().\nggplot() will add a legend to a map or chart whenever one or more layers of data are represented using an aesthetic property such as size, shape, colour or fill. In our current map, the density of shootings is represented using the fill colour of the polygons produced by the hotspot_kde() function, with darker colours representing more shootings.\nTo reveal the legend automatically generated, we can add another call to the theme() function to our existing ggplot() object. This overrides the previous code that set the legend position to none, instead placing the legend on the right-hand side of the plot. We only need to do this because we previously suppressed the legend – if we had not suppressed it, the legend would have appeared automatically.\n\nshootings_map_titled +\n  theme(\n    # Position the legend on the right-hand side of the plot\n    legend.position = \"right\"\n  )\n\n\n\n\nOur map now has a legend, but we may want to adjust its appearance by:\n\nchanging the default legend title from “kde” to something more meaningful,\nmoving the legend down the visual hierarchy by making it smaller (at the moment it is almost as visually prominent as the data),\nremoving the potentially confusing raw density values.\n\nWe can change the default legend title by once again using the labs() function. Since we want to change the title of the legend, you might reasonably think that we would do this using something like labs(legend = \"density\") but unfortunately that code would do nothing at all. Instead, we have to set the legend title using the aesthetic (colour, size, shape, etc.) that the legend represents. This makes it possible to specify multiple titles if there are separate legends for different layers that use different aesthetics. For example if a map used lines of different colours to show streets of different types and filled polygons to show the density of crime, it would be possible to have separate legends explaining each aesthetic. In this case, we’ve specified that the kde column in the data should control the fill aesthetic, so we can set the title for that legend using fill = \"title we want\".\n\nshootings_map_titled +\n  labs(fill = \"kernel density\\nof shootings\") +\n  theme(\n    # Position the legend on the right-hand side of the plot\n    legend.position = \"right\"\n  )\n\n\n\n\nTo make the legend smaller, we can use theme() in the same way as we did to change the appearance of the caption. We use the legend.title argument to format the legend title and the legend.text argument to format the labels for each value in the legend.\nWe will also make the colour bar in the legend (called the key by ggplot()) slightly smaller using the legend.key.width argument. To do this we will use the helper function unit(), which allows us to specify the size using any of several common units. In this case, we will specify the key size in lines (1 line = the height of one line of text) so that it is relative to the text size we have chosen.\nAt the same time, we will reduce the size of the legend title and the legend text (the numbers next to the colour bar), using the legend.text and legend.title arguments together with the the helper function element_text(). In both cases, we will set the text size relative to the default text size using the rel() helper function.\n\nshootings_map_titled +\n  labs(fill = \"kernel density\\nof shootings\") +\n  theme(\n    # Make the legend colour bar smaller\n    legend.key.width = unit(0.8, \"lines\"),\n    # Position the legend on the right-hand side of the plot\n    legend.position = \"right\",\n    # Make the legend text smaller\n    legend.text = element_text(size = rel(0.7)),\n    legend.title = element_text(size = rel(0.8))\n  )\n\n\n\n\nFinally, we want to remove the raw density values, since these are difficult to interpret and might distract readers from the key message that darker colours on the map represent higher densities of shootings.\nBy default, ggplot() sets the label for each legend key based on the data. To specify our own labels, we can use the labels argument to the scale_fill_distiller() function that we previously used to set the colour scheme of the density layer on the map.\nWhen we set colour bar labels manually, we have to also specify where on the colour bar we want those labels to appear. We do this using the breaks argument to scale_fill_distiller(), making sure the number of values we supply to the breaks argument is the same as the number of labels we’ve given to the labels argument (otherwise R will produce an error).\nIn this case, we want to add two labels (“higher” and “lower”), one at either end of the colour bar. We could look at kde column of the shootings_kde object to find the minimum and maximum values, but that would introduce the risk of us accidentally entering the wrong values. Instead, we can use the pull() function to extract the kde column from the shootings_kde dataset and then use the range() function to find the minimum and maximum values. Putting this together, we get breaks = range(pull(shootings_kde, \"kde\")).\n\nshootings_map_titled +\n  scale_fill_distiller(\n    palette = \"PuBu\", \n    direction = 1, \n    # Specify label positions as the minimum and maximum KDE values\n    breaks = range(pull(shootings_kde, \"kde\")), \n    labels = c(\"lower\", \"higher\")\n  ) +\n  labs(fill = \"kernel density\\nof shootings\") +\n  theme(\n    # Make the legend colour bar smaller\n    legend.key.width = unit(0.8, \"lines\"),\n    # Position the legend on the right-hand side of the plot\n    legend.position = \"right\",\n    # Make the legend text smaller\n    legend.text = element_text(size = rel(0.7)),\n    legend.title = element_text(size = rel(0.8))\n  )\n\nScale for fill is already present.\nAdding another scale for fill, which will replace the existing scale.\n\n\n\n\n\nNow that we have finished formatting the legend, we can again store the map as an object that we can build on further.\n\nshootings_map_legend &lt;- shootings_map_titled +\n  scale_fill_distiller(\n    palette = \"PuBu\", \n    direction = 1, \n    # Specify label positions as the minimum and maximum KDE values\n    breaks = range(pull(shootings_kde, \"kde\")), \n    labels = c(\"lower\", \"higher\")\n  ) +\n  labs(fill = \"kernel density\\nof shootings\") +\n  theme(\n    # Make the legend colour bar smaller\n    legend.key.width = unit(0.8, \"lines\"),\n    # Position the legend on the right-hand side of the plot\n    legend.position = \"right\",\n    # Make the legend text smaller\n    legend.text = element_text(size = rel(0.7)),\n    legend.title = element_text(size = rel(0.8))\n  )\n\nScale for fill is already present.\nAdding another scale for fill, which will replace the existing scale.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Giving a map context</span>"
    ]
  },
  {
    "objectID": "06_map_context/index.html#scales-and-north-arrows",
    "href": "06_map_context/index.html#scales-and-north-arrows",
    "title": "\n6  Giving a map context\n",
    "section": "\n6.8 Scales and north arrows",
    "text": "6.8 Scales and north arrows\nThe final elements we can add to our map are a scale bar and a north arrow, which can both be added using functions from the ggspatial package.\n\n6.8.1 Scale bars\nTo add a scale bar, we can add a call to the annotation_scale() function to our existing ggplot() object.\n\nshootings_map_legend +\n  annotation_scale()\n\n\n\n\nThe default scale bar is a little too visually dominant for the low place it should have in the visual hierarchy of our map, and the default placement in the bottom-left corner happens to overlap with the highest density of shootings. We can change the scale bar using arguments to the annotation_scale() function:\n\n\nwidth_hint = 1/5 changes the (approximate) proportion of the map width across which the scale bar stretches,\n\nstyle = \"ticks\" changes the style of the scale bar to the less visually prominent line-and-tick-marks style, and\n\nlocation = \"br\" moves the scale bar to the bottom-right corner of the map.\n\n\nshootings_map_legend +\n  annotation_scale(width_hint = 1/5, style = \"ticks\", location = \"br\")\n\n\n\n\n\n6.8.2 North arrows\nWe can add a north arrow using the annotation_north_arrow() function. The default arrow is too obtrusive to fit its position in the visual hierarchy, so we will change its appearance using the arguments:\n\n\nlocation = \"tr\" to move the north arrow to the top-right corner, since we have put the scale bar in the bottom-right where the north arrow would be placed by default,\n\nheight = unit(1.5, \"lines\") to make the arrow smaller, and\n\nstyle = north_arrow_minimal(text_size = 8) to use a smaller style of arrow, at the same time reducing the font size (measured in points) of the N symbol.\n\n\nshootings_map_legend +\n  annotation_scale(width_hint = 1/5, style = \"ticks\", location = \"br\") +\n  annotation_north_arrow(\n    location = \"tr\", \n    height = unit(1.5, \"lines\"), \n    style = north_arrow_minimal(text_size = 8)\n  )",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Giving a map context</span>"
    ]
  },
  {
    "objectID": "06_map_context/index.html#saving-maps",
    "href": "06_map_context/index.html#saving-maps",
    "title": "\n6  Giving a map context\n",
    "section": "\n6.9 Saving maps",
    "text": "6.9 Saving maps\nOnce you have a complete map, it is often useful to save it as an image file so that you can share it with others or embed it into a report or presentation. You can save plots created with ggplot() using the ggsave() function.\nBefore we do that, lets create a final map that includes a base map as well as some supporting elements. We won’t include a north arrow because north is at the top of the map and it’s unlikely anyone will be using this map for navigation. We will include a scale bar so that people looking at the map can see the approximate size of shooting hotspots. Remember we have already loaded the necessary packages, as well as data stored in the shootings, precincts and shootings_kde objects.\n\nThere is usually no need to save the map into several different objects (e.g.  shootings_map_titled or shootings_map_legend) as we have done in this tutorial. We have only done that here so you could learn about the different contextual elements one by one.\nIt is usually much better to create a whole ggplot() stack in one go, since it is easier to keep track of all the elements that way. In the code below all the elements have been combined into a single ggplot() stack.\n\n\nshootings_map_final &lt;- ggplot() +\n  annotation_map_tile(type = \"cartolight\", zoomin = 0, progress = \"none\") +\n  geom_sf(aes(fill = kde), data = shootings_kde, alpha = 0.75, colour = NA) +\n  geom_sf(data = precincts, colour = \"grey33\", fill = NA) +\n  geom_sf_label(\n    aes(label = scales::ordinal(precinct)), \n    data = precincts,\n    alpha = 0.5, \n    colour = \"grey33\", \n    size = 2.5, \n    label.size = NA\n  ) +\n  annotation_scale(width_hint = 1/5, style = \"ticks\", location = \"br\") +\n  scale_fill_distiller(\n    palette = \"PuBu\",\n    breaks = range(pull(shootings_kde, \"kde\")), \n    labels = c(\"lower\", \"higher\"),\n    direction = 1\n  ) +\n  labs(\n    title = \"Shootings are focused in the South Bronx\",\n    subtitle = \"Fatal and non-fatal shootings recorded by NYC Police, 2019\",\n    caption = str_glue(\n      \"Author: Joe Bloggs, Date produced: {lubridate::today()},\\n\",\n      \"Data: https://data.cityofnewyork.us/d/833y-fsy8\\n\",\n      \"Map data from OpenStreetMap\"\n    ),\n    fill = \"kernel density\\nof shootings\"\n  ) +\n  theme_void() +\n  theme(\n    legend.key.width = unit(0.8, \"lines\"),\n    legend.text = element_text(size = rel(0.7)),\n    legend.title = element_text(size = rel(0.8)),\n    plot.subtitle = element_text(size = rel(0.8), margin = margin(3, 0, 6, 0)),\n    plot.caption = element_text(colour = \"grey67\", size = rel(0.7), hjust = 0)\n  )\n\n# Display map\nshootings_map_final\n\n\n\n\nggsave() can create image files in many different formats, including PNG, JPEG and PDF. ggsave() will determine which type of file to create according to the file extension of the file name that you specify. So ggsave(\"bronx_shootings_2019.pdf\", plot = shootings_map_legend) produces a PDF file, while ggsave(\"bronx_shootings_2019.jpg\", plot = shootings_map_legend) produces a JPEG image file.\nYou can specify the size of the image that will be saved using the height and width arguments. Note that for historical reasons these values are in inches by default, but you can change this to either centimetres (using units = \"cm\"), millimetres (using units = \"mm\") or pixels (using units = \"px\").\nTo share our map with others, lets save it as an A4-size PDF.\n\nggsave(\n  \"bronx_shootings_2019.pdf\", \n  plot = shootings_map_final, \n  width = 210,\n  height = 297,\n  units = \"mm\"\n)\n\nWe can now use share this file by email, upload it to a website or embed it in another document.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Giving a map context</span>"
    ]
  },
  {
    "objectID": "06_map_context/index.html#in-summary",
    "href": "06_map_context/index.html#in-summary",
    "title": "\n6  Giving a map context\n",
    "section": "\n6.10 In summary",
    "text": "6.10 In summary\n\nIn this tutorial we have learned about the importance of understanding the purpose for which people will use a map when making decisions about map design. We have also learned about how establishing a visual hierarchy on our map can help steer readers towards the most-important elements and how to add titles, legends and scale bars to maps in R.\n\n\nYou can find out more about the topics we have covered in this tutorial:\n\nFor a short summary of research into how people read maps and what that tells us about how to design a map, see Cartography, visual perception and cognitive psychology by Amy Griffin.\nFor a more-detailed explanation of how visual hierarchy can be applied to maps, see Visual Hierarchy and Layout.\nFor more examples of how maps can mislead, read How to lie with maps by Alan Smith.\n\n\n\n\n\n\nXKCD.com comic ‘Geography’ licensed under the Creative Commons Attribution-NonCommercial license.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Giving a map context</span>"
    ]
  },
  {
    "objectID": "07_handling_bugs/index.html#introduction",
    "href": "07_handling_bugs/index.html#introduction",
    "title": "\n7  Handling bugs in your code\n",
    "section": "\n7.1 Introduction",
    "text": "7.1 Introduction\nIn this tutorial we will learn how to deal with bugs in the R code that we write. It’s inevitable that you will make mistakes in the R code that you write. Everyone types the wrong command every now and again, just as everyone sometimes clicks the wrong button in other pieces of software. Learning how to identify and fix bugs is part of the process of learning to code.\nSome errors are easy to identify and easy to fix. Imagine you are working on this dataset of frauds in Kansas City:\n\n\n\n\n\n\n\n\n\n\n\n\nuid\noffense_code\noffense_type\ndate\nlongitude\nlatitude\n\n\n\n9362175\n26B\ncredit card/automated teller machine fraud\n2015-02-14 11:05:00\n-94.5715\n39.1086\n\n\n9362176\n26B\ncredit card/automated teller machine fraud\n2015-02-14 11:05:00\n-94.5715\n39.1086\n\n\n9362201\n26A\nfalse pretenses/swindle/confidence game\n2015-02-14 13:30:00\n-94.6568\n39.2453\n\n\n9362202\n26A\nfalse pretenses/swindle/confidence game\n2015-02-14 13:30:00\n-94.6568\n39.2453\n\n\n9362213\n26C\nimpersonation\n2015-02-14 15:00:00\n-94.5886\n38.9956\n\n\n9362214\n26C\nimpersonation\n2015-02-14 15:00:00\n-94.5886\n38.9956\n\n\n\n\n\nIf you try to run this code:\n\nselect(frauds, offense_category)\n\nError in `select()`:\n! Can't subset columns that don't exist.\n✖ Column `offense_category` doesn't exist.\n\n\n\nYou will get an error message saying something like:\nError: Can't subset columns that don't exist. x Column `offense_category` doesn't exist\n\n\nIt causes an error.\n\nIn this case it is fairly easy to identify that one of the columns you have tried to select does not exist. Maybe you mis-remembered the name of the column. To find out what the correct column name is, you can either print the first few rows of the object (by typing the code head(frauds) into the R console) or use the names() function to print a list of column names present in the data:\n\nnames(frauds)\n\n[1] \"uid\"          \"offense_code\" \"offense_type\" \"date\"         \"longitude\"   \n[6] \"latitude\"    \n\n\n\nChange the following code to correct the mistaken column name:\n\nselect(frauds, offense_category)\n\nError in `select()`:\n! Can't subset columns that don't exist.\n✖ Column `offense_category` doesn't exist.\n\n\n\nselect(frauds, offense_type)\n\n# A tibble: 6 × 1\n  offense_type                              \n  &lt;chr&gt;                                     \n1 credit card/automated teller machine fraud\n2 credit card/automated teller machine fraud\n3 false pretenses/swindle/confidence game   \n4 false pretenses/swindle/confidence game   \n5 impersonation                             \n6 impersonation                             \n\n\n\nOther errors will be harder to identify and fix. In this tutorial we will go through the process of debugging – identifying, understanding and fixing errors in your code. Sometimes fixing issues with your code can feel like a bit of a roller coaster, but (like most things) it becomes much easier with practice, and if you approach errors in a systematic way.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Handling bugs in your code</span>"
    ]
  },
  {
    "objectID": "07_handling_bugs/index.html#errors-warnings-and-messages",
    "href": "07_handling_bugs/index.html#errors-warnings-and-messages",
    "title": "\n7  Handling bugs in your code\n",
    "section": "\n7.2 Errors, warnings and messages",
    "text": "7.2 Errors, warnings and messages\nWhen something is not quite right, or just when there is an issue in your code that you should be aware of, R has three ways of communicating with you: messages, warnings and errors.\n\n7.2.0.1 Messages\nMessages are usually for information only and typically don’t require you to take any action. For example, the function get_crime_data() from the crimedata package issues a message to tell you what data it is downloading. It does this because downloading data sometimes takes a few seconds and without a message you might wonder if the code was working.\nBy default, messages are displayed in RStudio in dark red text, although this might vary if you have changed any settings in the Appearance tab of the RStudio Options dialogue. You can generate a message for yourself using the message() function. This is useful if you are writing code and you want to remind yourself of something, or record a particular value. For example, if your code is likely to take a long time to run, you might want to record the time your code started running by generating a message on the first line of your code:\n\nmessage(stringr::str_glue(\"Code started: {Sys.time()}\"))\n\nCode started: 2024-02-01 13:29:43.308724\n\n\nWhen R prints a message about your code, any code underneath the code that generated the message will still run:\n\n2 * 2\n\n[1] 4\n\nmessage(\"This is a message. It might be important so make sure you understand it.\")\n\nThis is a message. It might be important so make sure you understand it.\n\n2 / 2\n\n[1] 1\n\n\n\n7.2.0.2 Warnings\nWarnings are generated when there is a potential problem with your code, but the problem was not serious enough to stop your code running entirely. For example, ggplot2 functions like geom_point() will send a warning if some of the rows in your data contain missing values (e.g. if some crimes have missing co-ordinates).\nWarnings are important and you should take time to read and understand them, but it is possible that having done so it will be safe to not take any action. Whether it is safe to take no action will often depend on exactly what you are trying to do, which is why it is important that you understand each warning that you see. For example, if you already know that some rows in your data contain missing values and are happy to plot the remaining values, it will be safe to ignore the warning produced by geom_point(). But if your dataset should not have any missing values in it, you will need to investigate why geom_point() is warning you about missing values and whether those values have been accidentally introduced by some part of your code.\n\nIt is not safe to ignore warnings unless you are sure why they occurred and certain that you don’t need to take any action.\nOne particularly dangerous scenario is where your code produces warnings but still produces what looks like a reasonable result. In these circumstances it can be tempting to ignore the warnings and assume that everything is fine, since the code still produced roughly what you were expecting. However, it’s possible that the plausible answer is nevertheless wrong because of whatever problem is generating the warning in R. Do not assume that warnings are safe to ignore just because they don’t stop your code running.\n\nWarnings are displayed in the same font as messages, but with the text Warning: at the start. You can generate your own warning messages using the warning() function:\n\nwarning(\"Something might be wrong. Check to make sure.\")\n\nWarning: Something might be wrong. Check to make sure.\n\n\nAs with messages, warnings will not stop your code running. This means that if the warning signalled a genuine problem with your code, the results of the lines underneath the warning might not be reliable. That is why it is important to understand warnings when you see them.\n\n7.2.0.3 Errors\nErrors are generated when there is something wrong with your code or the results it produces that means the code cannot continue running any further. An error might occur, for example, because the function read_csv() could not open the requested file (maybe because the file does not exist). In this case, it would make no sense for the rest of the code to run because it probably depends on the data that read_csv() was supposed to load but could not.\nIt would make sense to be able to generate your own errors using the error() function, but this is one of those times when the function to do something in R has a different name from that you might be expecting. In fact, you can generate an error using the stop() function:\n\nstop(\"Something is defintely wrong. Don't go any further until you've fixed it.\")\n\nError in eval(expr, envir, enclos): Something is defintely wrong. Don't go any further until you've fixed it.\n\n\nThere are two types of errors in R: syntax errors and logical errors. Syntax errors happen when R cannot read your code because of a mistake in how it has been typed out. For example, if you forget to put a comma between the arguments of a function, you will get this error:\n\nmessage(\"A\" \"B\")\n\nError: &lt;text&gt;:1:13: unexpected string constant\n1: message(\"A\" \"B\"\n                ^\n\n\nWhen you run R code, R reads all the code and checks if it can be interpreted as valid R code. If not, R will produce a syntax error. Because all of your code is checked for syntax errors before any code is actually run, a syntax error anywhere in your code will stop all of your code running. Syntax errors are typically fairly easy to fix, because they are usually caused by typos.\nThe second type of error that can happen is a logical error. This happens when R is able to interpret what you have written, but something is wrong with what you have asked it to do. These are called logical errors because there is usually some problem with the logic of what you are asking R to do. Like syntax errors, logical errors can be caused by typos, but logical errors can also have many other causes.\nThere is a saying in programming that a computer will do exactly what you tell it to do, which may not be the same thing as what you wanted it do. Logical errors happen when you have told R to do something that it cannot do. For example, you might be asking R to multiply together a numeric value and a character value (e.g. 3 * \"A\"), which is illogical.\nSince every step in your code depends on the steps that went before it, it is only possible to identify a logical error during the process of running the code. This means that a lot of your code might run successfully before an error occurs.\nLogical errors are typically harder to fix than syntax errors are, because fixing a logical error involves understanding (a) what you have asked R to do and (b) the current state of everything in your code at the moment when the error occurs. Fortunately, there are lots of ways to identify and fix logical errors.\nNow we know what errors, warning and messages are, we need to find out how to deal with them when they happen.\n\n7.2.1 Check your understanding\n\nquiz(\n  caption = \"\",\n  \n  question(\n    \"Which of the following terms is used in R to describe the way of communicating something for information only\",\n    answer(\"A message\", correct = TRUE),\n    answer(\n      \"A warning\",\n      message = \"Not quite right: warnings in R communicate information that you need to investigate so you can decide what action to take.\"\n    ),\n    answer(\n      \"An error\",\n      message = \"Not quite right: errors in R communicate problems that prevent your code from running.\"\n    ),\n    answer(\"A condition\"),\n    correct = random_praise(),\n    allow_retry = TRUE,\n    random_answer_order = TRUE\n  )\n  \n)",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Handling bugs in your code</span>"
    ]
  },
  {
    "objectID": "07_handling_bugs/index.html#finding-problems",
    "href": "07_handling_bugs/index.html#finding-problems",
    "title": "\n7  Handling bugs in your code\n",
    "section": "\n7.3 Finding problems",
    "text": "7.3 Finding problems\nIf an error or warning has a simple cause, such as the example of the incorrect column name in the previous section, you can just fix the problem and re-run the code. For problems that are more difficult to handle, you will need to follow a step-by-step process to find and fix them. Think of this as like being a mechanic fixing a car – first you work out what the problem is, then you fix it.\nIf you code only has one line, it will probably be obvious where any problem lies. But most of your code does several things to achieve a particular goal, such as making a map. The first task in dealing with a problem is therefore to work out exactly which function or other piece of code has caused it. For example, take this data-wrangling code to reduce the size of a dataset and sort it in date order:\n\nlibrary(tidyverse)\n\nfrauds |&gt; \n  select(offense_type, date, longitude, latitude) |&gt; \n  filter(offence_type == \"impersonation\") |&gt; \n  arrange(date)\n\nError in `filter()`:\nℹ In argument: `offence_type == \"impersonation\"`.\nCaused by error:\n! object 'offence_type' not found\n\n\nThis code produces a fairly complicated error message. As is often the case, the most useful part of the error message is the last part:\nInput `..1` is `offence_type == \"impersonation\"`\nThis suggests the error is on line 3 of the code, since that is the only line containing the code offence_type == \"impersonation\". To check this, we can comment out that line by placing a # symbol at the start of the line. Do this and re-run the code above – it should now run without a problem.\nNow we know the problem is on the line filter(offence_type == \"impersonation\"), we can look at that line in more detail. Can you spot the problem with that line?\nThe error message in this case has been caused by a typo – the code offence_type == \"impersonation\" uses the British spelling of the word ‘offence’ but in the dataset the variable is spelled using the American English ‘offense’ (you can see the US spelling in the line of code above the line that is causing the error).\n\nIf you correct the spelling in the code above, it should now run without a problem.\n\nSometimes it will not be as clear as this where to start in looking for the problem. In particular, some errors can be caused by a problem on one line of code, but only actually have a negative effect on a subsequent line of code. For example, if you run this code:\n\nfrauds |&gt; \n  select(offense_code, longitude, latitude) |&gt; \n  filter(offense_code == \"26E\") |&gt; \n  arrange(date)\n\nError in `arrange()`:\nℹ In argument: `..1 = date`.\nCaused by error:\n! `..1` must be a vector, not a function.\n\n\nThe error message produced suggests the problem is with the arrange() function, but everything is correct with that function since arrange() is a correct function name and we already know that date is a column in the tibble named frauds. So the problem must lie elsewhere. In cases like this, it can be helpful to comment out all the lines of code except the first one and then uncomment one line at a time until you find the one that causes the problem.\n\nWith the following code, uncomment one line at a time starting on line 2, re-run the code and then uncomment another line. After you run the code each time, look at the output produced until an error occurs or you find that the output is not what you expected.\n\nfrauds |&gt; \n#  select(offense_code, longitude, latitude) |&gt; \n#  filter(offense_code == \"26E\") |&gt; \n#  arrange(date)\n\nError: &lt;text&gt;:5:0: unexpected end of input\n3: #  filter(offense_code == \"26E\") |&gt; \n4: #  arrange(date)\n  ^\n\n\n\n\n\nDid you get an error beginning &lt;text&gt;:5:0: unexpected end of input 3?\n\n\nWhen commenting out lines of code in a code pipeline, it is important to remember to remove the pipe operator |&gt; from the end of the last line of your code. Otherwise you will get an error that begins &lt;text&gt;:5:0: unexpected end of input 3 because R is expecting the final |&gt; to have some code after it. Remember that R will ignore the lines that are commented out, so make sure every line in your pipeline ends with a |&gt; except the last line that is not commented out.\n\n\nIf you uncommented all but the last line and run the code, you would see that the line select(offense_code, longitude, latitude) removes the column date from the dataset, so it’s not possible to use date as an argument to the arrange() function later on.\nUncommenting one line at a time until you find an error or output that is not what you expected is a useful way to isolate problems, but it will not always work. In particular, it will not work if the problem is caused by some code that should have been included but is missing from your code entirely. For example, if you try to run the function st_transform() on a tibble without first changing it into an SF object:\n\nlibrary(sf)\n\nLinking to GEOS 3.11.0, GDAL 3.5.3, PROJ 9.1.0; sf_use_s2() is TRUE\n\nfrauds |&gt; \n  select(offense_code, longitude, latitude) |&gt; \n  filter(offense_code == \"26A\") |&gt; \n  st_transform(\"EPSG:3603\")\n\nError in UseMethod(\"st_transform\"): no applicable method for 'st_transform' applied to an object of class \"c('tbl_df', 'tbl', 'data.frame')\"\n\n\nIn these cases it is particularly useful to check every argument that you have used in a function to track down the error. We will look at this later in this tutorial.\n\n7.3.1 Errors caused by data problems\nMany logical errors will be caused by problems with the code you have written, such as when you try to use a function (e.g. st_intersection()) that only works on an SF object but specify that it should use an object of another type. But sometimes logical errors are caused not by your code, but by a mis-match between the structure that your data actually has and the structure you think your data has. We have already seen an example of this in this tutorial, in the code that tried to refer to a column called offense_category in a dataset that did not have a column with that name.\nErrors caused by a mismatch between the data you think you have and the data you actually have can be particularly frustrating, because there is no way to identify them from just looking at your code. For this reason, it is often important when trying to identify problems in your code to look at the data that is used as the input for your code, and the data that is produced by each step in your code. We used this technique to find the error in our code above that was caused by filter() removing all the rows from a dataset because we had told filter() to only keep rows containing a value that was not present in the dataset. There were no obvious problems with the code we had written, so the only way to find the cause of this problem was to view the dataset returned by the filter() function.\nFinding data problems is one of the reasons why we have used the head() function so often in these tutorials to look at the data at each step in writing a block of code. head() only shows us the first few rows of a dataset, which will not always be enough to identify a problem if the problem is caused by values that are only present in a few rows in the data. For small datasets, we can use the View() function (note the capital letter) to open the entire dataset in a new tab in RStudio.\nFor bigger datasets, this will not work. In that case, we can use the sample_n() or sample_frac() functions from the dplyr package to return a random sample of rows from the data. This can be useful to let us look at a representative sample of a large dataset. sample_n() returns a specific number of rows, e.g. sample_n(frauds, 10) returns 10 rows at random from the frauds dataset. sample_frac() returns a specific proportion of the dataset, e.g. sample_frac(frauds, 0.1) returns a sample of 10% of rows from the data.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Handling bugs in your code</span>"
    ]
  },
  {
    "objectID": "07_handling_bugs/index.html#understanding-problems",
    "href": "07_handling_bugs/index.html#understanding-problems",
    "title": "\n7  Handling bugs in your code\n",
    "section": "\n7.4 Understanding problems",
    "text": "7.4 Understanding problems\nSo far, we have tried two ways to deal with errors:\n\nreading a simple error message that makes the problem obvious,\ncommenting out all the code and then uncommenting one line a time until the error appears.\n\nSometimes you will encounter an error that is still not simple to solve. In this case, it is still important to identify the line of code causing the problem, either by working it out from the text of the error message or commenting out lines of code in turn.\nOnce you know what line is causing the problem, you should focus on understanding exactly what that line of code does. In particular:\n\nWhat data do any functions on that line expect to work on?\nWhat are the values of any arguments to the functions on that line?\nWhat value do the functions on that line produce?\n\nYou can get a lot of help in understanding each function by referring to its manual page. You can access the manual page for a function by:\n\ntyping a question mark followed by the function name without parentheses (e.g. ?mutate) into the R console,\ntyping the function name without parentheses into the search box in the Help panel in RStudio, or\nclicking on the function name anywhere in your R code to place the cursor on the function name, then pressing F1 on your keyboard.\n\nAny of these options opens up a manual page for that function in the Help panel in RStudio. For example, this is the manual page for the str_wrap() function from the stringr package. You can load it by typing ?str_wrap in the R console.\n\n\nDid you get an error beginning No documentation for ‘str_wrap’?\n\n\nWhen you use the ? operator to look up the manual page for a function, R searches through all the functions in the packages you have loaded. If there is no function of the name you have given in the loaded packages, R will produce an error.\nTo access the manual page of a function in a package that is not loaded, you can specify the package in which R should look for the function using the package name and the :: operator (this is the same way that we have already learned to call functions from packages that haven’t been loaded). For example, you can use the code ?stringr::str_wrap to access the manual page for the str_wrap() function from the stringr package even if the stringr package is not loaded.\n\n\n\n\nAll manual pages have the same format.\n\n\nDescription gives a short description of what the function does. If multiple related functions are described in a single manual page, this section will explain the differences between them. For example, the manual page for the mutate() function from the dplyr package explains the difference between the mutate() function and the closely related transmute() function.\n\nUsage shows a single example of how the function works. If there are any optional arguments to the function, this section will show what the default values of those optional arguments are. For example, the manual page for the str_wrap() function from the stringr package show that the default value of the width argument is width = 80.\n\nArguments gives a list of arguments and the values they can take. It is particularly important to note the type of value expected. So the st_transform() function from the sf package expects an SF object as its first argument – if you provide another sort of object (such as a tibble), this will cause an error.\n\nValue explains the type of value that the function will return, and whether this value might be of a different type depending on the values of particular arguments. For example, the mean() function in base R returns the arithmetic mean of a vector of numbers. However, if any of the numbers is NA then mean will return NA unless the argument na.rm = TRUE is used. In that case, mean() will ignore the missing values and return the mean of the values that are present.\n\nExamples gives more examples of how the function can be used.\n\nChecking the manual page for a function can often help you understand why a particular piece of code is not working. If you have set any optional arguments for a function that is producing an error, it may help to reset those arguments to their default values (as shown in the Usage section of the manual page) one by one to understand what effect this has on your code.\nBy reading the error message, isolating the error by commenting out and then reading the manual page, you will be able to fix almost all the errors you will come across in writing R code. Occasionally, however, you will find an error that you just can’t understand. In that case, you will need to get some help from others.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Handling bugs in your code</span>"
    ]
  },
  {
    "objectID": "07_handling_bugs/index.html#how-to-fix-some-common-errors",
    "href": "07_handling_bugs/index.html#how-to-fix-some-common-errors",
    "title": "\n7  Handling bugs in your code\n",
    "section": "\n7.5 How to fix some common errors",
    "text": "7.5 How to fix some common errors\nThere are some mistakes that it is common for people to make when writing code. As you get more experience in writing R code and dealing with error messages, you are likely to start to recognise some simple errors (especially those caused by typos) and know how to fix them quickly. One useful way to quickly find help on common errors is to check if the error (and the corresponding solution) appears on this list of common R errors and how to fix them.\n\n7.5.1 Check your understanding\n\nquiz(\n  caption = \"\",\n  \n  question(\n    'What might cause the error message `could not find function \"blah\"`?',\n    answer(\n      \"You have not loaded the package that contains the `blah()` function\", \n      correct = TRUE\n    ),\n    answer(\n      \"You have mis-typed the name of the `blah()` function\",\n      correct = TRUE\n    ),\n    answer(\n      \"You have mis-typed the name of the `blah` object\",\n      message = \"Not quite. This error message relates to a function called `blah`, not an object.\"\n    ),\n    answer(\n      \"You have tried to use a generic function with a type of object the function does not know how to use\"\n    ),\n    correct = random_praise(),\n    allow_retry = TRUE,\n    random_answer_order = TRUE\n  )\n  \n)",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Handling bugs in your code</span>"
    ]
  },
  {
    "objectID": "07_handling_bugs/index.html#getting-external-help",
    "href": "07_handling_bugs/index.html#getting-external-help",
    "title": "\n7  Handling bugs in your code\n",
    "section": "\n7.6 Getting external help",
    "text": "7.6 Getting external help\nIf you cannot fix an error using any of the techniques we have already covered, it is probably time to get some help from others. Fortunately, one of the big benefits of using R is that there is a friendly, welcoming community of R coders online who are willing to help fix problems. Almost everyone in this community will remember when they were knew to using R and so will be gentle with people who are asking their first question.\n\n7.6.1 Reproducible examples\n\nOne of the things that makes it much more likely that you will find help with your problem in the R community is if you phrase your plea for help in a way that makes it easier to help you. We can do this by providing a reproducible example or reprex of our problem (also sometimes called a minimum working example).\nProducing a reprex makes it much easier for someone to understand your issue. This not only makes it easier for someone to help you, but also shows that you know it will make it easier for them and that you value their time.\n\nImagine that you’ve made a cake, and for some reason it’s turned out absolutely awful – we’re talking completely inedible. Asking a question without a reprex is like asking, “Why didn’t my cake turn out right?” – there are hundreds of possible answers to that question, and it’s going to take a long time to narrow in on the exact cause for your inedible cake creation.\nAsking a question with a reprex is like asking, “My cake didn’t turn out, and here’s the recipe I used and the steps that I followed. Where did I go wrong?” Using this method is going to significantly increase the likelihood of you getting a helpful response, faster!\n\nTo make a reprex, we have to do two things:\n\nRemove everything from our code that does not contribute to causing the error. We do this by removing each line from our code and only keeping those lines that are necessary to produce the error – this is why a reproducible example is sometimes called a minimum working example.\nMake sure that someone trying to help us can reproduce the issue on their own computer even if they don’t have access to the particular dataset we are using. We do this by replacing our own dataset with a publicly available one, preferably one of the datasets that are built into R for exactly this purpose.\n\nWatch this video to see the process of making a reprex in RStudio:\n\n\n7.6.2 Reproducible code\nOur first step is to remove every line from our code that isn’t necessary to produce the error. For example, run the following code to see what error it produces.\n\n# Load packages\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(readr)\n\n# Load a dataset from your computer and wrangle it\nroad_deaths &lt;- read_csv(\"road_deaths_data.csv\") |&gt; \n  janitor::clean_names() |&gt; \n  rename(ksi_drivers = drivers, ksi_pass_front = front, ksi_pass_rear = rear) |&gt; \n  select(-petrol_price, -van_killed) |&gt; \n  mutate(\n    law = as.logical(law),\n    ksi_driver_rate = ksi_drivers / (kms / 1000)\n  )\n\nRows: 192 Columns: 9\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl  (8): DriversKilled, drivers, front, rear, kms, PetrolPrice, VanKilled, law\ndate (1): month_beginning\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n# Make a time-series chart of two continuous variables, coloured by a \n# categorical variable, then add a trend line\nroad_deaths + \n  ggplot(aes(x = month_beginning, y = ksi_driver_rate)) + \n  geom_point(aes(colour = law)) + \n  geom_smooth() +\n  scale_x_date(date_breaks = \"2 years\", date_labels = \"%Y\") +\n  scale_y_continuous(labels = scales::comma_format(), limits = c(0, NA)) +\n  scale_colour_brewer(type = \"qual\") +\n  labs(\n    x = NULL, \n    y = \"drivers killed or seriously injured per 1,000km travelled\", \n    colour = \"after seat belts made mandatory\"\n  ) +\n  theme_minimal() +\n  theme(\n    axis.line.x = element_line(colour = \"grey90\"),\n    axis.ticks = element_line(colour = \"grey90\"),\n    panel.grid.major.x = element_blank(),\n    panel.grid.minor.x = element_blank(),\n    legend.position = \"bottom\"\n  )\n\nError in `fortify()`:\n! `data` must be a &lt;data.frame&gt;, or an object coercible by `fortify()`,\n  not a &lt;uneval&gt; object.\nℹ Did you accidentally pass `aes()` to the `data` argument?\n\n\nAs you can see, this code produces an error message that is not easy to decipher, so we might need help to deal with it. As the first stage to making a reprex, remove all the lines of the code above that it is possible to remove while still producing the same error. If the error disappears and the chart is produced successfully, you have probably found the line that contains the error. If you remove a line and the code starts to produce a different error, you have removed a line that is needed to produce the original error and should put that line back into the code.\nIt is actually possible to remove a lot of the original code and still produce the same error. You can remove:\n\nAll the lines of code that fine-tune the appearance of the chart (lines 22–37 above).\nThe line creating the trend line (line 21).\nThe code that wrangles our data in ways that don’t affect the error (lines 8–14).\nThe line that loads the dplyr package (line 2), since we are no-longer using the data wrangling functions from that package.\n\nWe cannot remove the code that loads necessary packages (lines 3 and 4), loads the data (line 7) or produces the basic unformatted chart (lines 18–20), because if we remove any of those then the error message either changes or disappears.\nThis leaves us with the following code, which produces the same error message but is much easier for someone to check for errors because it is much shorter. Because we have removed the data-wrangling code, we have had to change the name of the argument on line 19 of the code above from y = ksi_driver_rate to y = drivers, since the column ksi_driver_rate is no longer in the data.\n\nIf we forgot to change y = ksi_driver_rate to y = drivers then the code would still produce an error, but it would be a different error. The purpose of producing a reprex is to find the minimum code that still produces the same error we are interested in. If you remove a line of code and the error message you see changes, put that line of code back.\n\nIf this shortened code were to run successfully then the resulting chart would look quite different to the original chart we wanted, but that does not matter because what we are interested in is showing code that produces a specific error. If you run this code, you will see it produces the same error as the code above.\n\n# Load packages\nlibrary(ggplot2)\nlibrary(readr)\n\n# Load a dataset from your computer and wrangle it\nroad_deaths &lt;- read_csv(\"road_deaths_data.csv\")\n\nRows: 192 Columns: 9\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl  (8): DriversKilled, drivers, front, rear, kms, PetrolPrice, VanKilled, law\ndate (1): month_beginning\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n# Make a time-series chart of two continuous variables, coloured by a \n# categorical variable, then add a trend line\nroad_deaths +\n  ggplot(aes(x = month_beginning, y = drivers)) + \n  geom_point(aes(colour = law))\n\nError in `fortify()`:\n! `data` must be a &lt;data.frame&gt;, or an object coercible by `fortify()`,\n  not a &lt;uneval&gt; object.\nℹ Did you accidentally pass `aes()` to the `data` argument?\n\n\n\n7.6.3 Reproducible data\nOur shortened code would make a great reproducible example except for one thing: the data file road_deaths_data.csv only exists on our computer. This means the example is not actually reproducible, since anyone trying to run this code on their computer to identify the error would find that they instead got a different error saying that the file road_deaths_data.csv was not found.\nYou could deal with this by uploading your dataset to a website and then having the read_csv() function read it from that URL. But you might not want to share your data (perhaps it is sensitive in some way), or your dataset might be too large to post online. For this reason, many R packages come with toy datasets that can be used in learning or in testing for errors. You can see a list of all the toy datasets available in the packages you have loaded by typing data() in the R console. This will produce a file that gives the name and description of each available dataset.\nTo use one of these toy datasets, you just use the the name of the dataset as you would use any other R object (like the road_deaths object we created above). One commonly used toy dataset is the mpg dataset from the ggplot2 package, which contains fuel economy data for 38 models of car.\nThe data in this dataset are on a completely different topic to the data we were trying to use, but this does not matter as long as the data contains variables of the same type (numeric, character, etc.) as the original data. We can see what variables are in the mpg dataset using the head() function as usual.\n\nlibrary(ggplot2)\n\nhead(mpg)\n\n# A tibble: 6 × 11\n  manufacturer model displ  year   cyl trans      drv     cty   hwy fl    class \n  &lt;chr&gt;        &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt;      &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; \n1 audi         a4      1.8  1999     4 auto(l5)   f        18    29 p     compa…\n2 audi         a4      1.8  1999     4 manual(m5) f        21    29 p     compa…\n3 audi         a4      2    2008     4 manual(m6) f        20    31 p     compa…\n4 audi         a4      2    2008     4 auto(av)   f        21    30 p     compa…\n5 audi         a4      2.8  1999     6 auto(l5)   f        16    26 p     compa…\n6 audi         a4      2.8  1999     6 manual(m5) f        18    26 p     compa…\n\n\nFrom this, we can see that there is a year variable that we can use as a substitute for the month_beginning variable in our original code, a variable called hwy that is numeric and so can be substituted for the drivers variable in our code, and a categorical variable called trans that we can substitute for the law variable in our data. This means we can use this data instead of our own data, knowing that anyone on any computer with the ggplot2 package installed can run the code and should get the same result.\n\nChange the code below so that it uses the mpg dataset rather than loads data from the road_deaths_data.csv file. You will need to change the variable names in the stack of ggplot() functions.\n\n# Load packages\nlibrary(ggplot2)\nlibrary(readr)\n\n# Load a dataset from your computer and wrangle it\nroad_deaths &lt;- read_csv(\"road_deaths_data.csv\")\n\nRows: 192 Columns: 9\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl  (8): DriversKilled, drivers, front, rear, kms, PetrolPrice, VanKilled, law\ndate (1): month_beginning\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n# Make a time-series chart of two continuous variables, coloured by a \n# categorical variable\nroad_deaths +\n  ggplot(aes(x = month_beginning, y = drivers)) + \n  geom_point(aes(colour = law))\n\nError in `fortify()`:\n! `data` must be a &lt;data.frame&gt;, or an object coercible by `fortify()`,\n  not a &lt;uneval&gt; object.\nℹ Did you accidentally pass `aes()` to the `data` argument?\n\n\n\n\nWe can change our existing reprex code to use the mpg dataset rather than load data from the road_deaths_data.csv file by changing the variable names in the ggplot() stack.\n\n\n# Load packages\nlibrary(ggplot2)\n\n# Make a time-series chart of two continuous variables, coloured by a \n# categorical variable\nmpg +\n  ggplot(aes(x = year, y = hwy)) + \n  geom_point(aes(colour = trans))\n\nError in `fortify()`:\n! `data` must be a &lt;data.frame&gt;, or an object coercible by `fortify()`,\n  not a &lt;uneval&gt; object.\nℹ Did you accidentally pass `aes()` to the `data` argument?\n\n\nWe have now managed to reduce our original 37 lines of code down to 8 lines, as well as making the example reproducible by using a widely available toy dataset. The shorter code still produces the same error while being much easier to read, so we are much more likely to get help quickly than if we had just sent someone our original code.\nMost of the time, the act of producing a reprex will be enough for us to find and fix the error without any external help. Can you see the problem with our code that is making this error happen? If not, we will reveal it at the end of this tutorial.\n\n7.6.4 Checking your reprex is reproducible\nNow that you have the minimum code needed to reproduce the error, it’s almost time to share it with people who can help you. But before you do that, it’s worth checking that the code is truly reproducible. To do this we will use the reprex package, which is part of the tidyverse suite of packages you already have installed.\n\n\n\nTo use the reprex package, first put your code in a separate R document in the Source panel in RStudio. Open a new R script in RStudio now and paste the code from the last exercise into it. Once you’ve done that, select all the code in that document. Now click the Addins button in RStudio and scroll down until you can choose Reprex selection.\nAfter a few seconds, some code should appear in the RStudio Viewer panel showing your code and the error message that it produces. This code has also been copied to your computer clipboard so that you can paste it into an email or web form when you are asking for help.\nIf the error message that you see along with the code in the Viewer panel is not the error message you were expecting, your example is not yet reproducible. For example if you tried to run the Reprex selection command on the original code that we started this section with, we would get an error message 'road_deaths_data.csv' does not exist in current working directory.\nOnce your reprex produces the same error as the code you originally had the issue with, you’re ready to share it to get help.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Handling bugs in your code</span>"
    ]
  },
  {
    "objectID": "07_handling_bugs/index.html#sources-of-help",
    "href": "07_handling_bugs/index.html#sources-of-help",
    "title": "\n7  Handling bugs in your code\n",
    "section": "\n7.7 Sources of help",
    "text": "7.7 Sources of help\nIf you are being taught R by a formal instructor, or you have friends or colleagues who can help, they will probably be the first people that you go to for help.\nIf this doesn’t work, or if you are the most proficient R user that you know, you might need another place to turn to. Fortunately, R has a large community of volunteers who will help you. Before you ask people online for help, it’s important to check that someone hasn’t already asked the same question and had it answered. Duplicate questions increase the workload of the volunteers who answer questions and slow everything down, so if your question has frequently been answered already it’s possible your question will just be ignored.\nTo find out if there is an answer to your question, the easiest thing to do is to search the error message online. Google, or another search engine of your choice, is definitely your friend. If you search online for the error message that was produced by our reprex code, you will see that there are over 100 pages discussing this error message and how to fix it.\n\n7.7.0.1 Stack Overflow\nLet’s imagine, though, that there were no relevant hits when we searched for the error message, or that none of the results was useful. In that case, we need to pose a new question to the R community. The place to find the largest slice of that community is probably the website Stack Overflow. This is a website for people who are writing code in any programming language imaginable to get help. It is part of the larger Stack Exchange Network of question-and-answer websites covering everything from travel to veganism.\nTo ask a new question on Stack Overflow, go to stackoverflow.com/questions/ask and create an account or log in. You will now be asked to complete a short form with your question. Questions are more likely to get an answer faster if you:\n\nGive the question a specific title. Over 20 million questions have been asked on Stack Overflow since it launched, so a generic title like ‘Help’, ‘R error’ or even ‘ggplot error’ will not help other people find your question. Look at some recent questions about R on Stack Overflow to get some ideas on what title to give for your question.\nIn the body of your question, briefly (2–3 lines should do it) explain what you were trying to do, then paste the reprex output that the Reprex selection addin copied to your clipboard into the question body box underneath your brief explanation. You will see that Stack Overflow recognises the format of your code and shows you a preview of it underneath the question box.\nFinally, add the tag r to the Tags box so that people know your question is about coding in R. This is crucial – if you do not tag your question as being about R, it is very unlikely that volunteers who know about R will be able to find your question.\n\nSubmit your question and wait for an answer. As soon as someone answers your question, or comments on it to ask for more detail, you will get an email alert. Many questions are typically answered within a few hours. Hopefully this will help you get to the final stage of the emotional roller coaster of debugging:",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Handling bugs in your code</span>"
    ]
  },
  {
    "objectID": "07_handling_bugs/index.html#coding-with-ai",
    "href": "07_handling_bugs/index.html#coding-with-ai",
    "title": "\n7  Handling bugs in your code\n",
    "section": "\n7.8 Coding with AI",
    "text": "7.8 Coding with AI\nOne topic that we haven’t covered yet is using artificial intelligence (AI) to help us produce R code. The type of AI that can be used to help produce code is called a large language model (LLM), sometimes known as a generative AI model. The best known of these among the public is probably ChatGPT, but there are lots of similar services available (and new ones being released all the time).\n\n7.8.0.1 Coding with AI: an example\nGenerative AI services can help you to write code, especially code that does very common tasks like loading data. For example, say that I want to load the Vancouver thefts dataset that we used in a previous tutorial and convert the column names in the data to snake_case. As a reminder, the code we previously used to do that looked like this:\nlibrary(tidyverse)\n\nvancouver_thefts &lt;- read_csv(\"https://mpjashby.github.io/crimemappingdata/vancouver_thefts.csv.gz\") |&gt; \n  janitor::clean_names()\nI can ask ChatGPT (version 3.5) to write code for me that will do this task. If I ask ChatGPT “Show me the R code needed to load the CSV dataset at https://mpjashby.github.io/crimemappingdata/vancouver_thefts.csv.gz and make the column names into snake case”, it will produce some R code that looks roughly like this:\n# Install and load the required packages if not already installed\n# install.packages(\"readr\")\n# install.packages(\"snakecase\")\nlibrary(readr)\nlibrary(snakecase)\n\n# URL of the CSV file\nurl &lt;- \"https://mpjashby.github.io/crimemappingdata/vancouver_thefts.csv.gz\"\n\n# Read the CSV file and convert column names to snake case\ndata &lt;- read_csv(url, col_names = TRUE) %&gt;%\n  snakecase::to_snake_case()\n\n# Display the first few rows of the dataset\nhead(data)\nChatGPT also produces a short explanation of what the code does:\n\nThis code uses the read_csv() function from the readr package to read the CSV file from the specified URL. The col_names = TRUE argument ensures that column names are read from the first row of the CSV file. The snakecase::to_snake_case() part applies the transformation to snake case for all column names.\n\n\nMake sure to install the readr and snakecase packages before running this code if you haven’t done so already. You can uncomment the install.packages(\"readr\") and install.packages(\"snakecase\") lines for installation.\n\nUnfortunately, if you try to run the code above you will find it produces two errors:\n\n\ncould not find function \"%&gt;%\". The code generated by ChatGPT uses the %&gt;% operator, which is an older version of the |&gt; pipe operator we use in this course. But the %&gt;% operator is part of the magrittr package, so to use it the code needs to load the magrittr package first.\n\nargument is not a character vector. The code produced by ChatGPT passes the tibble loaded by read_csv() to the to_snake_case() function from the snakecase package. But to_snake_case() expects a vector of character values (e.g. c(\"aCharacterValue\", \"ANOTHERCHR\")), not a tibble. This is in contrast to janitor::clean_names(), which performs the same function and does accept a tibble as input.\n\nThere are also a few of more-minor issues that won’t produce errors, but might make your code harder to work with:\n\nThe code creates an object called url. Storing the URL in a separate object is unnecessary (since we only need the URL once) and creates a risk that we could end up loading the wrong dataset if we later accidentally over-wrote the value of the url object with a different URL.\nThe code creates an object with the generic name data, which might make it harder to remember later on what data is stored in that object. This is especially likely to be an issue if we come back to the code after a break, or if we need to use multiple datasets as part of our analysis.\nThe code loads the snakecase package, but then also refers to the package using the :: notation when using the function to_snake_case(). This is not a big deal, but makes the code slightly longer and therefore marginally harder to read.\n\n\n\nWhy did I get a different result when I put the same prompt into ChatGPT?\n\n\nIf you put the text “Show me the R code needed to load the CSV dataset at https://mpjashby.github.io/crimemappingdata/vancouver_thefts.csv.gz and make the column names into snake case” into ChatGPT 3.5, it is possible that you will get a slightly different response to the one shown above. This is because large language models are stochastic or non-deterministic models, which means that there is some element of chance involved in exactly what response the model will produce for a particular input. In fact, if you provide exactly the same input to ChatGPT multiple times, it is likely you will get a slightly different answer each time.\n\n\n\n7.8.0.2 Why did ChatGPT produce code that doesn’t work?\nWhy do these problems happen? To understand that, we need to know just a little bit about how large language models such as ChatGPT work. LLMs work by looking for patterns in very large datasets (e.g. all the pages on the world wide web) and then using those patterns to predict what the most likely answer is to questions or requests that include particular words and phrases. To give a very simple example, an LLM might be able to identify that if the first few words in a sentence are ‘Today was Sunday, which meant that tomorrow …’ then it is likely (but – crucially – not certain!) that the next words would be ‘is Monday’.\nBecause they are trained on very large samples of data, LLMs can produce answers that are sophisticated enough that they almost look like magic. But behind the scenes, what models like ChatGPT are doing is predicting what the most likely combination of words a response will contain, based on the input that it is given. It is important to remember that models such as ChatGPT do not know the correct answer to a question, since they do not actually understand the question in the way that humans do. This means that ChatGPT cannot give you any indication of how confident it is about the answer it has given: it will be equally confident in its answer whether it is right or wrong.\n\n\n\n7.8.0.3 Should I use AI to help me code?\nSo given these issues, does that mean that we shouldn’t use LLMs to help us write code? Not necessarily. Lots of programmers find LLMs useful for helping them write code. However, LLMs like ChatGPT are useful for supporting humans to write effective code, not for replacing humans writing code. Even if you asked an LLM to write every part of your code for you, it is likely you would need to make some changes to the code it produced to make it work.\nThat means that to use an LLM effectively to help you write code, you need to understand the code that the LLM produces. That’s why this course teaches you to understand how to write code to produce crime maps in R, rather than just how to use an LLM to write the code for you.\nWhether to use an LLM to help you write code is up to you. Some people find it saves them time and helps them get started on how to produce a particular bit of code, while others find that they spend just as much time fixing the code produced by an LLM as it would take them to write the code themselves. You might want to experiment with getting help from an LLM so you can see what works for you.\n\n7.8.0.4 Using AI to help map crime\nThere are two further important points to make about using LLMs in the context of crime mapping. First, it is vital that you do not upload any sensitive data to an LLM that is hosted online, since it is likely that the company that runs the LLM will have reserved the right to use any data you submit to improve the model in future. You must make sure that sensitive data such as victim details are not posted online or shared unlawfully.\nThe second important point is that the analysis that we use crime mapping for is usually done to answer important questions such as where crime-prevention initiatives should be focused or where a serial offender might live. It is vital that the answers we produce to these questions are accurate. Part of making sure the results of our analysis are accurate is to make sure that we understand exactly how our code produces a particular result. If you use an LLM to support your coding, it is vital that you still understand what the code does.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Handling bugs in your code</span>"
    ]
  },
  {
    "objectID": "07_handling_bugs/index.html#in-summary",
    "href": "07_handling_bugs/index.html#in-summary",
    "title": "\n7  Handling bugs in your code\n",
    "section": "\n7.9 In summary",
    "text": "7.9 In summary\n\n7.9.0.1 A workflow for handling errors in R\n\nWhen you run some code in R and it produces an error, follow the steps learned in this tutorial to identify and fix the problem.\n\nRead the error message and fix any obvious problems.\nMake sure your code follows the style guide introduced in the previous tutorials, which will make it easier to see where there might be problems, especially those caused by typos. In particular, make sure that each function is on a separate line so you can comment them out individually.\nComment out all the lines of your code. Uncomment each line in turn (starting with the first line), re-running the code each time until you see the error you’re trying to understand. Now you know which line is likely to be causing the problem.\nIf the error message is one of the common errors explained in this tutorial, follow the steps outlined in the previous section to fix it.\nIf that does not fix the problem, read the manual page for the function that you think is causing the problem. Check that the values you have passed to each argument are what the function needs in order to work.\nIf that does not fix the problem, create a reproducible example by removing all the parts of your code not needed to create the error. Often, the act of creating a reproducible example will help you fix the problem, but if not then you can use it to get help from others.\n\nYou will be able to fix almost all the errors in your code by following the early steps in this list. Only quite rarely will you need to post a reprex online to get help.\n\n\n7.9.0.2 What caused the error in our reproducible example?\nThe error in our reproducible example was very simple, but quite difficult to spot. On line 7 of the code below, we try to add the ggplot() function to the mpg object using the + operator when what we wanted to do was pass the mpg object to the ggplot() function using the |&gt; operator. R does not know how to add a dataset to a function in this way so it produced an error message.\nIf you replaced + with |&gt; on line 6 of the code below, the code would now run normally. Since we have removed almost all of our original code to make a reproducible example, the resulting plot looks nothing like what we wanted. This does not matter – when we are producing a reprex we only care about reliably producing the same error. Now that we have fixed the error, we could go back and fix the original code to produce the chart we wanted.\n# load packages\nlibrary(ggplot2)\n\n# make a time-series chart of two continuous variables, coloured by a \n# categorical variable\nmpg + # &lt;---- THE `+` OPERATOR HERE SHOULD BE A `|&gt;` OPERATOR INSTEAD\n  ggplot(aes(x = year, y = hwy)) + \n  geom_point(aes(colour = trans))\nThe mistake in this code is a very easy one to make, because what + does inside a ggplot() stack is so similar to what |&gt; does in passing the result of one function to the next function in an data-wrangling pipeline. Remember that we only use + to combine functions inside a ggplot() stack, and use |&gt; to combine functions everywhere else.\n\nIn this tutorial we have learned how to handle messages, warnings and errors in R. We have learned to take time to understand error messages, to isolate errors so that we can better understand them, to use manual pages for functions to check that every argument is correct, and how to write reproducible examples so that we can get help online. This will help you become a more independent coder.\n\n\nFor more information on writing reproducible examples, see:\n\nWatch the webinar Creating reproducible examples with reprex by Jenny Bryan.\nRead the Reprex do’s and don’ts on the reprex package website.\nLearn more about how to use Six tips for better coding with ChatGPT\n\n\n\n\nArtwork by @allison_horst",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Handling bugs in your code</span>"
    ]
  },
  {
    "objectID": "08_projects/index.html#introduction",
    "href": "08_projects/index.html#introduction",
    "title": "\n8  Don’t get lost in data analysis\n",
    "section": "\n8.1 Introduction",
    "text": "8.1 Introduction\nImagine you’ve been working on some analysis intermittently for a few weeks. Unless you’re very lucky, you’re likely to experience one or more of these common problems:\n\nAs you work, you create various different files (for example containing the data you are using, code you have written and charts you have made) until it becomes difficult to keep track of what is in each file and how different files relate to each other.\nIt becomes difficult to remember exactly what data and which code produced which chart, so that if someone has a question about how you came to a particular conclusion it is difficult to be sure. Likewise, if you discover a mistake in your code then it is hard to know what outputs it affected.\nIf you need to take a break from your work (perhaps to work on something else) and come back to it later, it becomes irritating to have to close all the code and other files you are working on and then open them again later.\n\nAll of this can make data analysis much harder, especially if you’re trying to find a particular file or check if a result is correct just before a deadline.\n\n\nFortunately, we can solve these problems using a combination of the RStudio Projects feature and a plan for organising our files. Once you’ve formed the habit of working in this way, it becomes second nature and you’ll find your work is both easier and produces more-reliable results.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Don't get lost in data analysis</span>"
    ]
  },
  {
    "objectID": "08_projects/index.html#rstudio-projects",
    "href": "08_projects/index.html#rstudio-projects",
    "title": "\n8  Don’t get lost in data analysis\n",
    "section": "\n8.2 RStudio Projects",
    "text": "8.2 RStudio Projects\nProjects in RStudio are a way of keeping your analysis organised. When you work inside an project, RStudio will do various things in the background that make it easier to stay organised. For example, RStudio will save the project’s state: if you have to close the project and re-open it later, all the files that you had open before will re-open, too.\n\nI strongly recommend that you use projects for everything you do in RStudio. There are no real costs to using projects and the benefits can be substantial, so try to get used to using them for all your analysis. Have one RStudio project for each project that you work on, for example you might have one project for each research report that you write or piece of analysis you do on a particular topic.\n\nTo create a new project from within RStudio, click File then New Project … and choose to create the project in a new directory on your computer:\n\n\nChoose to create a new (empty) project, rather than use any of the templates for specific types of projects:\n\n\nChoose what the new project  directory should be called, and where on your computer it should be created (you might want to create a ‘Crime Mapping’ directory to hold all the projects you will create during this course), then click Create Project:\n\n\nThat’s it – you have created an RStudio project that you can organise your work in.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Don't get lost in data analysis</span>"
    ]
  },
  {
    "objectID": "08_projects/index.html#organising-a-project",
    "href": "08_projects/index.html#organising-a-project",
    "title": "\n8  Don’t get lost in data analysis\n",
    "section": "\n8.3 Organising a project",
    "text": "8.3 Organising a project\nNow you’ve created an empty project, you can create some directories within it to organise your files. There are lots of suggestions online for how to organise your files within a project – see this summary of project-organisation methods if you would like to understand the different options available – but the most important thing is that you have separate places to store:\n\nthe original/raw data that you have obtained (e.g. by downloading it from a website or extracting it from a police database),\nthe results of any data wrangling that you do,\nfiles containing the code that you use to wrangle the data and conduct your analysis, and finally,\nthe outputs from your analysis, including tables, charts and reports.\n\nPutting each of these four types of files in a separate directory within the project directory makes it much easier to keep track of them. We can create the necessary directories directly in R using the dir.create() function.\n\n# `dir.create()` does not produce any output unless there is a problem\ndir.create(\"source_data\")\ndir.create(\"analysis_data\")\ndir.create(\"code\")\ndir.create(\"outputs\")\n\n\nBecause we are running these commands from inside an RStudio project, these directories will automatically be created as subdirectories of the project directory we have already created – this saves a lot of moving between directories, and is another reason to use RStudio projects.\nOnce you are working within an RStudio project, RStudio knows to treat any reference to a file or folder as being relative to the folder that contains the RStudio project. So, for example, if your project for analysis of thefts in Vancouver is stored in a folder called /Documents/crime_mapping/vancouver_thefts/ and you run the R code read_csv(\"source_data/vancouver_thefts.csv\"), RStudio knows that you mean to open the file stored at /Documents/crime_mapping/vancouver_thefts/source_data/vancouver_thefts.csv. This can save you lots of time working out how to reference a particular file location in R.\n\n\n\nWhat about setting a working directory?\n\n\nIf you have learned some R before this course – especially if it was some time ago or taught by someone who themselves learned R a long time ago – you might have learned to start each analysis by setting a working directory using the setwd() function or the Session &gt; Set Working Directory menu option in RStudio. Not only do you not need to do this when you keep your analysis in an RStudio project, you should not set a working directory in R because it can have several unpleasant side effects.\nThe biggest problem with manually setting a working directory in your R code is that if you share your code with someone else then it is likely the setwd() function will produce an error because that person’s computer will have a different directory structure to your computer. So if the first line of your R script is setwd(\"/Users/uctzaaa/Documents/crime_mapping_vancouver_thefts/\") because that is the location of the working directory on your computer, this will produce an error on another computer that does not have a /Users/uctzaaa directory (i.e. it will cause an error on almost every computer in the world). This makes it much harder to get help on your code, for example if you are using a reproducible example (reprex) to show someone else a problem with your code.\nSetting a working directory using the Session &gt; Set Working Directory menu buttons in RStudio has the additional problem that it’s not possible to work out from reading your R code that you have set the working directory that way. So if someone else (or you, in a few weeks when you’ve forgotten) tried to run your code and it produced an error, there would be nothing in the code to tell them that they needed to change the working directory.\nFor more detail on why you should not set a working directory manually, read this blog post on project-oriented workflow.\n\n\n\nTyping out the dir.create() function four times is somewhat tedious, and certainly would be if we needed to create (say) 10 directories rather than four. One way to make this code shorter is to use the walk() function from the purrr package, which is part of the tidyverse suite of packages. walk() allows us to run any function several times, each time using a different input, one at a time. The function is called walk() because it ‘walks’ through all the items in a vector of inputs, applying the same function to each of them in turn.\nThe general format of calls to the walk() function is:\nwalk(vector_of_inputs, function_name_to_apply_to_those_inputs)\nSo if we wanted to create the same four directories using walk(), we could:\n\npurrr::walk(c(\"source_data\", \"analysis_data\", \"code\", \"outputs\"), dir.create)\n\nNote that the function name dir.create is not followed by a pair of parentheses when it is used inside walk().\nwalk() is a very powerful function and this is only a very simple example of what it can do, but we will return to it when we need it again.\n\n\nWhy didn’t we use a loop instead of walk()?\n\n\nIf you have learned any other programming languages, you might remember that the usual way to apply the same function to a series of values is to use a ‘loop’, which (depending on the language) might be called a ‘for loop’ or a ‘foreach loop’. While you can create ‘for’ loops in R that would do the same thing as the walk() function, it is generally better not to use ‘for’ loops in R. This is both because loops are often slower – which can be particularly important when analysing large spatial datasets – and because functions like walk() can easily be used inside pipelines constructed with the |&gt; operator.\nIf you’re ever tempted to use a ‘for’ loop in R, stop and think about how to achieve the same result using a function like walk().",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Don't get lost in data analysis</span>"
    ]
  },
  {
    "objectID": "08_projects/index.html#naming-files",
    "href": "08_projects/index.html#naming-files",
    "title": "\n8  Don’t get lost in data analysis\n",
    "section": "\n8.4 Naming files",
    "text": "8.4 Naming files\nThe final step in organising your work so that you can analyse data and stay sane is to give every file you create a name that will help you find it later on. To do this, stick to three principles:\n\nmake your file names easily readable by computers,\nmake your file names easily readable by humans, and\nuse file names that will by-default by sorted in a meaningful way.\n\nComputers can increasingly read file names that contain any characters you can think of, but many different characters make referencing your file names in your code more difficult than need be. To keep your code simple, stick to files names that contain only lower-case letters (so you don’t have to remember if each letter is upper-case or lower-case), numbers, hyphens (-) and underscores (_) – that’s it. If your language uses letters with accents, its best to not use those accents in file names. So, instead of having files with names like this:\nJohn's raw data.csv\nfigure 4.jpg\nDüsseldorfCrimeData.xlsx\nuse the names:\njohn_raw_data.csv\nfigure_4.jpg\ndusseldorf_crime_data.xlsx\n\nTo make your files easily readable by humans, make sure the file name describes what the file contains or does. So giving a file the name crime_data_atlanta_2020.csv is much more useful than calling the same file data_file_23.csv. If you’ve been using computers for a while (or you’ve been taught by someone who has), you may have been trained to use very short file names because old computers could only handle very short names. Fortunately, modern computers can handle much longer file names, so it’s generally safe to use a few extra characters to describe what your file is or does. File names are also easier to read if words are separated by a hyphen or underscore: police_recorded_robbery_data_2020 is much easier to read than policerecordedrobberydata2020.\nIf your files have a meaningful order, it’s useful to reflect this in the file name. Imagine you have three code files: one for data wrangling, one for carrying out spatial analysis tasks on that wrangled data and a third one for creating charts to show the results of the analysis. You cannot run the analysis code until the data wrangling is complete, so the files have a meaningful order.\nWhen you look at these files in the Windows File Explorer, Mac Finder or RStudio Files tab, they are likely to appear out of order. For example, they might be:\nanalyse_data.R\ncreate_charts.R\nwrangle_data.R\nIf you add a number to the start of the file name to show which order to use the files in, things become clearer:\n01_wrangle_data.R\n02_analyse_data.R\n03_create_charts.R\nNote that the numbers are padded with a zero so that they will still be sorted in the correct order if you end up with more than 10 files in my workflow (I’ve only used one zero for padding because I don’t expect to have more than 100 files).\n\n‘File Organization: Naming’ by Data Carpentry licensed under the Creative Commons Attribution licence. XKCD.com comic ‘Documents’ licensed under the Creative Commons Attribution-NonCommercial license.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Don't get lost in data analysis</span>"
    ]
  },
  {
    "objectID": "08_projects/index.html#in-summary",
    "href": "08_projects/index.html#in-summary",
    "title": "\n8  Don’t get lost in data analysis\n",
    "section": "\n8.5 In summary",
    "text": "8.5 In summary\n\nIn this tutorial we’ve learned how to stay organised while mapping crime. Following the rules outlined here might seem troublesome or unnecessary now, but they’ll make things much easier when you’re up against a deadline. The best thing to do is get into good habits when you start coding, so you can make use of them as you develop your coding skills further.\n\n\n8.5.0.1 Check your understanding\n\nquestion(\n  \"Which of these file names is most likely to make it easier for you to re-run your analysis in future if you need to?\",\n  answer(\"`01_wrangle_data.R`, `02_analyse_data.R`, `03_map_data.R`\", correct = TRUE),\n  answer(\"`wrangle_data.R`, `analyse_data.R`, `map_data.R`\", message = \"These file names aren't bad because they are easily readable by both computers and humans, but they are likely to appear in the wrong order in any listing of files. Since these files appear to have a logical order, it would be better to add numbers (with zeros to pad them if necessary) so that they are listed in the order they should be run in.\"),\n  answer(\"`1. Wrangle data.R`, `2. Analyse data.R`, `3. Map data.R`\", message = \"Having characters such as spaces and periods in your file names can create problems when you try to refer to those files in your code. It is better to only use lower-case letters, numbers, hyphens and underscores in your file names.\"),\n  answer(\"`file1.R`, `file2.R`, `file3.R`\", message = \"Using generic file names like this makes it impossible to know what a file does before you open it, so it becomes quite difficult if you need to find a file that does a particular thing. Better file names would make it clear what the code in the file does.\"),\n  correct = random_praise(),\n  incorrect = random_encouragement(),\n  allow_retry = TRUE,\n  random_answer_order = TRUE\n)\n\nIf you’d like more information about these topics, you might be interested in:\n\n\nChapter 8: Workflow projects in R for Data Science introduces RStudio projects and gives more detail about the benefits of using them.\n\nNaming Things by Jenny Bryan talks through good practice for naming files in more detail.\n\nProjects in RStudio is a 40-minute webinar given by Garrett Grolemund that explains how to make the best of RStudio Projects.\n\n\nArtwork by @allison_horst",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Don't get lost in data analysis</span>"
    ]
  },
  {
    "objectID": "09_mapping_areas/index.html#introduction",
    "href": "09_mapping_areas/index.html#introduction",
    "title": "\n9  Mapping area data\n",
    "section": "\n9.1 Introduction",
    "text": "9.1 Introduction\nIn previous tutorials we have produced maps based on the locations of individual crimes, sometimes known as point data because we know the point in space at which each crime occurred. But sometimes we will want to instead map counts of crimes for different areas. There are different reasons why we might want to do this:\n\nWe might want to compare the number of crimes in different areas, such as police districts. We might want to do this to decide which district to allocate extra funds to.\nWe might want to estimate the relative risk of a crime occurring in different places by calculating crime rates using counts of crimes and population data for local areas.\nWe might only have access to counts of crimes for different areas, rather than the location of each crime, perhaps in order to protect the privacy of crime victims.\n\nIn all these cases we need to make maps of crimes for different areas, rather than showing the locations of individual crimes or the density of crimes generated from datasets of individual crime locations.\nMaps that show data for areas are called thematic maps or choropleth maps.\n\n\n\nDo not aggregate point-level crime data to counts of crimes for areas unless you have a good reason to do so.\nChoropleth maps have several shortcomings that we will look at in this tutorial. If you have data on the locations of individual crimes then you should typically present those either as individual points (if there are only a few crimes) or using a kernel density map. If you have point-level crime data, you should only use a choropleth map if you need to compare administrative areas or because you want to calculate crime risk and you only have population data for areas.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Mapping area data</span>"
    ]
  },
  {
    "objectID": "09_mapping_areas/index.html#counting-crimes",
    "href": "09_mapping_areas/index.html#counting-crimes",
    "title": "\n9  Mapping area data\n",
    "section": "\n9.2 Counting crimes",
    "text": "9.2 Counting crimes\nSometimes we will want to know how many crimes have occurred in different formal areas within a city. For example, we might want to know how many crimes of a particular type had occurred in each neighbourhood in a city as part of a review of the performance of different police teams, or to decide which neighbourhoods should be given more crime-prevention funding.\nTo calculate counts of crimes for areas, we need\n\na dataset representing the crime locations and\na dataset representing the boundaries of the areas that we want to calculate counts for.\n\nIn this tutorial, we will count how many car-jacking offences occurred in each alcaldía (borough) of Mexico City in 2019. The result of this will be an SF object containing the outlines of each borough boundary and the count of car-jacking offences in each borough:\n\nborough_counts |&gt; \n  head() |&gt; \n  knitr::kable()\n\n\n\n\n\n\n\n\n\nmunicip\nnomgeo\ngeom\noffences\n\n\n\n9\nMilpa Alta\nPOLYGON ((-98.99723 19.2273…\n13\n\n\n14\nBenito Juárez\nPOLYGON ((-99.13679 19.4015…\n140\n\n\n5\nGustavo A. Madero\nPOLYGON ((-99.1186 19.58446…\n545\n\n\n3\nCoyoacán\nPOLYGON ((-99.13923 19.3568…\n152\n\n\n16\nMiguel Hidalgo\nPOLYGON ((-99.19058 19.4679…\n152\n\n\n8\nLa Magdalena Contreras\nPOLYGON ((-99.20859 19.3361…\n21\n\n\n\n\n\nTo get started, watch this video walk-through of the code we need to write to produce this dataset:\n\n\n9.2.1 Load the data\nFirst, we load the car-jacking data from the URL https://mpjashby.github.io/crimemappingdata/cdmx_car_jacking.gpkg – write the R code needed to load this file into an R object called cdmx_car_jacking. Remember to load any packages that you need first.\n\nlibrary(sf)\nlibrary(tidyverse)\n\ncdmx_car_jacking &lt;- read_sf(\"https://mpjashby.github.io/crimemappingdata/cdmx_car_jacking.gpkg\") \n\nhead(cdmx_car_jacking)\n\nSimple feature collection with 6 features and 3 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: -99.22413 ymin: 19.29605 xmax: -99.04177 ymax: 19.45565\nGeodetic CRS:  WGS 84\n# A tibble: 6 × 4\n  fecha_hechos        longitud latitud                 geom\n  &lt;dttm&gt;                 &lt;dbl&gt;   &lt;dbl&gt;          &lt;POINT [°]&gt;\n1 2019-05-17 17:00:00    -99.1    19.4  (-99.1088 19.39638)\n2 2019-05-17 23:50:00    -99.2    19.3 (-99.22413 19.29605)\n3 2019-05-09 22:00:00    -99.2    19.4 (-99.19087 19.40981)\n4 2019-05-17 17:00:00    -99.1    19.5 (-99.14227 19.45565)\n5 2019-05-17 21:30:00    -99.0    19.4 (-99.04177 19.38836)\n6 2019-05-18 09:00:00    -99.1    19.3 (-99.10008 19.32609)\n\n\nWe also need to load the borough boundaries as an SF object from the URL https://mpjashby.github.io/crimemappingdata/cdmx_alcaldias.gpkg and store it in an object called cdmx_alcaldias. Write the R code needed to do this. Use the notes you have from previous tutorials to help you if you need them, or go back to the tutorial called ‘Giving a map context’ to refresh your memory.\n\ncdmx_alcaldias &lt;- read_sf(\"https://mpjashby.github.io/crimemappingdata/cdmx_alcaldias.gpkg\")\n\n\n9.2.2 Count crimes in each borough\nNow that we have loaded our data, we can count the number of offences in each borough and produce an R object that contains the boundaries of the boroughs together with the number of offences in each one. To do this, we will:\n\nIdentify which borough each crime is in using the st_join() function from the sf package.\nCount the number of crimes in each borough using the count() function from the dplyr package.\nJoin the counts of crimes to the borough boundaries using the left_join() function from the dplyr package.\n\nFor this to work, the object containing the polygons must include a column that can act as a unique identifier for each row (i.e. a column where no value appears more than once). In the cdmx_alcaldias object, there are two columns where all the values are unique: municip and namgeo. If there wasn’t a column of unique values, we could create one using mutate() together with the row_number() function (both from the dplyr package):\ncdmx_alcaldias &lt;- mutate(cdmx_alcaldias, unique_id = row_number())\nSince we already have a column of unique values, we can move straight to using the st_join() function. st_join() comes from the sf package and needs two SF objects, one containing points and one containing polygons representing areas. st_join() takes all the columns from the polygons object and adds those values to the points based on which points are inside each polygon. When we use st_join() to join the cdmx_car_jacking object (containing points) and the cdmx_alcaldias object (containing polygons), st_join() takes cdmx_car_jacking, works out which polygon each point falls inside, then adds columns to the data associated with each point based on that polygon.\n\ncar_jackings_in_alcaldias &lt;- st_join(cdmx_car_jacking, cdmx_alcaldias)\n\nhead(car_jackings_in_alcaldias)\n\nSimple feature collection with 6 features and 5 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: -99.22413 ymin: 19.29605 xmax: -99.04177 ymax: 19.45565\nGeodetic CRS:  WGS 84\n# A tibble: 6 × 6\n  fecha_hechos        longitud latitud                 geom municip nomgeo      \n  &lt;dttm&gt;                 &lt;dbl&gt;   &lt;dbl&gt;          &lt;POINT [°]&gt;   &lt;int&gt; &lt;chr&gt;       \n1 2019-05-17 17:00:00    -99.1    19.4  (-99.1088 19.39638)       6 Iztacalco   \n2 2019-05-17 23:50:00    -99.2    19.3 (-99.22413 19.29605)      12 Tlalpan     \n3 2019-05-09 22:00:00    -99.2    19.4 (-99.19087 19.40981)      16 Miguel Hida…\n4 2019-05-17 17:00:00    -99.1    19.5 (-99.14227 19.45565)      15 Cuauhtémoc  \n5 2019-05-17 21:30:00    -99.0    19.4 (-99.04177 19.38836)       7 Iztapalapa  \n6 2019-05-18 09:00:00    -99.1    19.3 (-99.10008 19.32609)       7 Iztapalapa  \n\n\nAs well as producing a joined dataset as expected, this code has produced a message noting that st_join() assumes that co-ordinates are planar even though they are specified as latitudes and longitudes. This message is there to remind us that st_join() uses certain assumptions about the shape of the earth that might make identifying which borough each offence occurred in incorrect in some circumstances. This is largely a problem for maps covering very-large areas or places near the North or South poles. To be on the safe side, we can first transform both the spatial layers into a local co-ordinate system that is appropriate for Mexico City.\nTo identify a suitable co-ordinate system for this data, we can use the suggest_crs() function from the crsuggest package. suggest_crs() takes an SF object that has co-ordinates specified using longitudes and latitudes, and suggests local co-ordinate systems that are suitable for that part of the world.\n\ncrsuggest::suggest_crs(cdmx_car_jacking)\n\n# A tibble: 10 × 6\n   crs_code crs_name                        crs_type crs_gcs crs_units crs_proj4\n   &lt;chr&gt;    &lt;chr&gt;                           &lt;chr&gt;      &lt;dbl&gt; &lt;chr&gt;     &lt;chr&gt;    \n 1 31975    SIRGAS 2000 / UTM zone 21N      project…    4674 m         +proj=ut…\n 2 8036     WGS 84 / TM Zone 21N (ftUS)     project…    4326 us-ft     +proj=tm…\n 3 2203     REGVEN / UTM zone 20N           project…    4189 m         +proj=ut…\n 4 31989    SIRGAS 1995 / UTM zone 20N      project…    4170 m         +proj=ut…\n 5 31974    SIRGAS 2000 / UTM zone 20N      project…    4674 m         +proj=ut…\n 6 8035     WGS 84 / TM Zone 20N (ftUS)     project…    4326 us-ft     +proj=tm…\n 7 2003     Grenada 1953 / British West In… project…    4603 m         +proj=tm…\n 8 31988    SIRGAS 1995 / UTM zone 19N      project…    4170 m         +proj=ut…\n 9 31973    SIRGAS 2000 / UTM zone 19N      project…    4674 m         +proj=ut…\n10 2202     REGVEN / UTM zone 19N           project…    4189 m         +proj=ut…\n\n\nWe can see that sugggest_crs() has suggested 10 co-ordinate systems that are suitable for the part of the world covered by the cdmx_car_jacking dataset. If you look at the crs_units column, you will see that some of those co-ordinate systems use metres (m) and some use feet and inches (us-ft). It is probably easiest to use co-ordinate systems specified in metres. In this case, the first row has a co-ordinate system that uses metres, so we will use that system. You can see that the crs_code (31975) column gives us an EPSG code we can use together with st_transform() (i.e. in the form “EPSG:31975”).\nWrite code that uses the st_transform() function to transform both layers, st_join() to join them and head() to display the first few rows of the result. The result of st_join() should be stored in an object called car_jackings_in_alcaldias.\n\ncdmx_car_jacking &lt;- st_transform(cdmx_car_jacking, \"EPSG:31975\")\ncdmx_alcaldias &lt;- st_transform(cdmx_alcaldias, \"EPSG:31975\")\n\ncar_jackings_in_alcaldias &lt;- st_join(cdmx_car_jacking, cdmx_alcaldias)\n\nhead(car_jackings_in_alcaldias)\n\nSimple feature collection with 6 features and 5 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: -4276576 ymin: 2803532 xmax: -4249036 ymax: 2822940\nProjected CRS: SIRGAS 2000 / UTM zone 21N\n# A tibble: 6 × 6\n  fecha_hechos        longitud latitud               geom municip nomgeo        \n  &lt;dttm&gt;                 &lt;dbl&gt;   &lt;dbl&gt;        &lt;POINT [m]&gt;   &lt;int&gt; &lt;chr&gt;         \n1 2019-05-17 17:00:00    -99.1    19.4 (-4257417 2813506)       6 Iztacalco     \n2 2019-05-17 23:50:00    -99.2    19.3 (-4276576 2804269)      12 Tlalpan       \n3 2019-05-09 22:00:00    -99.2    19.4 (-4267535 2818569)      16 Miguel Hidalgo\n4 2019-05-17 17:00:00    -99.1    19.5 (-4259315 2822940)      15 Cuauhtémoc    \n5 2019-05-17 21:30:00    -99.0    19.4 (-4249036 2809786)       7 Iztapalapa    \n6 2019-05-18 09:00:00    -99.1    19.3 (-4259181 2803532)       7 Iztapalapa    \n\n\nWe can see that this time, no messages are produced because the inputs to st_join() do not use longitude and latitude co-ordinates.\nIf you look at the car_jackings_in_alcaldias object, you will see that there are columns called municip and nomgeo that show which row in the cdmx_alcaldias object covers the location of each offence from the cdmx_car_jacking object. We can use either of these columns to count the number of car jackings in each borough using the count() function from the dplyr package. Running count() on SF objects can sometimes be slow, so before we do this we will convert car_jackings_in_alcaldias to a tibble using the st_drop_geometry() function.\n\ncar_jacking_counts &lt;- car_jackings_in_alcaldias |&gt; \n  st_drop_geometry() |&gt; \n  # Call the column that holds the murder counts `offences`\n  count(nomgeo, name = \"offences\")\n\nhead(car_jacking_counts)\n\n# A tibble: 6 × 2\n  nomgeo                offences\n  &lt;chr&gt;                    &lt;int&gt;\n1 Azcapotzalco               210\n2 Benito Juárez              140\n3 Coyoacán                   152\n4 Cuajimalpa De Morelos       10\n5 Cuauhtémoc                  88\n6 Gustavo A. Madero          545\n\n\nThe object car_jacking_counts has two columns: one showing the name of each borough and one showing the number of car jackings occurring there. Note that this object contains only those boroughs in which at least one car jacking occurred, since we used count() to count car jackings, not boroughs.\nTo create a dataset containing the boundary of each borough and the relevant offence count, we will join car_jacking_counts object to the original cdmx_alcaldias object using the left_join() function.\nleft_join() is one of a family of joining functions in the dplyr package that join two objects together based on the values of particular columns. If we had two datasets, x and y:\n\n\nwe could use left_join(x, y) to merge them into one combined dataset. This function has left in the name because it joins the two datasets by adding matching rows from the right-hand dataset (y) to each row in the left-hand dataset (x). This means the combined dataset will include all the rows from x, but only the rows from y that match rows in x. Rows in y that do not match any rows in x will be discarded.\n\n\nIn our case, we want to join the cdmx_alcaldias and car_jacking_counts objects so that the combined dataset includes exactly one row for each borough in cdmx_alcaldias, even if no car jackings occurred in that borough (which will mean that borough is not present in the car_jacking_counts dataset). Since we want to keep all the rows from cdmx_alcaldias, we use that object as the first argument to left_join(), i.e.  left_join(cdmx_alcaldias, car_jacking_counts). If there are any rows in the right-hand object car_jacking_counts that do not match any rows in the left-hand object cdmx_alcaldias, those rows will be discarded from the result.\nBy default, left_join() will match the rows in the cdmx_alcaldias and car_jacking_counts objects using all the column names that are present in both datasets. This can sometimes have unexpected results, so it is safer to specify which columns we want to match to be based on using the by argument to left_join(), in this case to specify by = \"nomgeo\" since that is the name of the column that represents the borough names in both cdmx_alcaldias and car_jacking_counts.\n\nSince any boroughs with no car jackings will not be present in the car_jacking_counts dataset, those boroughs will have missing values (i.e.  NA) in place of offence counts in the result produced by left_join(). For that reason, when you use left_join() it’s a good idea to clean up the resulting object using the replace_na() function from the tidyr package (another part of the tidyverse).\nreplace_na() needs one argument, which must be a list created with the list() function. The list() function should have one argument for each column in the data for which we want to replace missing values with the values that we specify. In this case, we want to replace any missing values in the offences column (which contains the counts of car jackings) with the number zero, since any missing values produced by left_join() are the result of those boroughs not having any car jackings and so not being included in the car_jacking_counts dataset.\nTying all this together, we can create a new SF object called borough_counts that contains both the outlines and the count of car jackings in each borough.\n\nborough_counts &lt;- cdmx_alcaldias |&gt; \n  left_join(car_jacking_counts, by = \"nomgeo\") |&gt; \n  # Replace missing (NA) values in the `offences` column with the value 0\n  replace_na(list(offences = 0))\n\nhead(borough_counts)\n\nSimple feature collection with 6 features and 3 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: -4292001 ymin: 2763846 xmax: -4246392 ymax: 2840978\nProjected CRS: SIRGAS 2000 / UTM zone 21N\n# A tibble: 6 × 4\n  municip nomgeo                                                   geom offences\n    &lt;int&gt; &lt;chr&gt;                                           &lt;POLYGON [m]&gt;    &lt;int&gt;\n1       9 Milpa Alta             ((-4249843 2785992, -4249876 2785983,…       13\n2      14 Benito Juárez          ((-4260843 2815313, -4260855 2815285,…      140\n3       5 Gustavo A. Madero      ((-4250891 2839641, -4250798 2839162,…      545\n4       3 Coyoacán               ((-4263007 2809283, -4262814 2809210,…      152\n5      16 Miguel Hidalgo         ((-4265084 2826525, -4265177 2826255,…      152\n6       8 La Magdalena Contreras ((-4272894 2809155, -4272910 2809142,…       21\n\n\nWe now have an SF object containing the boundary of each borough in Mexico City and the number of car jackings that occurred there. We could use this object to make a choropleth map of car jackings, but before we do this we need to understand some of the limitations of this sort of map.\nWe can now put together all the code needed to take the locations of car jackings in Mexico City and produce counts of offences at borough level. When we run all this code together, we can make better use of the pipe operator to create fewer unnecessary intermediate objects as we go.\n\nlibrary(sf)\nlibrary(tidyverse)\n\n# Load the data and transform to an appropriate co-ordinate system\ncdmx_car_jacking &lt;- read_sf(\"https://mpjashby.github.io/crimemappingdata/cdmx_car_jacking.gpkg\")\ncdmx_alcaldias &lt;- read_sf(\"https://mpjashby.github.io/crimemappingdata/cdmx_alcaldias.gpkg\")\n\n# Calculate counts of car jackings\ncar_jacking_counts &lt;- cdmx_car_jacking |&gt; \n  # Join the borough names to the offence locations\n  st_join(cdmx_alcaldias) |&gt; \n  # Convert the SF object to a tibble so that `count()` runs faster\n  st_drop_geometry()  |&gt;  \n  # Call the column that holds the murder counts `offences`\n  count(nomgeo, name = \"offences\")\n\n# Join offence counts back to borough boundaries\nborough_counts &lt;- cdmx_alcaldias |&gt; \n  left_join(car_jacking_counts, by = \"nomgeo\") |&gt; \n  # Replace missing (NA) values in the `offences` column with the value 0\n  replace_na(list(offences = 0))\n\nhead(borough_counts)\n\nSimple feature collection with 6 features and 3 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: -99.32331 ymin: 19.04824 xmax: -98.95239 ymax: 19.59276\nGeodetic CRS:  WGS 84\n# A tibble: 6 × 4\n  municip nomgeo                                                   geom offences\n    &lt;int&gt; &lt;chr&gt;                                           &lt;POLYGON [°]&gt;    &lt;int&gt;\n1       9 Milpa Alta             ((-98.99723 19.22731, -98.99745 19.22…       13\n2      14 Benito Juárez          ((-99.13679 19.40156, -99.13682 19.40…      140\n3       5 Gustavo A. Madero      ((-99.1186 19.58446, -99.11691 19.581…      545\n4       3 Coyoacán               ((-99.13923 19.35687, -99.13772 19.35…      152\n5      16 Miguel Hidalgo         ((-99.19058 19.46794, -99.19066 19.46…      152\n6       8 La Magdalena Contreras ((-99.20859 19.3361, -99.20867 19.335…       21\n\n\n\nSometimes the difference between left_join() and st_join() can be confusing. Remember:\n\n\nst_join() is used to join datasets based on matching spatial locations.\n\nleft_join() is used to join datasets based on matching values in one or more columns in the data.\n\n\n\nTidy Animated Verbs licensed under the Creative Commons Zero licence.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Mapping area data</span>"
    ]
  },
  {
    "objectID": "09_mapping_areas/index.html#mapping-areas",
    "href": "09_mapping_areas/index.html#mapping-areas",
    "title": "\n9  Mapping area data\n",
    "section": "\n9.3 Mapping areas",
    "text": "9.3 Mapping areas\nMaps showing counts of crimes, or a wide range of other data, for different areas are extremely common in all types of spatial analysis. Watch this video to find out introduce yourself to some of the issues you should be aware of when mapping areas.\n\nBecause so much of the data that we might use in crime mapping is only available for specific areas, we often have no choice as to what types of area we use to produce our maps. But whether we choose the areas or not, we have to be mindful that data for areas can only tell us about the area as a whole, not individual people or places within that area. Assuming information about areas applies to individual people or places within them is known as the ecological fallacy, a type of logical error that can be dangerous when we try to draw conclusions from choropleth maps. Watch this video to find out more about the ecological fallacy.\n\nThe ecological fallacy is not a problem that we can solve – it is inherent to choropleth maps. For this reason, whenever we plan to show some spatial data using a choropleth map, it is worth stopping and thinking about whether another type of map (such as a density map) might show the data more effectively and without the problems that choropleth maps have.\nNevertheless, in some circumstances a choropleth map will be the best choice either because we only have data for pre-defined areas or because we are specifically interested in comparing areas (e.g. in working out if there is more crime in one neighbourhood or police district than another).",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Mapping area data</span>"
    ]
  },
  {
    "objectID": "09_mapping_areas/index.html#making-a-choropleth-map",
    "href": "09_mapping_areas/index.html#making-a-choropleth-map",
    "title": "\n9  Mapping area data\n",
    "section": "\n9.4 Making a choropleth map",
    "text": "9.4 Making a choropleth map\nIn this section we will make a choropleth map of murders in different districts within the state of Uttar Pradesh in northern India in 2014. Uttar Pradesh is the largest state in India, with more than 200 million residents.\n\ndistrict_murders_map &lt;- ggplot(district_murders_pop) +\n  annotation_map_tile(type = \"cartolight\", zoomin = 0, progress = \"none\") +\n  geom_sf(aes(fill = murder), data = district_murders, alpha = 0.75) +\n  scale_fill_distiller(palette = \"Oranges\", direction = 1) +\n  labs(\n    title = \"Murders in districts in Uttar Pradesh, 2014\",\n    fill = \"number of\\nmurders\"\n  ) +\n  theme_void() +\n  theme(panel.border = element_rect(colour = \"black\", fill = NA))\n\nggsave(\n  here::here(\"inst/tutorials/09_mapping_areas/images/uttar_pradesh_murders.jpg\"), \n  district_murders_map,\n  width = 800 / 150, \n  height = 500 / 150,\n  dpi = 150\n)\n\n\n\n\n9.4.1 Loading and joining datasets\nFirst, we load the counts of murders in each district and the district boundaries as separate objects.\n\nlibrary(sf)\nlibrary(tidyverse)\n\nmurders &lt;- read_csv(\"https://mpjashby.github.io/crimemappingdata/uttar_pradesh_murders.csv\")\n\nRows: 75 Columns: 2\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): district\ndbl (1): murder\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ndistricts &lt;- read_sf(\"https://mpjashby.github.io/crimemappingdata/uttar_pradesh_districts.gpkg\")\n\nmessage(\"First few rows of the `murders` object …\")\n\nFirst few rows of the `murders` object …\n\nhead(murders, n = 3)\n\n# A tibble: 3 × 2\n  district  murder\n  &lt;chr&gt;      &lt;dbl&gt;\n1 Agra         178\n2 Aligarh      179\n3 Allahabad    132\n\nmessage(\"First few rows of the `districts` object …\")\n\nFirst few rows of the `districts` object …\n\nhead(districts, n = 3)\n\nSimple feature collection with 3 features and 2 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 77.42308 ymin: 26.7417 xmax: 79.79299 ymax: 28.90105\nGeodetic CRS:  WGS 84\n# A tibble: 3 × 3\n  state         district_name                                               geom\n  &lt;chr&gt;         &lt;chr&gt;                                              &lt;POLYGON [°]&gt;\n1 Uttar Pradesh Agra          ((77.6444 27.23444, 77.65405 27.23463, 77.65336 2…\n2 Uttar Pradesh Bareilly      ((78.97474 28.41527, 78.97981 28.41454, 78.98546 …\n3 Uttar Pradesh Etah          ((79.20766 27.56734, 79.20418 27.56314, 79.19801 …\n\n\nNote that we have used the message() function here to make it easier to decipher the output of this code, since we are printing both the murders and districts objects. We have also used the n = 3 argument to the head() function to print just the first three rows of each dataset.\nWe will need to join these two objects using left_join() to create a choropleth map. From the output above we can see that the murders object contains the count of murders in each district and the name of the district in the district column. The districts object also contains the name of each district, but in a column called district_name. To join the two datasets based on this column, we will have to specify that we want to use those differently named columns using the left_join() argument by = c(\"district_name\" = \"district\"). As in the Mexico City example earlier in this tutorial, we will use left_join() and specify the districts object as the first argument in order to keep all the districts in the resulting object, whether there were any murders in the district or not.\n\ndistrict_murders &lt;- left_join(\n  districts, \n  murders, \n  # We can join two datasets based on columns with different names by specifying\n  # `by = c(\"column_name_in_lefthand_data\" = \"column_name_in_righthand_data\")`\n  by = c(\"district_name\" = \"district\")\n)\n\nhead(district_murders)\n\nSimple feature collection with 6 features and 3 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 77.42308 ymin: 24.80419 xmax: 82.35455 ymax: 28.90105\nGeodetic CRS:  WGS 84\n# A tibble: 6 × 4\n  state         district_name                                        geom murder\n  &lt;chr&gt;         &lt;chr&gt;                                       &lt;POLYGON [°]&gt;  &lt;dbl&gt;\n1 Uttar Pradesh Agra          ((77.6444 27.23444, 77.65405 27.23463, 77.…    178\n2 Uttar Pradesh Bareilly      ((78.97474 28.41527, 78.97981 28.41454, 78…    132\n3 Uttar Pradesh Etah          ((79.20766 27.56734, 79.20418 27.56314, 79…     65\n4 Uttar Pradesh Shahjahanpur  ((80.30972 28.46321, 80.29755 28.46668, 80…     88\n5 Uttar Pradesh Pilibhit      ((79.67865 28.84923, 79.67203 28.83937, 79…     54\n6 Uttar Pradesh Allahabad     ((81.54684 25.1848, 81.55587 25.1857, 81.5…    132\n\n\nLooking at the district_murders object, we can see that each row now contains the name of a district and the number of murders that occurred there. There are no districts in which there were zero murders, so we do not need to replace any NA values with zeros using replace_na() as in the previous example.\n\n\n9.4.2 Making a choropleth map\nThe district_murders object we have produced is an SF object, which means we can use geom_sf() to plot the districts.\n\nlibrary(ggspatial)\n\nggplot(district_murders) +\n  annotation_map_tile(type = \"cartolight\", zoomin = 0, progress = \"none\") +\n  geom_sf() +\n  theme_void() +\n  theme(panel.border = element_rect(colour = \"black\", fill = NA))\n\n\n\n\nThere are at least two problems with this map:\n\nwe cannot tell how many murders there were in each district, and\nwe cannot see the base map under the polygons representing the districts.\n\nWe can solve these two problems by changing the aesthetics for the geom_sf() layer in our code. Specifically, we can make the district polygons semi-transparent by setting the alpha argument to geom_sf() to a value of less than one, while also using aes(fill = murder) to specify that the fill colour of the polygons should depend on the number of murders in each district. At the same time we will use scale_fill_distiller() to use a colour scheme that is accessible to the widest range of users.\n\nggplot(district_murders) +\n  annotation_map_tile(type = \"cartolight\", zoomin = 0, progress = \"none\") +\n  # Add the districts layer with colour controlled by the number of murders\n  geom_sf(aes(fill = murder), alpha = 0.75) +\n  # Use a better colour scheme\n  scale_fill_distiller(palette = \"Oranges\", direction = 1) +\n  # Add some labels\n  labs(\n    title = \"Murders in districts in Uttar Pradesh, 2014\",\n    fill = \"number of\\nmurders\"\n  ) +\n  theme_void() +\n  theme(panel.border = element_rect(colour = \"black\", fill = NA))",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Mapping area data</span>"
    ]
  },
  {
    "objectID": "09_mapping_areas/index.html#making-an-interactive-map",
    "href": "09_mapping_areas/index.html#making-an-interactive-map",
    "title": "\n9  Mapping area data\n",
    "section": "\n9.5 Making an interactive map",
    "text": "9.5 Making an interactive map\n\nDue to a bug on some versions of Windows, the R code chunks in this section are not interactive. If you would like to run the R code here, please copy and paste it into the R console or a blank R script (.R) file.\n\nSo far in this course, all the maps we have made using ggplot() have been static images. These are useful for lots of different circumstances, and are usually the only choice if we want to produce a written report as a Word document or PDF. But in some circumstances it may be useful to produce a map that you can interact with by zooming in or panning around different areas. This can be useful when readers might want to look at areas of the map in more detail than is possible without zooming in.\nIn the example of murders in Uttar Pradesh, we might want to use an interactive map to zoom in to see which cities are included in each district on the map. To see how this works, explore this map of murder rates in Uttar Pradesh.\n\n\n\n\n\n\nTo make an interactive choropleth map we can use functions from the leaflet package. The functions in the leaflet package work in a similar way to ggplot(), but with a few differences.\nWatch this video that walks through the code needed to produce an interactive map to get started.\n\nTo make maps with leaflet, we use functions to create a stack in a similar way to the ggplot() stacks we have already created. Note that while the concept of a stack is similar, we cannot use functions from the ggplot2 package in a leaflet stack, or vice versa. Also, we construct leaflet stacks using the pipe operator |&gt; rather than the plus operator + that we use in ggplot2.\nWe can create a very basic leaflet map using the leaflet() function to create the stack, the addProviderTiles() function to add a base map and the addPolygons() function to add the district outlines.\nlibrary(leaflet)\n\nleaflet(district_murders) |&gt; \n  addProviderTiles(\"CartoDB.Voyager\") |&gt; \n  addPolygons()\nIn this map we use the CartoDB.Voyager style of base map, but leaflet can use a large number of different base maps that you can view in this online gallery.\nThis map isn’t very useful because it doesn’t show how many murders there were in each district. To do that, we need to specify that each district should be coloured according to the values in the murder column of the district_murders object. With a ggplot() map, we would do this with a function such as scale_fill_distiller(), but with a leaflet map the code we need is slightly more complicated and has two separate stages.\nThe first stage is to create a custom function that converts the values in the murder column of the district_murders objects into colours. To do this, we use the colorNumeric() function from the leaflet package. Yes, this means that we are using a function to create a function, which we will then later use to set an argument of another function – programming languages are very powerful, but that sometimes means they are complicated.\nThe colorNumeric() function (note the spelling of “color”) allows us to specify the colour scheme we want using the palette argument. We also have to set the domain argument to be NULL. The palette argument to the colorNumeric()function accepts the same values as the palette argument to the scale_fill_distiller() function from ggplot:\n\n\n\n\n\nTo create the custom colour palette function, we use the &lt;- operator to assign it to a name, just as we would with an object. We can give this custom function any name we like, but in the code below we’ll call it colours_red.\nOnce we have created a custom colour palette function using colorNumeric(), we can use that function to create the appropriate values for the fillColor argument of the addPolygons() function in our existing leaflet stack. Since we want the map colours to be controlled by the murder column in the district_murders object, we specify murder as the only argument to the colours_red() palette function we have created.\nOne slight quirk of leaflet is that in order for colours_red() to have access to the columns in the district_murders object, we need to add a tilde (~) operator before the function name, so that the fillColor argument is fillColor = ~ colours_red(murder) rather than fillColor = ~ colours_red(murder). If you forget add do the ~, you will see an error saying:\nError in colours_red(murder) : object 'murder' not found\n# Create a custom colour palette function\ncolours_red &lt;- colorNumeric(palette = \"Reds\", domain = NULL)\n\nleaflet(district_murders) |&gt; \n  addProviderTiles(\"CartoDB.Voyager\") |&gt; \n  addPolygons(\n    fillColor = ~ colours_red(murder),\n    fillOpacity = 0.75,\n    weight = 2,\n    color = \"black\"\n  )\nThere are three improvements we can make to this interactive map. The first is to add a legend, using the addLegend() function. This function needs three arguments:\n\n\npal specifies which custom palette function controls the legend colours. This should be the same function as used in the fillColor argument of the addPolygons() function.\n\nvalues specifies which column in the data the legend will show. Once again, the column name should be preceded by a ~ operator.\n\ntitle specifies the legend title.\n\nleaflet(district_murders) |&gt; \n  addProviderTiles(\"CartoDB.Voyager\") |&gt; \n  addPolygons(\n    fillColor = ~ colours_red(murder),\n    fillOpacity = 0.75,\n    weight = 2,\n    color = \"black\"\n  ) |&gt; \n  addLegend(pal = colours_red, values = ~ murder, title = \"number of murders\")\nNext, we can add an inset map in the corner of the main map. This is useful for interactive maps because if we zoom in to show only a small area, we will be able to use the inset map to stay aware of the wider context.\nWe can do this by adding the addMiniMap() function to the leaflet stack. We will add the argument toggleDisplay = TRUE to add the ability to minimise the inset map if necessary.\nleaflet(district_murders) |&gt; \n  addProviderTiles(\"CartoDB.Voyager\") |&gt; \n  addPolygons(\n    fillColor = ~ colours_red(murder),\n    fillOpacity = 0.75,\n    weight = 2,\n    color = \"black\"\n  ) |&gt; \n  addLegend(\n    pal = colours_red, \n    values = ~ murder, \n    title = \"number of murders\"\n  ) |&gt; \n  addMiniMap(toggleDisplay = TRUE)\nFinally, we can add labels to the districts, which will appear when we move the pointer on a screen over a particular district. We do this by adding the label argument to the existing addPolygons() function. As with the previous sections, we can specify that the labels should be based on the district_name column in the district_murders data using the ~ operator, by setting label = ~ district_name.\nleaflet(district_murders) |&gt; \n  addProviderTiles(\"CartoDB.Voyager\") |&gt; \n  addPolygons(\n    fillColor = ~ colours_red(murder),\n    fillOpacity = 0.75,\n    label = ~ district_name,\n    weight = 2,\n    color = \"black\"\n  ) |&gt; \n  addLegend(\n    pal = colours_red, \n    values = ~ murder, \n    title = \"number of murders\"\n  ) |&gt; \n  addMiniMap(toggleDisplay = TRUE)\nThis map allows you to move around as well as zooming in and out. This is very useful for maps where people may need to see both overall patterns for a large area and details for a smaller area within it. Obviously, interactive maps only work on screens, not for printed maps.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Mapping area data</span>"
    ]
  },
  {
    "objectID": "09_mapping_areas/index.html#calculating-crime-rates",
    "href": "09_mapping_areas/index.html#calculating-crime-rates",
    "title": "\n9  Mapping area data\n",
    "section": "\n9.6 Calculating crime rates",
    "text": "9.6 Calculating crime rates\nOne use of choropleth maps is to combine crime counts with other data. One reason to do this is to calculate a crime rate, i.e. an expression of the frequency of crime that in some way controls for the population that is exposed to crime. We often use choropleth maps for this because we only have population counts for areas, rather than knowing where each individual person lives.\nBefore we learn about different types of crime rate, lets create a targeting table for murder in districts in Uttar Pradesh: a table that shows the districts with the highest number of murders.\n\ndistrict_murders |&gt; \n  # Remove the `geometry` column to make the table clearer\n  st_drop_geometry() |&gt; \n  # Also remove the `state` column, since it is the same for every row\n  select(-state) |&gt; \n  # Arrange the table so the districts with the most murders are first\n  arrange(desc(murder))\n\n# A tibble: 75 × 2\n   district_name murder\n   &lt;chr&gt;          &lt;dbl&gt;\n 1 Meerut           226\n 2 Aligarh          179\n 3 Agra             178\n 4 Bulandshahr      171\n 5 Ghaziabad        166\n 6 Firozabad        140\n 7 Kanpur Nagar     137\n 8 Muzaffarnagar    137\n 9 Bareilly         132\n10 Allahabad        132\n# ℹ 65 more rows\n\n\n\ndistrict_murders_ranked &lt;- district_murders |&gt; \n  st_set_geometry(NULL) |&gt; \n  arrange(desc(murder)) |&gt;  \n  pluck(\"district_name\")\n\nFrom this, we can see that the highest number of murders in 2014 occurred in the Meerut, Aligarh and Agra districts.\n\n9.6.1 Types of crime rate\nThere are three types of crime rate, each of which measures risk in a different way. The most common, and the type of rate that people almost always mean if they talk about a ‘crime rate’ without specifying another type, is the incidence rate. This is the number of crimes in an area divided by the number of people, households or places against which a crime could occur. For example, the incidence rate of burglary in an area might be calculated as the number of crimes per 1,000 households, while the incident rate of homicide might be expressed as the number of homicides per 100,000 people.\n\\[\n\\textrm{incidence} = \\frac{\\textrm{crimes}}{\\textrm{population}}\n\\]\nIncidence rates are useful for comparing the risk from crime in areas of different sizes. For example, knowing that Meerut had more murders in 2014 than any other district in Uttar Pradesh might be useful in deciding where to send more homicide detectives, but it doesn’t tell us much about the risk of being murdered in Meerut because we don’t know if this district has more murders simply because it has more people.\nIncidence rates are useful for comparing areas, but they tell us a little about how likely an individual is to be a victim of crime because crime is heavily concentrated against a few victims (think back to the law of crime concentration we discussed in a previous tutorial). To understand the average risk of a person being a victim of crime once or more, we calculate the prevalence rate. This is the number of people, households, etc. who were victims at least once, divided by the total number of people, households, etc. Prevalence rates are usually expressed as a percentage, so we might say that 1% of the population has been a victim of a particular crime at least once during a year.\n\\[\n\\textrm{prevalence} = \\frac{\\textrm{victims}}{\\textrm{population}}\n\\]\nThe final type of rate is the concentration rate. This tells us how concentrated crime is, and is usually expressed as a single number, e.g. if the concentration rate for a crime is 2.5 then we can say that everyone who was a victim of that crime at least once was on average victimised 2.5 times. This is particularly useful for understanding how important repeat victimisation is to driving up the frequency of crime in an area. This, in turn, might lead us to focus crime-prevention efforts on work to protect recent victims of crime from being victimised again.\n\\[\n\\textrm{concentration} = \\frac{\\textrm{crimes}}{\\textrm{victims}}\n\\]\n\nAll three types of crime rate are average values for an area as a whole, so we are always at risk of committing the ecological fallacy. Remember that while rates are useful for describing areas, that does not imply everyone in an area faces the same risk from crime.\n\n\n9.6.2 Choosing a population measure\nTo calculate a crime rate, we need to be able to measure the population that is at risk from that crime. It is very common for analysts to calculate rates based on the number of people who live in an area, not least because that information is often easily available. However, there are many situations in which the residential population of an area is a poor measure of the population at risk for crime there. For example:\n\nRobberies in a shopping area where most of the victims do not live in the area but instead travel from elsewhere to go shopping. This can lead to vastly inflated crime rates for commercial and entertainment districts that have very small residential populations but very large numbers of people coming into the area to work, shop or visit. Crime rates based on residential population almost always give misleading rates for city centres or business districts.\nAssaults on public transport where victims happen to be passing through a given area at the time they are victimised but are doing so as part of a longer journey through several areas, with the crime potentially occurring in any one of them. These crimes are known as interstitial offences.\nResidential burglaries where the targets of crime are homes, not people, so the crime rate may be influenced by the number of people in each household.\n\nIn all these cases, the residential population is a poor measure of the population at risk from crime. Unfortunately, other measures of population (such as counts of people on public transport or walking along a shopping street) are often expensive or difficult to obtain. This is known as the denominator dilemma: should we use residential population to calculate crime rates just because it is the only data that is available?\n\n9.6.3 Calculating murder rates in Uttar Pradesh\nIn the case of murder in Uttar Pradesh, residential population is likely to be an acceptable (although not perfect) denominator to use in calculating an incidence rate. This is because Indian districts are relatively large areas and so we can expect that most murder victims will be killed in the district where they live (because people everywhere tend to spend most of their time close to where they live). However, there will definitely be exceptions to this, with people being murdered in a different district to where they live. Working out the extent of this problem would be an important question for a detailed investigation into murder in Uttar Pradesh, but we will not consider it any further here.\nTo calculate an incidence rate, we need to join population data to the existing district_murders object containing murder counts and district outlines. Write the code needed to download population data from https://mpjashby.github.io/crimemappingdata/uttar_pradesh_population.csv and join it to the district_murders object, saving the result as district_murders_pop.\n\ndistrict_pop &lt;- read_csv(\"https://mpjashby.github.io/crimemappingdata/uttar_pradesh_population.csv\")\n\nRows: 75 Columns: 7\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (4): code, district, headquarters, division\ndbl (3): population, area, density_km2\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ndistrict_murders_pop &lt;- left_join(\n  district_murders, \n  district_pop, \n  by = c(\"district_name\" = \"district\")\n)\n\nhead(district_murders_pop)\n\nSimple feature collection with 6 features and 9 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 77.42308 ymin: 24.80419 xmax: 82.35455 ymax: 28.90105\nGeodetic CRS:  WGS 84\n# A tibble: 6 × 10\n  state        district_name                      geom murder code  headquarters\n  &lt;chr&gt;        &lt;chr&gt;                     &lt;POLYGON [°]&gt;  &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;       \n1 Uttar Prade… Agra          ((77.6444 27.23444, 77.6…    178 AG    Agra        \n2 Uttar Prade… Bareilly      ((78.97474 28.41527, 78.…    132 BR    Bareilly    \n3 Uttar Prade… Etah          ((79.20766 27.56734, 79.…     65 ET    Etah        \n4 Uttar Prade… Shahjahanpur  ((80.30972 28.46321, 80.…     88 SJ    Shahjahanpur\n5 Uttar Prade… Pilibhit      ((79.67865 28.84923, 79.…     54 PI    Pilibhit    \n6 Uttar Prade… Allahabad     ((81.54684 25.1848, 81.5…    132 AH    Allahabad   \n# ℹ 4 more variables: division &lt;chr&gt;, population &lt;dbl&gt;, area &lt;dbl&gt;,\n#   density_km2 &lt;dbl&gt;\n\n\nNow that we have a single dataset containing all the variables we need, we can calculate the incidence of murders per 100,000 people.\n\ndistrict_murders_pop &lt;- district_murders_pop |&gt; \n  mutate(murder_rate = murder / (population / 100000)) |&gt; \n  # At the same time we will remove unnecessary variables to clean up the data\n  select(district_name, murder, population, murder_rate)\n\nhead(district_murders_pop)\n\nSimple feature collection with 6 features and 4 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 77.42308 ymin: 24.80419 xmax: 82.35455 ymax: 28.90105\nGeodetic CRS:  WGS 84\n# A tibble: 6 × 5\n  district_name murder population murder_rate                               geom\n  &lt;chr&gt;          &lt;dbl&gt;      &lt;dbl&gt;       &lt;dbl&gt;                      &lt;POLYGON [°]&gt;\n1 Agra             178    4418797        4.03 ((77.6444 27.23444, 77.65405 27.2…\n2 Bareilly         132    4448359        2.97 ((78.97474 28.41527, 78.97981 28.…\n3 Etah              65    1774480        3.66 ((79.20766 27.56734, 79.20418 27.…\n4 Shahjahanpur      88    3006538        2.93 ((80.30972 28.46321, 80.29755 28.…\n5 Pilibhit          54    2031007        2.66 ((79.67865 28.84923, 79.67203 28.…\n6 Allahabad        132    5954391        2.22 ((81.54684 25.1848, 81.55587 25.1…\n\n\n\n9.6.4 Mapping crime rates\nTo create a choropleth map of the murder rate using leaflet, we simply use the code from our previous interactive map but specify that the murder_rate column be used to determine the colour of each district polygon rather than the murder column. We will also change the base map to a different style.\n\nWhenever we map a crime rate, it is important that we are explicit about how that rate was calculated. For example, using a legend title such as “rate of murders” would not give enough information to allow readers to understand how to interpret the map. A much better legend title would be “murders per 100,000 residents” or something similar.\n\nWe we need to use a longer legend title, or other text, it can be useful to break the text over multiple lines. Since leaflet maps are built using the same technologies as web pages, the way we wrap text is slightly different to how we do it for maps created with ggplot2. Rather than using the new-line character (\\n) or the str_wrap() function, we will instead use the HTML new-line separator &lt;br&gt; and wrap the whole legend title in the HTML() function from the htmltools package.\n\nleaflet(district_murders_pop) |&gt; \n  addProviderTiles(\n    \"Esri.WorldImagery\", \n    # Since the base map layer we have chosen is very colourful, we will make it\n    # partially transparent to reduce its visual prominence\n    options = providerTileOptions(opacity = 0.3)\n  ) |&gt; \n  # The base map we have chosen does not have any labels, so we will add a\n  # second base map layer over the top. To do this we choose a layer that \n  # consists only of labels.\n  addProviderTiles(\"Stamen.TonerLabels\") |&gt; \n  addPolygons(\n    fillColor = ~ colours_red(murder_rate),\n    fillOpacity = 0.75,\n    label = ~ district_name,\n    weight = 2,\n    color = \"black\"\n  ) |&gt; \n  addLegend(\n    pal = colours_red, \n    values = ~ murder_rate, \n    title = htmltools::HTML(\"murders per&lt;br&gt;100,000 residents\")\n  ) |&gt; \n  addMiniMap(toggleDisplay = TRUE)\n\n\n\n\n\nWe now have all the code we need to load the necessary data, calculate crime rates and make an interactive map of murders in districts in Uttar Pradesh.\n\nlibrary(leaflet)\nlibrary(sf)\nlibrary(tidyverse)\n\n# Load data\nmurders &lt;- read_csv(\"https://mpjashby.github.io/crimemappingdata/uttar_pradesh_murders.csv\")\n\nRows: 75 Columns: 2\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): district\ndbl (1): murder\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ndistricts &lt;- read_sf(\"https://mpjashby.github.io/crimemappingdata/uttar_pradesh_districts.gpkg\")\ndistrict_pop &lt;- read_csv(\"https://mpjashby.github.io/crimemappingdata/uttar_pradesh_population.csv\")\n\nRows: 75 Columns: 7\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (4): code, district, headquarters, division\ndbl (3): population, area, density_km2\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n# Wrangle data\ndistrict_murders_pop &lt;- districts |&gt; \n  # Add murders data to district boundaries\n  left_join(murders, by = c(\"district_name\" = \"district\")) |&gt; \n  # Add population to district boundaries\n  left_join(district_pop, by = c(\"district_name\" = \"district\")) |&gt; \n  # Calculate murder rate\n  mutate(murder_rate = murder / (population / 100000))\n\n# Create a custom colour palette function\ncolours_red &lt;- colorNumeric(palette = \"Reds\", domain = NULL)\n\n# Create interactive map\nleaflet(district_murders_pop) |&gt; \n  addProviderTiles(\n    \"Esri.WorldImagery\", \n    # Since the base map layer we have chosen is very colourful, we will make it\n    # partially transparent to reduce its visual prominence\n    options = providerTileOptions(opacity = 0.3)\n  ) |&gt; \n  # The base map we have chosen does not have any labels, so we will add a\n  # second base map layer over the top. To do this we choose a layer that \n  # consists only of labels.\n  addProviderTiles(\"Stamen.TonerLabels\") |&gt; \n  addPolygons(\n    fillColor = ~ colours_red(murder_rate),\n    fillOpacity = 0.75,\n    label = ~ district_name,\n    weight = 2,\n    color = \"black\"\n  ) |&gt; \n  addLegend(\n    pal = colours_red, \n    values = ~ murder_rate, \n    title = htmltools::HTML(\"murders per&lt;br&gt;100,000 residents\")\n  ) |&gt; \n  addMiniMap(toggleDisplay = TRUE)\n\n\n\n\n\n\n\n9.6.4.1 Check your understanding\nAnswer the following questions to check your understanding of what we’ve learned so far in this tutorial. If you get a question wrong, you can keep trying until you get the right answer.\n\nquiz(\n  caption = \"\",\n  \n  question(\n    \"What type of rate describes the chances of a person being a victim of a crime at least once in a given period?\",\n    answer(\"prevalence rate\", correct = TRUE),\n    answer(\"incidence rate\"),\n    answer(\"concentration rate\"),\n    correct = random_praise(),\n    incorrect = random_encouragement(),\n    allow_retry = TRUE,\n    random_answer_order = TRUE\n  ),\n  \n  question(\n    \"What type of rate describes the frequency of crime after controlling for some measure of the population at risk?\",\n    answer(\"incidence rate\", correct = TRUE),\n    answer(\"prevalence rate\"),\n    answer(\"concentration rate\"),\n    correct = random_praise(),\n    incorrect = random_encouragement(),\n    allow_retry = TRUE,\n    random_answer_order = TRUE\n  ),\n  \n  question(\n    \"What type of rate describes the average number of times each victim of a particular crime is victimised?\",\n    answer(\"concentration rate\", correct = TRUE),\n    answer(\"incidence rate\"),\n    answer(\"prevalence rate\"),\n    correct = random_praise(),\n    incorrect = random_encouragement(),\n    allow_retry = TRUE,\n    random_answer_order = TRUE\n  )\n  \n)",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Mapping area data</span>"
    ]
  },
  {
    "objectID": "09_mapping_areas/index.html#in-summary",
    "href": "09_mapping_areas/index.html#in-summary",
    "title": "\n9  Mapping area data\n",
    "section": "\n9.7 In summary",
    "text": "9.7 In summary\n\nIn this tutorial we have learned how to calculate the number of points in different areas, join different sources of data together, and map crime counts and crime rates on both static and interactive maps.\nWe have also learned about two disadvantages of choropleth maps: the ecological fallacy and the modifiable areal unit problem.\n\n\nTo find out more about the topics covered in this tutorial:\n\nRead the article Crime seen through a cone of resolution by Paul Brantingham, Delmar Dyreson and Patricia Brantingham for more detail about the ecological fallacy in studying crime.\nRead the article Smallest is Better? The Spatial Distribution of Arson and the Modifiable Areal Unit Problem for an example of the modifiable areal unit problem in studying crime.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Mapping area data</span>"
    ]
  },
  {
    "objectID": "appendices/read_functions.html",
    "href": "appendices/read_functions.html",
    "title": "Appendix A — Functions for reading data into R",
    "section": "",
    "text": "R can read data in many different formats. Different functions (often from different packages) are needed to read files of different formats. These are some of the functions needed to read common types of file used in data analysis, including spatial data formats.\n\n\n\n\n\nFunctions for reading data into R\n\n\n\n\n\n\n\n\nData file type\nPackage\nFunction\nCan load compressed files?\nCan load files from URL?\n\n\n\n\nComma-separated values (.csv)\n\n\nreadr\n\n\nread_csv() (data from English-speaking countries) or read_csv2() (data from elsewhere)\n\n\nyes\n\n\nyes\n\n\n\n\nFixed-width files (usually .txt)\n\n\nreadr\n\n\nread_fwf()\n\n\nyes\n\n\nyes\n\n\n\n\nGeoJSON (.geojson)\n\n\nsf\n\n\nread_sf()\n\n\nno\n\n\nyes\n\n\n\n\nGeoPackage (.gpkg)\n\n\nsf\n\n\nread_sf()\n\n\nno\n\n\nyes\n\n\n\n\nGoogle Sheets\n\n\ngooglesheets4\n\n\nread_sheet()\n\n\nn/a\n\n\nyes\n\n\n\n\nHTML (.htm or .html)\n\n\nxml2\n\n\nread_html() (probably used with functions from the rvest package)\n\n\nn/a\n\n\nyes\n\n\n\n\nJSON (.json)\n\n\njsonlite\n\n\nread_json()\n\n\nyes\n\n\nyes\n\n\n\n\nMicrosoft Excel (.xlsx or .xls)\n\n\nreadxl\n\n\nread_excel()\n\n\nno\n\n\nno\n\n\n\n\nOpenDocument Spreadsheet (.ods)\n\n\nreadODS\n\n\nread_ods()\n\n\nno\n\n\nyes\n\n\n\n\nR Data (.rds)\n\n\nreadr\n\n\nread_rds()\n\n\nn/a\n\n\nyes\n\n\n\n\nSAS (.sas7bdat)\n\n\nhaven\n\n\nread_sas()\n\n\nyes\n\n\nyes\n\n\n\n\nShapefile (.shp)\n\n\nsf\n\n\nread_sf()\n\n\nno\n\n\nno\n\n\n\n\nSPSS Statistics (.sav)\n\n\nhaven\n\n\nread_sav()\n\n\nyes\n\n\nyes\n\n\n\n\nStata (.dta)\n\n\nhaven\n\n\nread_dta()\n\n\nyes\n\n\nyes\n\n\n\n\nTab-separated values (.tsv or .tab)\n\n\nreadr\n\n\nread_tsv()\n\n\nyes\n\n\nyes\n\n\n\n\nXML (usually .xml)\n\n\nxml2\n\n\nread_xml()\n\n\nyes\n\n\nyes",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Functions for reading data into R</span>"
    ]
  },
  {
    "objectID": "appendices/common_errors.html",
    "href": "appendices/common_errors.html",
    "title": "Appendix B — Common R errors and how to fix them",
    "section": "",
    "text": "In these error messages, the code blah represents the function, object or value that the error relates to.\n\n\n\n\n\nCommon R errors\n\n\n\n\n\nError message\nHow to fix it\n\n\n\n\nthere is no package called 'blah'\n\n\nYou have either mis-typed the package name or the package is not installed. Check the spelling or use install.packages() to install the package.\n\n\n\n\ncould not find function \"blah\"\n\n\nYou have either mis-typed the function name or the package containing that function is not loaded. Check the spelling or use library() to load the package.\n\n\n\n\nobject 'blah' not found\n\n\nYou have either mis-typed the name of the object or the object does not exist. Check the spelling and make sure you have run the code that creates the object. This error can also occur when you have forgotten to put quotes around a character value in an argument, since R treats words without quotes around them as the names of objects. In this case, check that you have used quote marks around any character values in your code.\n\n\n\n\n'blah' does not exist in current working directory\n\n\nYou have either mis-typed the name of a file that R is trying to access or the file does not exist in the location you have specified. Check the spelling and make sure the file exists.\n\n\n\n\nnon-numeric argument to binary operator\n\n\nYou have tried to use a mathematical operator such as + or - with a non-numeric value. For example, you might have written the code 1 + blah thinking that blah holds a numeric value, but if blah actually holds a character value then trying to add it to 1 makes no sense. Check that any objects in your code have the values you expect them to.\n\n\n\n\nobject of type 'closure' is not subsettable\n\n\nYou have tried to use a function as if it is an object, which can happen when you store data in an object that has the same name as an R function (most commonly, when you store some data in an object called data, since there is a function called data()). The circumstances that produce this error are often not simple to understand, so the best way to handle this error is to avoid it by not naming objects using the names of functions.\n\n\n\n\nno applicable method for 'blah' applied to an object of class \"blah\"\n\n\nSome R functions (called generic functions) work in different ways depending on what type of object you use them on. But if you use a generic function on an object that it does not know how to handle, you will see this error. Check the object(s) that you provided to the function causing the error to make sure it is the type of object you are expecting it to be.\n\n\n\n\nunexpected numeric constant in \"blah\", unexpected string constant in \"blah\" or unexpected symbol in \"blah\"\n\n\nYou have a typo somewhere in your code. Check the line of code producing the error to make sure it is formatted correctly. The most common typos that cause this error are a missing comma or closing parenthesis, but there are several other typos that can cause similar errors.\n\n\n\n\nunused argument (var = \"blah\")\n\n\nYou have used an argument name in a function that does not understand it. Check the manual page for that function.\n\n\n\n\nargument \"blah\" is missing, with no default\n\n\nYou have used a function without providing all the necessary arguments. Check the manual page for that function.\n\n\n\n\nThe pipe operator requires a function call as RHS\n\n\nThere is a pipe operator at the end of the final line of a code pipeline (or what R thinks should be the final line), or you have omitted the parentheses at the end of a function name.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Common R errors and how to fix them</span>"
    ]
  }
]