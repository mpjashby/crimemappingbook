[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Learn Crime Mapping with R",
    "section": "",
    "text": "Welcome!",
    "crumbs": [
      "Welcome!"
    ]
  },
  {
    "objectID": "index.html#who-is-this-book-for",
    "href": "index.html#who-is-this-book-for",
    "title": "Learn Crime Mapping with R",
    "section": "Who is this book for?",
    "text": "Who is this book for?\nThis book is for you if you are:\n\nSomeone who wants to learn how to understand patterns of crime using maps and related forms of data analysis.\nSomeone who already knows how to map crimes and would like to learn how to code in a familiar context.",
    "crumbs": [
      "Welcome!"
    ]
  },
  {
    "objectID": "index.html#why-use-this-book",
    "href": "index.html#why-use-this-book",
    "title": "Learn Crime Mapping with R",
    "section": "Why use this book?",
    "text": "Why use this book?\nThere are several books available for learning crime mapping. The advantages of using this book are that:\n\nIt teaches up-to-date crime mapping techniques. Some of the most-popular books on crime mapping were written over a decade ago and do not reflect substantial developments in the field since then.\nIt teaches crime mapping using exclusively free software (R and RStudio). Some other books teach crime mapping in expensive proprietary software that most people who need to make crime maps do not have access to.\nIt is an online book, so includes videos to introduce theoretical concepts and walk you through the process of creating different maps.\nIt uses examples from across the world, so it is not only useful to readers from one country.\nIt’s completely free to read and redistribute!",
    "crumbs": [
      "Welcome!"
    ]
  },
  {
    "objectID": "index.html#why-learn-crime-mapping-in-r",
    "href": "index.html#why-learn-crime-mapping-in-r",
    "title": "Learn Crime Mapping with R",
    "section": "Why learn crime mapping in R?",
    "text": "Why learn crime mapping in R?\nWe could make crime maps in several different apps. This includes commercial geographic information systems such as ArcGIS or MapInfo, free software such as QGIS, and data analysis programmes such as Tableau. So why learn crime mapping in a programming language like R?\nThere are several reasons:\n\nMaking maps using a programming language makes your work much more efficient, especially if (as is common in crime analysis) you need to produce similar maps periodically using updated data, or need to produce multiple similar maps for different areas or crime types.\nR is free. This can be important for people working in organisations with very limited budgets for software for data analysis.\nR has extensive mapping capabilities, supported by a large team of friendly experts who provide online support.\nR is good for other types of data analysis, so everything you learn here can be used for analysing crime data using techniques that are unrelated to maps.\n\nLearning a programming language like R involves a little extra work at the start, but this book is written to make this as easy as possibly by gently introducing you to programming ideas a little at a time. Once you’ve got started, you’ll find mapping crime in this way makes it much easier to advance further in the future.\nThis book is used for teaching crime mapping to BSc Crime and Security Science students at University College London, so the code here has been tested by hundreds of people learning crime mapping before you.",
    "crumbs": [
      "Welcome!"
    ]
  },
  {
    "objectID": "contents.html",
    "href": "contents.html",
    "title": "Contents",
    "section": "",
    "text": "This book is divided into 17 chapters, each covering a different topic related to crime mapping with R. Some of the chapters focus on mapping skills, while others focus more on how good coding practice can make your life easier. The chapters build on one another, so the easiest way to go through the book is to work through the chapters in order. But if you already know some R coding, and something about mapping, you might want to jump into a specific chapter to get what you need.\nBefore you start, make sure you install the software you need for this book.\n\nChapter 1: Getting Started\n\nGet started with crime mapping by learning about why putting crime on maps is useful, and get a tour of the RStudio software we will use throughout this course.\n\nChapter 2: Your first crime map\n\nQuickly walk (or jog) through the process of creating a complete crime map in R.\n\nChapter 3: Wrangling data\n\nLearn the building blocks of data analysis: loading, filtering, manipulating and saving data for crime mapping.\n\nChapter 4: Your second crime map\n\nWalk through each specific stage in the process of creating a crime map.\n\nChapter 5: Code with style\n\nLearn how to format your code so that it is easier to use, both for you and for others.\n\nChapter 6: Mapping crime patterns\n\nLearn how to use maps to identify patterns of crime in cities.\n\nChapter 7: Giving a map context\n\nLearn how to make your maps easier to understand using supporting elements such as titles and legends.\n\nChapter 8: Handling bugs in your code\n\nLearn how to find and fix problems with the R code that you write.\n\n\nChapters 9 to 16 have not yet been updated for 2025 – check back in mid-February 2025 for updated content.\n\n\n\n\n\n\nImportant things you should know\n\n\n\nThroughout the book you will see boxes marked with an exclamation mark that contain information that it is particularly important for you to know to avoid common mistakes in writing code or making maps. Pay special attention to these points and remember to ask questions if anything isn’t clear.\n\n\n\n\n\n\n\n\nClick on this line of text\n\n\n\n\n\nIn this book you will also see lines of text marked by a light-bulb symbol that you can click on to find out more information about a particular issue. This is generally information that you do not need to know to complete a task, but which might be useful in other circumstances or which might answer some questions that you have. You can skip these boxes if you want to, or come back to them later if you have a question.\n\n\n\n\n\n\n\n\n\nHow to use this book\n\n\n\nFinally, you will see boxes like this one throughout the book that give you the opportunity to check your understanding of a particular section of that chapter. Sometimes this will be in the form of a multiple-choice question, and sometimes you’ll be asked to write some code in the R Console. You will always be able to check if your answers are correct.",
    "crumbs": [
      "Contents"
    ]
  },
  {
    "objectID": "setup.html",
    "href": "setup.html",
    "title": "Install the software needed for this book",
    "section": "",
    "text": "Step 1: install R\nThe first step is to download and install R, a programming language designed for analysing and visualising data, including making maps. To install R, visit the R website and download R for either Windows or Mac, depending on what type of computer you are using. If you already have R installed on your computer, please update it to the latest release.\nThis video talks you through the process of installing R:",
    "crumbs": [
      "Install the software needed for this book"
    ]
  },
  {
    "objectID": "setup.html#step-2-install-rstudio",
    "href": "setup.html#step-2-install-rstudio",
    "title": "Install the software needed for this book",
    "section": "Step 2: install RStudio",
    "text": "Step 2: install RStudio\nThe next step is to download RStudio, an app that you can use to work with the R programming language more efficiently. Download RStudio Desktop for your computer from the Posit website (that’s the company that makes RStudio) and install. If you already have RStudio Desktop installed on your machine, please update it to the latest release.\nThis video talks you through the process of installing RStudio:",
    "crumbs": [
      "Install the software needed for this book"
    ]
  },
  {
    "objectID": "setup.html#step-3-install-rtools-windows-only",
    "href": "setup.html#step-3-install-rtools-windows-only",
    "title": "Install the software needed for this book",
    "section": "Step 3: install RTools (Windows only)",
    "text": "Step 3: install RTools (Windows only)\nIf you are using a Windows computer you should install Rtools, which will be needed by RStudio for some of the code in this book. If you are using a Mac or Linux computer, you do not need to install Rtools.\nTo install RTools:\n\nDownload the latest version from the R website and open the downloaded file.\nFollow the installation instructions (accept all the default options).\nOpen RStudio.\nFind the panel (in the bottom-left) marked Console.\nFind the &gt; symbol at the bottom of that panel.\nCopy and paste the following code to the right of the &gt; symbol:\n\nwrite('PATH=\"${RTOOLS40_HOME}\\\\usr\\\\bin;${PATH}\"', file = \"~/.Renviron\", append = TRUE)\n\nPress Enter.",
    "crumbs": [
      "Install the software needed for this book"
    ]
  },
  {
    "objectID": "setup.html#if-you-cannot-install-software-on-your-computer",
    "href": "setup.html#if-you-cannot-install-software-on-your-computer",
    "title": "Install the software needed for this book",
    "section": "If you cannot install software on your computer",
    "text": "If you cannot install software on your computer\nYou may not be able to install software on the computer you want to use for crime mapping, or you may prefer not to for various reasons. In that case, you can run RStudio online using Posit Cloud. Posit Cloud is free for a certain number of hours each month, after which you can pay to continue using it.\nUsing Posit Cloud allows you to avoid the installation steps for R, RStudio and RTools above, but remember that since Posit Cloud operates online, you should not use it work on confidential or personal data unless you have the necessary permission to do so.",
    "crumbs": [
      "Install the software needed for this book"
    ]
  },
  {
    "objectID": "setup.html#set-up-r-for-crime-mapping",
    "href": "setup.html#set-up-r-for-crime-mapping",
    "title": "Install the software needed for this book",
    "section": "Set up R for crime mapping",
    "text": "Set up R for crime mapping\nAs we will learn in subsequent chapters, most of the mapping capabilities in R are provided by add-on packages. To download and install the packages you will need to run the code included in this book:\n\nOpen RStudio.\nFind the panel (usually in the bottom-left) marked Console.\nFind the &gt; symbol at the bottom of that panel.\nCopy and paste the following code to the right of the &gt; symbol:\n\n\n\n\nR Console\n\nif (!require(\"remotes\")) install.packages(\"remotes\")\nremotes::install_github(\"mpjashby/learncrimemapping\")\n\n\n\nPress Enter or Return on your keyboard.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDo you want to install from sources the package which needs compilation? – what should I do?\n\n\n\n\n\nIf you see a popup message appear asking Do you want to install from sources the package which needs compilation?, you can safely choose ‘No’.\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nIt will take a few minutes for the set-up process to finish. Once the process is complete, you will see the &gt; symbol has appeared in the R Console again.",
    "crumbs": [
      "Install the software needed for this book"
    ]
  },
  {
    "objectID": "01_getting_started/index.html",
    "href": "01_getting_started/index.html",
    "title": "1  Getting started",
    "section": "",
    "text": "1.1 Welcome\nWelcome to Learn Crime Mapping with R. This book will help you learn about using maps and spatial analysis techniques to understand crime. Watch this video to learn more.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Getting started</span>"
    ]
  },
  {
    "objectID": "01_getting_started/index.html#why-put-crimes-on-maps",
    "href": "01_getting_started/index.html#why-put-crimes-on-maps",
    "title": "1  Getting started",
    "section": "1.2 Why put crimes on maps?",
    "text": "1.2 Why put crimes on maps?\nThis course is about how we can use maps and other spatial analysis tools to help understand, prevent and respond to crime. Watch this video to understand why spatial analysis is a useful tool for understanding crime.\n\n\n\n\n\n\n\nConcentration of crime\n\n\n\nWhy is it important to map crime?\n\n To estimate the total number of crimes in a city To understand where different types of crime happen most often To predict the weather in crime-prone areas To determine the average time crimes occur\n\nWhat does the law of crime concentration state?\n\n Most crime is caused by a large number of offenders About half of all crime happens in half of a city’s neighborhoods Most crime occurs in a small minority of places Crime is approximately equally distributed across all streets in a city\n\nAccording to research on the concentration of crime, how much crime typically happens in the 5–8% of streets or addresses with the most crime?\n\n 25% 50% 75% 100%\n\nWhy do researchers often prefer to focus on micro places, such as streets and addresses, rather than neighbourhoods?\n\n Most crime in a high-crime neighborhood occurs at only a few streets or addresses Crime is usually random at the neighborhood level Analyzing neighborhoods is harder It is easier to access data on crime at particular addresses than for neighbourhoods\n\nWhich of the following findings is consistent with the law of crime concentration?\n\n Crimes in Medellin are evenly distributed across the city No specific places in Kaduna experience concentrated burglary rates Crime in Oslo is spread equally across the city About half of crimes in Seattle occur on 5% of streets\n\n\n\n\nWeisburd, D. (2015). The law of crime concentration and the criminology of place. Criminology, 53(2), 133-157.\nJohnson, S. (2010). A brief history of the analysis of crime concentration. European Journal of Applied Mathematics, 21(4-5), 349.\nFarrell, G. (2015). Crime concentration theory. Crime Prevention and Community Safety, 17(4), 233-248.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Getting started</span>"
    ]
  },
  {
    "objectID": "01_getting_started/index.html#why-is-crime-concentrated-in-space",
    "href": "01_getting_started/index.html#why-is-crime-concentrated-in-space",
    "title": "1  Getting started",
    "section": "1.3 Why is crime concentrated in space?",
    "text": "1.3 Why is crime concentrated in space?\nWhy is crime concentrated in space? Watch this video to find out more about how our environment influences opportunities for crime and how that causes clusters of different crimes.\n\n\n\n\n\n\n\nRoutine activities\n\n\n\nWhy are opportunities for crime concentrated in some places more than others?\n\n Crime happens randomly in all places Some places have more environmental features that facilitate specific crimes Police allocate more resources to high-crime areas Criminals prefer remote locations to commit crimes\n\nAccording to the routine activities approach, which of the following is not required for a crime opportunity to occur?\n\n A motivated offender A suitable target A controller who prevents the crime A place where the offender and target can meet\n\nWhat role do controllers play in preventing crime, according to the routine activities approach?\n\n They reduce the likelihood of opportunities for crime occurring, e.g. by making targets or places less vulnerable They deter crimes by arresting offenders They create opportunities for offenders and targets to meet They plan environments to increase crime rates\n\nWhy do different types of crime concentrate in different places?\n\n Criminals always prefer urban areas for all types of crime Police enforcement varies between locations Opportunities for crime are spread evenly across all areas Opportunities for crime are influenced by specific environmental features that vary by crime type\n\nWhat is a target in the context of the routine activities approach?\n\n A location where offenders and controllers meet The person or object that an offender aims to harm, steal, or engage with A police operation to reduce crime in specific areas Any place where crime does not occur\n\n\n\n\nSantos, R. B. (2015). Routine Activity Theory: A Cornerstone of Police Crime Analyst Work. In The Criminal Act: \nCohen, L. E., and Felson, M. (1979). Social Change and Crime Rate Trends: A Routine Activity Approach. American Sociological Review, 44(4), 588–608.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Getting started</span>"
    ]
  },
  {
    "objectID": "01_getting_started/index.html#finding-your-way-around-rstudio",
    "href": "01_getting_started/index.html#finding-your-way-around-rstudio",
    "title": "1  Getting started",
    "section": "1.4 Finding your way around RStudio",
    "text": "1.4 Finding your way around RStudio\nWe will use RStudio for almost all of this course. Watch this video to find your way around the different panels in the RStudio window.\n\n\n1.4.1 Slightly adjusting how RStudio works\nBefore we start using RStudio, we should make a few changes to how it is set up that will make it easier to fix any mistakes we make while coding. To do this, click on the Tools menu in RStudio and then on Global Options…. In the dialogue box that opens, click on General in the left-hand panel if General is not selected already.\n\n\n\nIn the “Workspace” section of the right-hand panel, find an option that says “Restore .RData into workspace at startup” and make sure the check box to the left of that option is not checked. On the next line down, click the drop-down menu labelled “Save workspace to .RData on exit:” and choose the option Never.\nNow click on Code in the left-hand panel. Find the box that is marked “Use native pipe operator, |&gt; (requires R 4.1+)” and make sure it is checked.\nClick Apply and then OK to close the dialogue box.\n\nThe RStudio IDE Cheat Sheet highlights some of the features available in RStudio and gives a list of available keyboard short-cuts.\nWriting Code in RStudio is a webinar that talks you through RStudio in more detail.\n\n\n\n1.4.2 Creating an RStudio project\nRStudio includes a feature called Projects, which make it much easier to manage all the files that you will need to use as part of many data-analysis projects. We will learn more about projects later on, but for now it’s enough to create a single project in which you can store all the code you write while working through this book.\nTo create a new project from within RStudio, click File then New Project … and choose to create the project in a new directory on your computer:\n\n\n\nChoose to create a new (empty) project, rather than use any of the templates for specific types of projects:\n\n\n\nChoose what the new project  directory should be called (probably ‘Crime Mapping’), and where on your computer it should be created, then click Create Project:\n\n\n\nThat’s it – you have created an RStudio project that you can organise your work in.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Getting started</span>"
    ]
  },
  {
    "objectID": "01_getting_started/index.html#in-summary",
    "href": "01_getting_started/index.html#in-summary",
    "title": "1  Getting started",
    "section": "1.5 In summary",
    "text": "1.5 In summary\nNow that you know why crime mapping is useful for understanding crime, why crime is typically concentrated in space and how to find your way around RStudio, in the next chapter we will produce our first crime map in R.\nIf you’re not feeling too confident at this point in the course, don’t worry – learning something new is always a bit of a roller coaster and there is lots of help available in subsequent chapters.\n\n\n\n\nArtwork by @allison_horst",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Getting started</span>"
    ]
  },
  {
    "objectID": "02_your_first_crime_map/index.html",
    "href": "02_your_first_crime_map/index.html",
    "title": "2  Your first crime map",
    "section": "",
    "text": "2.1 Introduction\nIn this chapter we will use R to produce a simple crime map. To do this we will skip over lots of the detail of how R works and what choices we should make in creating maps. We will return to all these details in future sessions, so for-now please don’t worry about understanding every single line of code. Everything will become clear as we work through this book.\nThe map we’re going to create shows the locations of four homicides in downtown Atlanta in 2019:",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Your first crime map</span>"
    ]
  },
  {
    "objectID": "02_your_first_crime_map/index.html#sec-permanent-code",
    "href": "02_your_first_crime_map/index.html#sec-permanent-code",
    "title": "2  Your first crime map",
    "section": "2.2 Permanent and temporary R code",
    "text": "2.2 Permanent and temporary R code\nTo get started, open RStudio. Now click the File menu, then New File and then R Script. A new file will open in the left-hand panel in RStudio. Click File then Save As... to save this file in the project directory you created when you completed the previous chapter. Save the file as chapter_02a.R.\n\n\n\n\n\n\nWhy are we naming the file chapter_02a.R?\n\n\n\n\n\nIn each chapter of this book, you will create one or more R script files to store the code needed to produce a map or complete some other task. We will create two separate code files in this chapter, so we will use letters after the chapter number of distinguish between the different files.\n\n\n\nEach chapter in this book includes chunks of R code that you can run in RStudio. We can think of this code as falling into two categories. Permanent code is code that we need to run to complete a piece of data analysis. For example, in the next section we will see some permanent code that loads some crime data. We type permanent code in the R script file that we have just created. By the end of this chapter, the script file we have just created will contain all the code needed to make a basic crime map.\nAs well as permanent code, we sometimes also need to write temporary code. This is code that we don’t need to complete an analytical task, but we do need to write the permanent code that completes a task. For example, we might need to write a piece of code that shows us the name of each column in a dataset, so we can refer to columns by name in our script file. Once we know the names of the columns, we don’t need to code we wrote to find out the column names, which is why we refer to it as temporary code.\nTemporary code is not written in our R script file, because including everything in a script file would make it much more complicated and harder to keep track of. Instead, we write temporary code in the R Console.\nTo help keep track of which code in this book is permanent code and which is temporary code, each chunk of code will either be labelled with the name of the R script file you should add it to, or the word ‘Console’ to indicate that it is temporary code you should type into the R Console.\n\n\n\n\n\n\nPermanent and temporary code\n\n\n\nWhat is the difference between permanent and temporary code?\n\n Permanent code is stored in a script file, while temporary code is used only in the Console. Permanent code is run manually, while temporary code runs automatically. Temporary code is saved, while permanent code is discarded. Permanent code produces errors, while temporary code avoids them.\n\nWhy are temporary code written in the R Console instead of a script file?\n\n Temporary code requires real-time user input. RStudio is not capable of storing temporary code in R script files. To keep our R script files as simple as possible Temporary code must run on a separate server.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Your first crime map</span>"
    ]
  },
  {
    "objectID": "02_your_first_crime_map/index.html#loading-crime-data",
    "href": "02_your_first_crime_map/index.html#loading-crime-data",
    "title": "2  Your first crime map",
    "section": "2.3 Loading crime data",
    "text": "2.3 Loading crime data\n\n2.3.1 Loading packages\nBefore we can work with our data, we first load packages of functions for use in the analysis. For example, we will load the tidyverse package, which automatically loads several packages that are useful for data wrangling and analysis.\nCopy the code below and paste it into the blank file chapter_02a.R that you created earlier. This adds these lines of code to your script, but you need to run that code for it to do anything. To run these three lines of code, you should highlight the lines of code you want to run and then either:\n\nclick the Run button at the top of the R script panel, or\npress Ctrl+Enter (Windows) or Command+Return (Mac) on your keyboard.\n\n\n\n\n\n\n\nWhat are the Command, Ctrl, Enter or Return keys?\n\n\n\n\n\nThe Enter or Return key on your keyboard may be marked ⏎. On Mac keyboards, the Command key is sometimes marked ⌘.\n\n\n\nIf you just want to run one line of code, you don’t need to highlight the whole line to run it. Just place the cursor anywhere on that line, then either click the Run button or press Ctrl+Enter/Command+Return.\nHighlight all three lines of code and press Run. You will see that nothing happens in the R script, but several messages appear in the R Console. You will also see that the three lines of code have been copied into the Console just above those messages. This means you can use the R Console as a record of all the code you have run in RStudio.\n\n\n\n\n\n\nPackage loading messages\n\n\n\nLoading packages will sometimes produce various messages. For now, you can safely ignore these messages. It is not always safe to ignore R messages – we will cover how you should deal with messages, warnings and errors in a later chapter.\n\n\n\n\n\nchapter_02a.R\n\n# Load the R packages we need to analyse this data\npacman::p_load(ggspatial, sf, tidyverse)\n\n\n\n\n\n\n\n\nWhat do these messages mean?\n\n\n\n\n\nFor now you don’t need to worry about these messages, but if you really want to know what they mean …\nSome R packages make use of other apps and utilities on your computer. For example, the sf package makes use of a piece of software call GDAL that is used for managing spatial data. So that you know which version of GDAL is being used, sf prints a message telling you.\nThe tidyverse package itself loads several packages that are commonly used together for analysing data. When you load tidyverse, it will print a message telling you which packages it has loaded, along with the version number for each package. It also prints a message saying if any functions from the tidyverse packages have replaced (“masked”) any functions from packages that were previously loaded.\nIn general, R packages use start-up messages to remind you of information that is not likely to be critical to your work, but which it might be useful to know at some point in the future.\n\n\n\n\n\n\n\n\n\nLoading packages\n\n\n\nHow do you run a line of code in RStudio for Windows without highlighting the whole line?\n\n Press Alt+F4. Place the cursor anywhere on the line and press Ctrl+Enter. Double-click the line and press Run. Click on the Console and press Shift+Enter.\n\nWhich package do we load to simultaneously load common packages used for data wrangling and analysis?\n\n ggspatial tidyverse pacman sf\n\n\n\n\n\n2.3.2 Loading data\nThe first task in creating any crime map is to obtain the crime and other data necessary. In many cases preparing the data for analysis and mapping will be a substantial task, but in this case we are going to use some pre-prepared crime data together with a pre-drawn street map (which we will ask R to download automatically when it draws the final map).\nThe data we will use will be records of homicides in the Downtown neighbourhood of Atlanta, Georgia, in 2019. We can load the homicide data using the read_csv() function. A function in R is a piece of code that performs an action. You can think of functions as being like verbs (i.e. ‘doing words’), which is why the names of functions are often verbs such as filter(), select(), etc. The read_csv() function loads data from a file and prints a message showing the name of each column in the data and the type of data (number, text etc.) in each column. Again, you can ignore this message for now.\n\n\n\n\n\n\nMultiple functions with similar names\n\n\n\nSometimes R has several functions that have similar names but do different things. That means it is very important to pay attention to which function you need in any particular set of circumstances. In this case we are using the read_csv() function, which does a slightly different thing from the similarly named read.csv() function (note the . instead of the _). It is usually better to use read_csv() because it produces a type of object (called a tibble) that is easier to work with than the object (called a data frame) that is produced by read.csv(). For that reason, we will always use read_csv() in this course.\n\n\nCopy these lines of code into your R script file, then run the line containing the read_csv() function by clicking anywhere on that line and pressing Ctrl+Enter (Windows) or Command+Return (Mac) on your keyboard.\n\n\n\nchapter_02a.R\n\n# Download the data directly from a URL and store it as an object\nhomicides &lt;- read_csv(\"https://mpjashby.github.io/crimemappingdata/downtown_homicides.csv\")\n\n\nRows: 4 Columns: 4\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): label\ndbl (3): report_number, longitude, latitude\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\n\n\n\n\n\nWhat does this output mean?\n\n\n\n\n\nWhen read_csv() loads data from a file, it produces a short summary of the data and prints that in the R Console. Looking at this message, you can see that the data contains 4 rows and 4 columns of values in each row. You can also see the names of the columns: ‘report_number’, ‘label’, ‘longitude’ and ‘latitude’.\n\n\n\nWe have stored the results of the read_csv() function in an R object called homicides. An object in R is anything that stores any type of data. There are many types of objects, but for this chapter we don’t need to explore these in any more detail. All you need to remember for now is that objects store data (which is why their names are often nouns) and functions do things.\n\n\n2.3.3 Viewing the data\nTo check the data has been loaded correctly, we can view the loaded data using the head() function. By default, head() prints the first six rows of the data stored in an object. Copy this code into the R Console and press Enter or Return on your keyboard.\n\n\n\nR Console\n\nhead(homicides)\n\n\n# A tibble: 4 × 4\n  report_number label                                         longitude latitude\n          &lt;dbl&gt; &lt;chr&gt;                                             &lt;dbl&gt;    &lt;dbl&gt;\n1     190191530 \"400 W PEACHTREE ST NW\\n19 January @ 15:00\"       -84.4     33.8\n2     190570315 \"171 AUBURN AVE NE @CITY WALK APARTMENTS\\n26…     -84.4     33.8\n3     192160018 \"241 FORSYTH ST SW\\n 4 August @ 00:00\"            -84.4     33.7\n4     193302338 \"80 JESSE HILL JR DR SE @GRADY\\n26 November …     -84.4     33.8\n\n\nThe data contain four columns: a unique identifier for a homicide, a label describing when and where that homicide occurred, and the longitude and latitude of the homicide location. We can use this data to plot the homicides on a map.\n\n\n\n\n\n\nQuoted and unquoted values\n\n\n\nIn the code head(homicides), there are no quote marks around the word homicides.\nAlmost all programming languages will interpret words differently depending on whether they have quotes around them or not. In this case, if you type the code head(homicides) then R will print the first few rows of the data stored in the homicides object.\nOn the other hand, if you type the code head(\"homicides\") or head('homicides'), R will interpret this as an instruction to print the first few elements of the literal text ‘homicides’. Since the text ‘homicides’ contains only one element (more about that later), head(\"homicides\") will just print the word ‘homicides’.\n\n\n\n\n\n\n\n\nLoading data\n\n\n\nWhy is the read_csv() function preferred over read.csv() in this chapter?\n\n read_csv() handles larger datasets. read_csv() produces a tibble instead of a data frame. read.csv() cannot process numeric data. read_csv() is the only function compatible with RStudio.\n\nWhat is the output of the following R code: head(homicides)?\n\n A plot of all homicides on a map. A summary of the entire homicides dataset. The first six rows of the homicides dataset. An error message about missing data.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Your first crime map</span>"
    ]
  },
  {
    "objectID": "02_your_first_crime_map/index.html#processing-the-data",
    "href": "02_your_first_crime_map/index.html#processing-the-data",
    "title": "2  Your first crime map",
    "section": "2.4 Processing the data",
    "text": "2.4 Processing the data\nBefore we can plot the data on a map, we have to complete some pre-processing steps. Having to process data before being able to analyse or visualise it is common in all types of data analysis, but spatial analysis often involves additional processing that takes account of the special features of spatial data.\n\n2.4.1 Converting the data into a spatial format\nTwo data-processing tasks are needed to produce this map. The first is to convert the data into a simple features or SF object, which is a special type of R object that can be used by functions that process spatial data. We will cover the details of the st_as_sf() function that converts our data into into an SF object later on.\nCopy this code into the chapter_02a.R file, then select these lines of code and click the Run button or press Ctrl+Enter/Command+Return.\n\n\n\nchapter_02a.R\n\n# Convert the data to a simple features object, which we can use in functions \n# that work on spatial data\nhomicides_sf &lt;- st_as_sf(\n  homicides, \n  coords = c(\"longitude\", \"latitude\"), \n  crs = \"EPSG:4326\"\n)\n\n\nWhen you run this code, it looks like nothing happened. This is because the results of the code are stored in the homicides_sf object. We can check the contents of homicides_sf in the R Console as before:\n\n\n\nR Console\n\nhead(homicides_sf)\n\n\nSimple feature collection with 4 features and 2 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: -84.39732 ymin: 33.74827 xmax: -84.38185 ymax: 33.76614\nGeodetic CRS:  WGS 84\n# A tibble: 4 × 3\n  report_number label                                              geometry\n          &lt;dbl&gt; &lt;chr&gt;                                           &lt;POINT [°]&gt;\n1     190191530 \"400 W PEACHTREE ST NW\\n19 January @ …  (-84.3876 33.76614)\n2     190570315 \"171 AUBURN AVE NE @CITY WALK APARTME… (-84.38185 33.75546)\n3     192160018 \"241 FORSYTH ST SW\\n 4 August @ 00:00\" (-84.39732 33.74827)\n4     193302338 \"80 JESSE HILL JR DR SE @GRADY\\n26 No… (-84.38198 33.75168)\n\n\nThe data looks identical to before running the function st_as_sf(), except that the two columns called longitude and latitude have disappeared and there is now an extra column called geometry. The geometry column is important because lots of functions in R can recognise that the geometry column represents a location on the surface of the Earth that can be used to analyse and map data in space.\n\n\n2.4.2 Changing the data projection\nThe geometry column in the homicides_sf object represents locations on the surface of the earth using co-ordinates (pairs of numbers). In this case, the co-ordinates are expressed as longitudes and latitudes, but there are lots of other types of co-ordinates (known as co-ordinate reference systems).\nWe’ll learn more about co-ordinate reference systems in Chapter 4, but for now it’s enough to know that each different system has advantages and disadvantages. To make the homicide locations easier to add to a map, we are going to first transform the co-ordinates from longitudes and latitudes to a co-ordinate reference system that is specifically designed for mapping data for the US state of Georgia.\nTo do this, we will use the st_transform() function, together with a code representing the co-ordinate reference system we want to use (you don’t need to understand this code at this stage). Copy this code into the chapter_02a.R file, click anywhere on the new line of code and press the Run button or press Ctrl+Enter/Command+Return.\n\n\n\nchapter_02a.R\n\n# Transform the data co-ordinate reference system\nhomicides_sf_trans &lt;- st_transform(homicides_sf, \"EPSG:26967\")\n\n\nOnce again, we can check what the result looks like in the R Console.\n\n\n\nR Console\n\nhead(homicides_sf_trans)\n\n\nSimple feature collection with 4 features and 2 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 678630.6 ymin: 415608.5 xmax: 680065.5 ymax: 417588.4\nProjected CRS: NAD83 / Georgia West\n# A tibble: 4 × 3\n  report_number label                                             geometry\n          &lt;dbl&gt; &lt;chr&gt;                                          &lt;POINT [m]&gt;\n1     190191530 \"400 W PEACHTREE ST NW\\n19 January @ … (679535.4 417588.4)\n2     190570315 \"171 AUBURN AVE NE @CITY WALK APARTME… (680065.5 416402.8)\n3     192160018 \"241 FORSYTH ST SW\\n 4 August @ 00:00\" (678630.6 415608.5)\n4     193302338 \"80 JESSE HILL JR DR SE @GRADY\\n26 No… (680052.6 415983.6)\n\n\nThe data looks almost identical, except that the values in the geometry column have changed (you don’t need to understand yet the details of how these numbers are different). Now that we’ve completed the data processing, we can go on to produce the map itself.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Your first crime map</span>"
    ]
  },
  {
    "objectID": "02_your_first_crime_map/index.html#checking-our-progress",
    "href": "02_your_first_crime_map/index.html#checking-our-progress",
    "title": "2  Your first crime map",
    "section": "2.5 Checking our progress",
    "text": "2.5 Checking our progress\nAt the moment, the file chapter_02a.R that you have created should look like this:\n\n\n\nchapter_02a.R\n\n# Load the R packages we need to analyse this data\npacman::p_load(ggspatial, sf, tidyverse)\n\n# Download the data directly from a URL and store it as an object\nhomicides &lt;- read_csv(\"https://mpjashby.github.io/crimemappingdata/downtown_homicides.csv\")\n\n# Convert the data to a simple features object, which we can use in functions \n# that work on spatial data\nhomicides_sf &lt;- st_as_sf(\n  homicides, \n  coords = c(\"longitude\", \"latitude\"), \n  crs = \"EPSG:4326\"\n)\n\n# Transform the data co-ordinate reference system\nhomicides_sf_trans &lt;- st_transform(homicides_sf, \"EPSG:26967\")\n\n\nNote how this code tells a clear story about exactly what you have done. This code is easier to understand because:\n\nwe have left blank lines between each piece of code so that it’s easy to see each separate task that is being completed, and\nwe have used comments (lines of code starting with #) to explain what each step does.\n\nBefore going any further, let’s check all of our code has been run successfully. To do this, find the Environment panel in RStudio: if you followed the instructions on setting up RStudio in Section 1.4.1, you should see Environment below your R script file. Click ‘Environment’ and RStudio will show you a list of all the objects that you have created since the start of your R session. You should see three objects listed:\n\nhomicides, which should have 4 observations (i.e. rows) of 4 variables (i.e. columns)\nhomicides_sf, which should have 4 observations of 3 variables\nhomicides_sf_trans, which should have 4 observations of 3 variables\n\nIf any of those objects are missing from the list of objects in the Environment panel, you might have forgotten to run the code after pasting it into your script file. If so, you can run that part of the code now by selecting the relevant lines of code and pressing the Run button in RStudio or Ctrl+Enter/Command+Return on your keyboard.\n\n\n\n\n\n\nProcessing the data\n\n\n\nWhat does the st_as_sf() function do?\n\n Converts non-spatial data into a simple features (SF) object. Analyzes spatial relationships between features. Exports data to a spatial file format. Visualizes data as a map.\n\nWhy is it important to use comments in your R script?\n\n To make the script executable by others. To explain what each part of the code does. To automatically debug the code. To make the R script run faster.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Your first crime map</span>"
    ]
  },
  {
    "objectID": "02_your_first_crime_map/index.html#drawing-the-map",
    "href": "02_your_first_crime_map/index.html#drawing-the-map",
    "title": "2  Your first crime map",
    "section": "2.6 Drawing the map",
    "text": "2.6 Drawing the map\nWe are now ready to produce our map of homicides in downtown Atlanta. So that people viewing the map will understand where the homicides occurred, we will plot the homicides on top of a base map showing streets, parks and other geographic features obtained from an online web mapping service.\nThe code needed to produce the map looks quite complicated, but we don’t need to understand it all at this point – we will go through this code in Chapter 4. For now, copy the code into the chapter_02a.R file, put the cursor somewhere on the line that includes the function ggplot(), then click the Run button or Ctrl+Enter/Command+Return on your keyboard.\n\n\n\nchapter_02a.R\n\n# Create the map\nggplot(homicides_sf_trans) + \n  annotation_map_tile(type = \"osm\", zoomin = 0, progress = \"none\") + \n  geom_sf_label(aes(label = label), hjust = 1, size = 2.5, vjust = 0) + \n  geom_sf(colour = \"orangered1\", size = 4) + \n  scale_x_continuous(expand = expansion(mult = 0.5)) +\n  scale_y_continuous(expand = expansion(mult = 0.2)) +\n  labs(\n    title = \"Homicides in Downtown Atlanta, 2019\",\n    caption = \"Background map by OpenStreetMap\"\n  ) +\n  theme_void()\n\n\nLoading required namespace: raster\n\n\n\n\n\n\n\n\n\nYou can change the appearance of the map by changing various parts of the code above. For example, you could change the colour of the points that mark the homicides by changing the code colour = \"orangered1\" to colour = \"mediumblue\", or change the base map to a different style by changing the code type = \"osm\" to type = \"cartolight\". Try some of these out – after you make each change, press the Run button in RStudio or Ctrl+Enter/Command+Return on your keyboard to see how the map changes.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Your first crime map</span>"
    ]
  },
  {
    "objectID": "02_your_first_crime_map/index.html#putting-the-code-together",
    "href": "02_your_first_crime_map/index.html#putting-the-code-together",
    "title": "2  Your first crime map",
    "section": "2.7 Putting the code together",
    "text": "2.7 Putting the code together\nNow we have walked through the different parts of the code, we can create a map from scratch in a single block of code. In this example, we will map homicides from in Glenrose Heights neighbourhood of Atlanta, and a different style of base map. Since the area covered by the map is derived from the data itself, the extent of the map will update automatically.\nCreate a new script file in RStudio by clicking File, New File then R Script. Paste the following code into this new file, then save the file as chapter_02b.R.\n\n\n\nchapter_02b.R\n\n# Load the R packages we need to analyse this data\npacman::p_load(ggspatial, sf, tidyverse)\n\n# Download the data directly from a URL and store it as an object\nhomicides &lt;- read_csv(\"https://mpjashby.github.io/crimemappingdata/glenrose_heights_homicides.csv\")\n\n# Convert the data to a simple features object, which we can use in functions \n# that work on spatial data\nhomicides_sf &lt;- st_as_sf(\n  homicides, \n  coords = c(\"longitude\", \"latitude\"), \n  crs = \"EPSG:4326\"\n)\n\n# Transform the data to a co-ordinate reference system for the state of Georgia\nhomicides_sf_trans &lt;- st_transform(homicides_sf, \"EPSG:26967\")\n\n# Plot the map\nggplot(homicides_sf_trans) + \n  annotation_map_tile(type = \"osm\", zoomin = 0, progress = \"none\") + \n  geom_sf_label(aes(label = label), hjust = 1, size = 2.5, vjust = 0) + \n  geom_sf(colour = \"mediumblue\", size = 4) + \n  scale_x_continuous(expand = expansion(mult = 1.5)) + \n  scale_y_continuous(expand = expansion(mult = 0.2)) + \n  labs(\n    title = \"Homicides in Glenrose Heights, 2019\",\n    caption = \"Background map by OpenStreetMap\"\n  ) +\n  theme_void()\n\n\n\n\nRows: 4 Columns: 4\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): label\ndbl (3): report_number, longitude, latitude\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\n\n\n\n\n\n\nWhen we want to run all the code in a file, it would be tedious to run each chunk of code separately. Instead, we can run all the code in a file in one go by:\n\nclicking the tiny downward arrow next to the Source button in the script panel in RStudio (to the right of the Run button) then clicking Source with Echo, or\nclicking anywhere in our code and pressing Shift+Ctrl+Enter (Windows) or Shift+Command+Return (Mac) on the computer keyboard.\n\nThis runs all the code in our script in one go. You should see some messages appear in the R Console, and the final map appear in the RStudio Plots panel.\nSave the chapter_02b.R file by hitting Ctrl+S on Windows or Command+S on Mac, then restart R to start a new session by clicking on the Session menu and then clicking Restart R. This creates a blank canvas for the next chapter.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Your first crime map</span>"
    ]
  },
  {
    "objectID": "02_your_first_crime_map/index.html#in-summary",
    "href": "02_your_first_crime_map/index.html#in-summary",
    "title": "2  Your first crime map",
    "section": "2.8 In summary",
    "text": "2.8 In summary\nWell done – you have created your first map!\nYou may not have understood every line of code in this chapter, but we will cover them all in more detail over the rest of this course. By the end of this course, you will be able to write code like this to create many different types of crime map.\nIn this chapter you have learned how to load data into R, prepare it for use in making a map and then used it to make your first crime map of this course.\nThe map we have produced in this chapter is effective for showing the locations of just a few crimes, but is too limited to show more complicated patterns or larger datasets. In the following chapter, we will learn how to produce more sophisticated maps and spatial analysis. We will also learn how each of the functions that we have used in this chapter work.\n\n\n\n\n\n\nRevision questions\n\n\n\nAnswer these questions to check you have understood the main points covered in this chapter. Write between 50 and 100 words to answer each question.\n\nWhy is it important to differentiate between temporary and permanent code when working in R? Provide an example of each and explain how they are used in creating a crime map.\nExplain why it is useful to store spatial data as a simple features (SF) object. What steps are required to convert crime data into an SF object, and why is this conversion necessary?\nWhat are coordinate reference systems (CRS), and why might you need to transform the CRS of spatial data? Describe how this was applied in the chapter to map homicides in Atlanta.\nWhat steps are involved in creating a crime map in R from loading data to visualizing it? Highlight the key functions used and the purpose of each in the process.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Your first crime map</span>"
    ]
  },
  {
    "objectID": "03_data_wrangling/index.html",
    "href": "03_data_wrangling/index.html",
    "title": "3  Wrangling data",
    "section": "",
    "text": "3.1 Introduction\nA major step in using any data to make decisions or draw conclusions is data wrangling: the process of transforming data from the format in which we originally have it to the format needed to analyse and present it to our audience.\nTo get started, open RStudio and check that you are working in the Crime Mapping project you created previously. To do this, looking on the top right-hand corner of the RStudio window. If you see the text Crime Mapping, all is well. If you see the text Project: (None), click on the downward triangle next to that text and then select the Crime Mapping project – this will cause RStudio to restart and open your project.\nNow create a new R script file by clicking the File menu, then New File and then R Script. A new file will open in the left-hand panel in RStudio. Click File then Save As... to save this file in the project directory you created when you completed the previous chapter. Save the file as chapter_03a.R.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Wrangling data</span>"
    ]
  },
  {
    "objectID": "03_data_wrangling/index.html#introduction",
    "href": "03_data_wrangling/index.html#introduction",
    "title": "3  Wrangling data",
    "section": "",
    "text": "3.1.1 Functions\nIn this chapter we will learn how to wrangle data in R using functions – specialised pieces of code that do something to the data we give it. The code to use a function (sometimes called calling the function) has two parts: the function name followed by a pair of parentheses, inside which are zero or more arguments separated by commas. Arguments are a way of providing input that a function works on, or to fine-tune the way the function works (we will see many examples of this later). Remember that you can identify a function in R because the name will always have parentheses after it.\nOne basic R function is sqrt(), which calculates the square root of a number. The sqrt() function has only one argument: the number that we want to find the square root of. If we typed the following code into the R Console, R would show us the square root of 2.\n\n\n\nR Console\n\nsqrt(2)\n\n\nWhen you run code in R, by default R prints the output of your code – in this case, just the number 1.414214 (for now, you can ignore the number [1] in square brackets).\n\n\n3.1.2 Packages\nR contains thousands of different functions that do different things. A few functions are contained in the default installation of R that you have already installed (this is sometimes referred to as base R). But most functions are contained in packages, which are extensions to base R. Most packages focus on a particular type of data analysis, so that there are packages devoted to time-series analysis, testing whether events are clustered in particular places, network analysis and thousands of other tasks. Packages are often developed by experts in the field, and are typically updated to introduce new features.\nTo use a package in R, we must do two things:\n\ninstall the package, which we have to do just once on each computer we want to use, then\nload the package, which we have to do each time we restart R (which happens when we open RStudio or switch between projects).\n\nThe install.packages() function downloads and installs packages from the Comprehensive R Archive Network (universally known as CRAN), which contains about 22,000 different packages. Some packages that are still in the early stages of development are not available on CRAN, but all the packages we will use are there.\n\nSo to install (for example) the package called tidyverse, which we will use extensively in this chapter, we would run the R code:\n\n\n\nR Console\n\ninstall.packages(\"tidyverse\")\n\n\nWe only have to install a package once for each computer that we will use to run R, although we would have to do it again if we updated to a new version of R.\n\n\n\n\n\n\nNever add install.packages() to an R script\n\n\n\nYou should only ever use the install.packages() function in the R Console, never in an R script. There are two reasons for this:\n\nWhen you share your code with other people – which you will do often in this course – they may have their system set up in such a way that running install.packages() in a script file might cause other code on their computer to stop working.\nSince you don’t need to install a package every time you need it (you only need to install it once), your code will run much more slowly if you needlessly install packages every time you run your code.\n\n\n\nOnce a package is installed on our computer, we have to load it so that we can use it in our code. We load packages using the p_load() function from the pacman package. So to load the tidyverse package, we run the code:\n\n\n\nchapter_03a.R\n\n# Load packages\npacman::p_load(tidyverse)\n\n\n\n\n\n\n\n\nWhy does this line of code start with pacman::?\n\n\n\n\n\nThe pacman package is an R package that is used to manage other R packages. The p_load() function from that package is used to load packages. But we learned above that we must load a package before we can use it, which means we cannot use functions from the pacman package until we have loaded the pacman package. To get around that, instead of just using the function name p_load(), we can instead specify which package p_load() is from by prefixing the function name with the relevant package name, separated by ::.\n\n\n\nOne major advantage of using p_load() to load packages is that before loading any packages, this function will first install them if they have not already been installed on the computer we are using. This means that once we have the pacman package installed we won’t need to worry about whether a package is installed or not – we can just load the package, knowing it will automatically be installed if necessary.\nMany packages are focused on specialist tasks and so are only used occasionally, but a few packages are likely to be useful in almost all the code we write. Fortunately, packages can themselves load other packages, and all the main packages we need are loaded by the tidyverse package. That is why you will often see pacman::p_load(tidyverse) at the top of R code in subsequent chapters – that short line of code loads several packages containing hundreds of functions that we can use in data analysis.\n\n\n\n\n\n\nData wrangling\n\n\n\nWhat is the purpose of data wrangling?\n\n The process of collecting data from multiple sources. The process of transforming raw data into a format suitable for analysis. The process of visualizing data on a map. The process of cleaning corrupted data files.\n\nIn general, what is a function in R?\n\n A visual representation of data. A mathematical formula used in graphs. A set of instructions for saving data. A specialized piece of code that performs a task on data.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Wrangling data</span>"
    ]
  },
  {
    "objectID": "03_data_wrangling/index.html#sec-read-data",
    "href": "03_data_wrangling/index.html#sec-read-data",
    "title": "3  Wrangling data",
    "section": "3.2 Loading data",
    "text": "3.2 Loading data\nBefore we can do anything with any data, we have to load it into R. In this course we will read tabular data in comma-separated values (CSV) and Excel formats, as well as spatial data in different formats (because there are lots of ways to store spatial data). We will learn how to read CSV and Excel data now, but leave loading spatial data until later.\nTabular data (sometimes known as rectangular data) describes data formats with multiple columns where every column has the same number of rows. For example, crime data might have columns for the type of crime, date and address at which the crime occurred.\n\n\n\nCrime data in rectangular format\n\n\ntype\ndate\naddress\n\n\n\n\nhomicide\n14 Feb 2024\n274 Main St\n\n\nnon-residential burglary\n24 Sep 2024\n541 Station Rd\n\n\npersonal robbery\n27 Jan 2025\n10 North Av\n\n\n\n\n\nAlmost all the data we will use in this course will be in this rectangular format, and most of the functions we will use expect data to be rectangular.\n\n3.2.1 Loading CSV data\n\nData stored in CSV format is easy to load with the read_csv() function from the readr package. readr is one of the packages loaded by the tidyverse package, so all we need to do to use this package is include the code pacman::p_load(tidyverse) on the first line of our R script. We will use comments (lines of code beginning with #) to help explain the code as we go.\nread_csv() can handle data stored locally on your computer, as well as data stored remotely on an external website. During this course, most of the data we will use will be remote data stored on the crimemappingdata website.\n\n\n\nR Console\n\n# We can load data from a file in the same folder as our R script\nsan_fran_rob &lt;- read_csv(\"san_francisco_robbery.csv\")\n\n# Or another folder on your computer ('../' is short for the parent \n# folder of the current folder)\nsan_fran_rob &lt;- read_csv(\"../san_francisco_robbery.csv\")\n\n# Or directly from a file online\nsan_fran_rob &lt;- read_csv(\"http://example.com/san_francisco_robbery.csv\")\n\n\nIn each of these examples, the code stores the result of the read_csv() function in an object named san_fran_rob. Objects are places where we can store data. To create an object and store our data in it, we use the assignment operator &lt;- (a less-than sign followed by a dash). Continually typing &lt;- can be tedious, so in RStudio we can use the keyboard short cut Option+- (on Mac) or Alt+- (on Windows or Linux) to insert the complete operator.\n\n\n\n\n\n\nObject names can be overwritten\n\n\n\nWhen choosing object names, it is important to remember that if you assign a value (such as the number 1 or the result of the function read_csv()) to an object name, R will overwrite any existing value of that object name. We can see this in a simple example:\n\none_to_ten &lt;- 1:10\none_to_ten &lt;- sqrt(2)\n\nIf we were to run this code, the object one_to_ten would not actually hold the numbers from one to ten, but instead the value 1.414214 (the square root of two). There is no way to undo assignment of a value to an object, so once you have run the code one_to_ten &lt;- sqrt(2) it is not possible to recover any previous value that was assigned to the object one_to_ten.\n\n\nObjects come in several different types, with tabular data typically being stored as a data frame. The read_csv() function actually produces a modern variation on the data frame called (slightly strangely) a tibble, which makes use of some advances in how R handles data since the data-frame format was set 20 years ago. Tibbles behave just like data frames almost all of the time (so much so that people working with tibbles often call them data frames) except for a few occasions where they behave in a more-convenient way. We will always use tibbles instead of data frames in this course.\nWe can use read_csv() to load data from https://mpjashby.github.io/crimemappingdata/san_francisco_robbery.csv and store it in an object called san_fran_rob. Add this code to the script file chapter_03a.R and then run that code as you learned about in Section 2.3.1.\n\n\n\nchapter_03a.R\n\n# Load San Francisco robbery data\nsan_fran_rob &lt;- read_csv(\"https://mpjashby.github.io/crimemappingdata/san_francisco_robbery.csv\")\n\n\nRows: 951 Columns: 5\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (1): offense_type\ndbl  (3): uid, longitude, latitude\ndttm (1): date_time\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nIf the data are loaded successfully, R will list the columns in the data and the type of variable (numeric, date etc.) stored in each column. The format of this is somewhat esoteric, but if you are interested they are explained in the ‘Extra detail’ box below.\n\n\n\n\n\n\nWhat do the messages produced by read_csv() mean?\n\n\n\n\n\nBy default, the read_csv() function prints a message when it loads data to summarise the format of each data column. In the case of the san_fran_rob dataset, read_csv() tells us that:\n\nthere is one column called offense_type that contains character (chr) values,\nthere are three columns called uid, longitude and latitude containing numeric (dbl) values, and\nthere is one column called date_time that contains values stored as dates and times (dttm).\n\nThere are some other possible types of data, but we will learn about these later on. The numeric values are referred to as dbl values because they are stored in a format that can handle numbers that are not whole numbers (e.g. 123.456). This format for storing numbers is called the double-precision floating-point format, which is often known as the double format for short. Most numbers in R are stored in double format, so you can think of the format code dbl as meaning ‘numeric’.\n\n\n\nTo see the first few rows of data currently stored in an object, we can use the head() function.\n\n\n\nR Console\n\nhead(san_fran_rob)\n\n\n# A tibble: 6 × 5\n       uid offense_type     date_time           longitude latitude\n     &lt;dbl&gt; &lt;chr&gt;            &lt;dttm&gt;                  &lt;dbl&gt;    &lt;dbl&gt;\n1 24103841 personal robbery 2019-01-01 19:50:00     -122.     37.8\n2 24103948 personal robbery 2019-01-02 08:00:00     -122.     37.8\n3 24104162 personal robbery 2019-01-03 00:30:00     -122.     37.8\n4 24104203 personal robbery 2019-01-03 03:13:00     -122.     37.8\n5 24104237 personal robbery 2019-01-03 09:30:00     -122.     37.7\n6 24104238 personal robbery 2019-01-03 09:30:00     -122.     37.7\n\n\n\n\n3.2.2 Loading Excel data\n\nLoading data from Microsoft Excel files is very similar to loading CSV data, with a few important differences. Functions to load Excel data are contained in the readxl package.\nThere are two main things we must do to import Excel data that are not required for importing CSV data. The first is that the readxl package cannot directly load files from a URL, instead only loading files that are present on your computer. To get round this, we will first download an Excel file and store it in a temporary directory (to avoid cluttering up our computers).\nTo practice loading data from an Excel file, create a new R script file called chapter_03b.R and paste the following code into it, then run the code.\n\n\n\nchapter_03b.R\n\n# Load packages\npacman::p_load(readxl, tidyverse)\n\n# Specify the name of and location of our temporary file: it does not matter\n# what this file is called or where it is stored, so we use the tempfile()\n# function to create a file in the correct location automatically\ntemp_file &lt;- tempfile(fileext = \".xlsx\")\n\n# Download the Excel file and store it in the temporary location\ndownload.file(\n  url = \"https://mpjashby.github.io/crimemappingdata/aggravated_assaults.xlsx\",\n  destfile = temp_file,\n  mode = \"wb\"\n)\n\n\nThe download.file() function does not produce any output if the file has been successfully downloaded, so you will not see any output when you run this code.\n\n\n\n\n\n\nImportant\n\n\n\nUsing download.file() on Windows\nIf you are using a Windows computer, you may find that the download.file() function in the code below does not work as expected. This is because Windows handles files in a way that distinguishes between plain-text files such as .txt and .csv files and binary files, which includes most other file types (including compressed files). Since Excel files are not plain-text files, on Windows you need to specify that you want them to be downloaded as a binary file. To do this, add the argument mode = \"wb\" to the download.file() function so that it reads:\ndownload.file(\n  url = \"https://mpjashby.github.io/crimemappingdata/aggravated_assaults.xlsx\",\n  destfile = temp_file,\n  mode = \"wb\"\n)\nIf you are using a Mac or a Linux computer then you do not need to worry about this.\n\n\nNow we have downloaded our data, we can load it into R. Since Excel files can contain multiple sheets, we need to specify which sheet we would like to load into a tibble. We can use the excel_sheets() function to get a list of sheets in an Excel file:\n\n\n\nR Console\n\n# Get a list of sheets in an Excel file\nexcel_sheets(temp_file)\n\n\n[1] \"Austin\"     \"Fort Worth\" \"Seattle\"   \n\n\nWe can now load the sheet containing data for Austin and view the first few rows of the resulting object:\n\n\n\nchapter_03b.R\n\nagg_assault_data &lt;- read_excel(temp_file, sheet = \"Austin\")\n\n\nand, as usual, we can use the head() function to look at the data:\n\n\n\nR Console\n\nhead(agg_assault_data)\n\n\n# A tibble: 6 × 5\n  date                longitude latitude location_type location_category\n  &lt;dttm&gt;                  &lt;dbl&gt;    &lt;dbl&gt; &lt;chr&gt;         &lt;chr&gt;            \n1 2019-01-01 00:00:00     -97.7     30.3 residence     residence        \n2 2019-01-01 00:00:00     -97.8     30.2 residence     residence        \n3 2019-01-01 00:01:00     -97.7     30.3 &lt;NA&gt;          &lt;NA&gt;             \n4 2019-01-01 00:15:00     -97.7     30.3 &lt;NA&gt;          &lt;NA&gt;             \n5 2019-01-01 00:27:00     -97.8     30.2 residence     residence        \n6 2019-01-01 00:30:00     -97.7     30.3 &lt;NA&gt;          &lt;NA&gt;             \n\n\nNow we have learned how to load our data into an object, we can use other R functions to work with that data in many different ways.\n\n\n\n\n\n\nImportant\n\n\n\nDifferent types of data are loaded into R with different functions, e.g. CSV files are loaded with the read_csv() function from the readr package and Microsoft Excel files are loaded with the read_excel() function from the readxl package. Appendix A has a list of which function to use to load each type of file.\n\n\n\nLearn more about how to read data into R by reading this chapter of the free online book R for Data Science.\nExcel data can often be messy and the readxl package contains various other functions that can be used to deal with this. You can learn more about how to handle messy Excel data in this online tutorial.\n\n\n\n\n\n\n\nReading data\n\n\n\nAnswer the following questions to check your understanding of what we’ve learned so far in this chapter. If you get a question wrong, you can keep trying until you get the right answer.\nWhat R package contains the function read_csv() to read CSV data?\n\n readxl reader readr readcsv\n\nWhat R code prints the first few rows of the tibble called san_fran_rob?\n\n message(san_fran_rob) summary(san_fran_rob) peak_inside(san_fran_rob) head(san_fran_rob)\n\nIf we create an object using the code number_ten &lt;- 10 and then run the code number_ten &lt;- sqrt(2), what value will the object number_ten now have?\n\n 10 (the number 10) 1.414214 (the square root of 2) 10.41421 (10 plus the square root of 2) 14.14214 (10 times the square root of 2)",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Wrangling data</span>"
    ]
  },
  {
    "objectID": "03_data_wrangling/index.html#selecting-columns",
    "href": "03_data_wrangling/index.html#selecting-columns",
    "title": "3  Wrangling data",
    "section": "3.3 Selecting columns",
    "text": "3.3 Selecting columns\nIn this section we will learn how to reduce the size of our data by selecting only the columns we need and discarding the rest. This can be particularly useful if we are working with a very-large dataset, or if we want to produce a table containing only some columns.\n\n\n\nWe can use the select() function from the dplyr package (one of the packages that is loaded automatically when we call the pacman::p_load(tidyverse) function) to select columns.\nIf we wanted to select just the date and location_type columns from the agg_assault_data we loaded in the previous section, we can use this code:\n\n\n\nR Console\n\nselect(agg_assault_data, date, location_type)\n\n\n# A tibble: 8,696 × 2\n   date                location_type\n   &lt;dttm&gt;              &lt;chr&gt;        \n 1 2019-01-01 00:00:00 residence    \n 2 2019-01-01 00:00:00 residence    \n 3 2019-01-01 00:01:00 &lt;NA&gt;         \n 4 2019-01-01 00:15:00 &lt;NA&gt;         \n 5 2019-01-01 00:27:00 residence    \n 6 2019-01-01 00:30:00 &lt;NA&gt;         \n 7 2019-01-01 00:51:00 &lt;NA&gt;         \n 8 2019-01-01 01:00:00 residence    \n 9 2019-01-01 01:00:00 &lt;NA&gt;         \n10 2019-01-01 01:12:00 residence    \n# ℹ 8,686 more rows\n\n\n\nIn a previous section, we mentioned that the code needed to run (or call) a function in R has two parts: the function name followed by a pair of parentheses, inside which are zero or more arguments separated by commas. The arguments in the select() function (and many other functions in the dplyr package) work in a slightly different way to many other functions. Here, the first argument is the name of the data object that we want to select from. All the remaining arguments (here, date and location_type) are the names of the columns we want to select from the data.\nWe can select as many columns as we want, by just adding the names of the columns separated by commas. The columns in our new dataset will appear in the order in which we specify them in the select() function.\nWe can also use select() to rename columns at the same time as selecting them. For example, to select the columns date and location_type while also renaming location_type to be called type, can use:\n\n\n\nR Console\n\nselect(agg_assault_data, date, type = location_type)\n\n\n# A tibble: 8,696 × 2\n   date                type     \n   &lt;dttm&gt;              &lt;chr&gt;    \n 1 2019-01-01 00:00:00 residence\n 2 2019-01-01 00:00:00 residence\n 3 2019-01-01 00:01:00 &lt;NA&gt;     \n 4 2019-01-01 00:15:00 &lt;NA&gt;     \n 5 2019-01-01 00:27:00 residence\n 6 2019-01-01 00:30:00 &lt;NA&gt;     \n 7 2019-01-01 00:51:00 &lt;NA&gt;     \n 8 2019-01-01 01:00:00 residence\n 9 2019-01-01 01:00:00 &lt;NA&gt;     \n10 2019-01-01 01:12:00 residence\n# ℹ 8,686 more rows\n\n\nselect() removes any columns that we don’t explicitly choose to keep. If we want to rename a column while keeping all the existing columns in the data, we can instead use the rename() function (also from the dplyr package):\n\n\n\nR Console\n\nrename(agg_assault_data, type = location_type)\n\n\n# A tibble: 8,696 × 5\n   date                longitude latitude type      location_category\n   &lt;dttm&gt;                  &lt;dbl&gt;    &lt;dbl&gt; &lt;chr&gt;     &lt;chr&gt;            \n 1 2019-01-01 00:00:00     -97.7     30.3 residence residence        \n 2 2019-01-01 00:00:00     -97.8     30.2 residence residence        \n 3 2019-01-01 00:01:00     -97.7     30.3 &lt;NA&gt;      &lt;NA&gt;             \n 4 2019-01-01 00:15:00     -97.7     30.3 &lt;NA&gt;      &lt;NA&gt;             \n 5 2019-01-01 00:27:00     -97.8     30.2 residence residence        \n 6 2019-01-01 00:30:00     -97.7     30.3 &lt;NA&gt;      &lt;NA&gt;             \n 7 2019-01-01 00:51:00     -97.7     30.3 &lt;NA&gt;      &lt;NA&gt;             \n 8 2019-01-01 01:00:00     -97.7     30.4 residence residence        \n 9 2019-01-01 01:00:00     -97.7     30.3 &lt;NA&gt;      &lt;NA&gt;             \n10 2019-01-01 01:12:00     -97.7     30.3 residence residence        \n# ℹ 8,686 more rows\n\n\nRemember that functions in R generally do not change existing objects, but instead produce (or return) new ones. This means if we want to store the result of this function so we can use it later, we have to assign the value returned by the function to a new object (or overwrite the existing object):\n\n\n\nR Console\n\nagg_assault_locations &lt;- select(agg_assault_data, lon = longitude, lat = latitude)\n\nhead(agg_assault_locations)\n\n\n# A tibble: 6 × 2\n    lon   lat\n  &lt;dbl&gt; &lt;dbl&gt;\n1 -97.7  30.3\n2 -97.8  30.2\n3 -97.7  30.3\n4 -97.7  30.3\n5 -97.8  30.2\n6 -97.7  30.3\n\n\n\nYou can learn more about selecting, filtering and arranging data using the functions in the dplyr package by reading this Introduction to dplyr tutorial.\n\n\n\n\n\n\n\nselect() and rename()\n\n\n\nWhich dplyr function allows you to change the name of columns while keeping all the columns in the original data\n\n rename() select()\n\nWhich dplyr function allows you to choose only some columns in the original data?\n\n rename() select()",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Wrangling data</span>"
    ]
  },
  {
    "objectID": "03_data_wrangling/index.html#filtering-rows",
    "href": "03_data_wrangling/index.html#filtering-rows",
    "title": "3  Wrangling data",
    "section": "3.4 Filtering rows",
    "text": "3.4 Filtering rows\nOften in crime mapping we will only be interested in part of a particular dataset. In the same way that we can select particular columns in our data, we can filter particular rows using the filter() function from the dplyr package.\n\n\n\nIf we were only interested in offences in the agg_assault_data dataset that occurred in residences, we could use filter():\n\n\n\nR Console\n\nfilter(agg_assault_data, location_type == \"residence\")\n\n\n# A tibble: 4,385 × 5\n   date                longitude latitude location_type location_category\n   &lt;dttm&gt;                  &lt;dbl&gt;    &lt;dbl&gt; &lt;chr&gt;         &lt;chr&gt;            \n 1 2019-01-01 00:00:00     -97.7     30.3 residence     residence        \n 2 2019-01-01 00:00:00     -97.8     30.2 residence     residence        \n 3 2019-01-01 00:27:00     -97.8     30.2 residence     residence        \n 4 2019-01-01 01:00:00     -97.7     30.4 residence     residence        \n 5 2019-01-01 01:12:00     -97.7     30.3 residence     residence        \n 6 2019-01-01 01:20:00     -97.7     30.4 residence     residence        \n 7 2019-01-01 01:50:00     -97.7     30.3 residence     residence        \n 8 2019-01-01 02:21:00     -97.8     30.2 residence     residence        \n 9 2019-01-01 02:26:00     -97.8     30.2 residence     residence        \n10 2019-01-01 02:35:00     -97.7     30.4 residence     residence        \n# ℹ 4,375 more rows\n\n\nNote that:\n\nthe column name location_type is not surrounded by quotes (because it represents a column in the data) but the column value \"residence\" is (because it represents the literal character value “residence”), and\nthe == (equal to) operator is used, since a single equals sign = has another meaning in R.\n\nWe can filter using the values of more than one column simultaneously. To filter offences in which the location_category is ‘retail’ and the location_type is ‘convenience store’:\n\n\n\nR Console\n\nfilter(agg_assault_data, \n  location_category == \"retail\", \n  location_type == \"convenience store\"\n)\n\n\n# A tibble: 90 × 5\n   date                longitude latitude location_type     location_category\n   &lt;dttm&gt;                  &lt;dbl&gt;    &lt;dbl&gt; &lt;chr&gt;             &lt;chr&gt;            \n 1 2019-01-06 16:58:00     -97.8     30.5 convenience store retail           \n 2 2019-01-08 17:56:00     -97.7     30.3 convenience store retail           \n 3 2019-01-10 01:25:00     -97.7     30.4 convenience store retail           \n 4 2019-01-12 00:49:00     -97.6     30.4 convenience store retail           \n 5 2019-01-12 05:53:00     -97.7     30.4 convenience store retail           \n 6 2019-01-12 17:01:00     -97.7     30.4 convenience store retail           \n 7 2019-01-12 19:12:00     -97.8     30.3 convenience store retail           \n 8 2019-01-12 19:14:00     -97.7     30.4 convenience store retail           \n 9 2019-01-13 04:10:00     -97.8     30.2 convenience store retail           \n10 2019-01-22 22:01:00     -97.7     30.3 convenience store retail           \n# ℹ 80 more rows\n\n\n\n\n\n\n\n\nfilter() returns rows that meet all the criteria you specify\n\n\n\nWhen you run the code above, the result will contain only those rows in the original data for which the location_category column has the value ‘retail’ and the location_type column has the value ‘convenience store’.\n\n\nAs well as filtering using the == (equals) operator, we can filter using the greater-than (&gt;), less-than (&lt;), greater-than-or-equal-to (&gt;=) and less-than-or-equal-to (&lt;=) operators. For example, we can choose offences that occurred in residences on or after 1 July 2019:\n\n\n\nR Console\n\nfilter(\n  agg_assault_data, \n  location_type == \"residence\", \n  date &gt;= ymd(\"2019-07-01\")\n)\n\n\n# A tibble: 2,286 × 5\n   date                longitude latitude location_type location_category\n   &lt;dttm&gt;                  &lt;dbl&gt;    &lt;dbl&gt; &lt;chr&gt;         &lt;chr&gt;            \n 1 2019-07-01 02:40:00     -97.8     30.4 residence     residence        \n 2 2019-07-01 07:26:00     -97.7     30.3 residence     residence        \n 3 2019-07-01 08:25:00     -97.8     30.2 residence     residence        \n 4 2019-07-01 09:39:00     -97.7     30.3 residence     residence        \n 5 2019-07-01 09:40:00     -97.7     30.4 residence     residence        \n 6 2019-07-01 16:24:00     -97.8     30.1 residence     residence        \n 7 2019-07-01 17:30:00     -97.8     30.2 residence     residence        \n 8 2019-07-01 17:41:00     -97.8     30.2 residence     residence        \n 9 2019-07-01 18:03:00     -97.8     30.2 residence     residence        \n10 2019-07-01 18:16:00     -97.8     30.1 residence     residence        \n# ℹ 2,276 more rows\n\n\n\n\n\n\n\n\nWhat does the ymd() function do?\n\n\n\n\n\nThe ymd() function from the lubridate package (which we will find out more about below) converts a date stored as a text value to a value that R understands represents a calendar date. The function is called ymd() because it processes dates that are stored in year-month-date format. We will learn more about working with dates in Chapter 16.\n\n\n\nSometimes we will want to filter rows that are one thing or another. We can do this with the | (or) operator. For example, we can filter offences that occurred either in leisure facilities or shopping malls on or after 1 July 2019:\n\n\n\nR Console\n\nfilter(\n  agg_assault_data, \n  location_category == \"leisure\" | location_type == \"mall\", \n  date &gt;= ymd(\"2019-07-01\")\n)\n\n\n# A tibble: 10 × 5\n   date                longitude latitude location_type location_category\n   &lt;dttm&gt;                  &lt;dbl&gt;    &lt;dbl&gt; &lt;chr&gt;         &lt;chr&gt;            \n 1 2019-07-01 09:00:00     -97.7     30.3 entertainment leisure          \n 2 2019-07-15 14:06:00     -97.8     30.3 mall          retail           \n 3 2019-08-06 23:20:00     -97.7     30.4 mall          retail           \n 4 2019-08-18 13:31:00     -97.7     30.3 mall          retail           \n 5 2019-08-25 15:35:00     -97.8     30.3 mall          retail           \n 6 2019-09-06 15:58:00     -97.7     30.4 mall          retail           \n 7 2019-10-05 16:47:00     -97.7     30.2 mall          retail           \n 8 2019-11-01 22:23:00     -97.7     30.3 entertainment leisure          \n 9 2019-11-18 16:29:00     -97.7     30.4 mall          retail           \n10 2019-11-27 14:14:00     -97.8     30.3 mall          retail           \n\n\nIf we want to filter offences that have any one of several different values of the same column, we can use the %in% (in) operator. To filter offences that occurred in either streets or publicly accessible open spaces:\n\n\n\nR Console\n\nfilter(agg_assault_data, location_category %in% c(\"open space\", \"street\"))\n\n\n# A tibble: 165 × 5\n   date                longitude latitude location_type location_category\n   &lt;dttm&gt;                  &lt;dbl&gt;    &lt;dbl&gt; &lt;chr&gt;         &lt;chr&gt;            \n 1 2019-01-07 21:37:00     -97.8     30.5 green space   open space       \n 2 2019-01-08 14:02:00     -97.7     30.3 green space   open space       \n 3 2019-01-10 01:59:00     -97.6     30.3 green space   open space       \n 4 2019-01-11 17:00:00     -97.7     30.3 green space   open space       \n 5 2019-01-12 15:50:00     -97.8     30.2 green space   open space       \n 6 2019-01-12 16:00:00     -97.8     30.4 green space   open space       \n 7 2019-01-15 07:20:00     -97.7     30.2 green space   open space       \n 8 2019-01-17 00:00:00     -97.7     30.4 green space   open space       \n 9 2019-01-22 22:00:00     -97.7     30.2 green space   open space       \n10 2019-01-29 01:40:00     -97.7     30.2 green space   open space       \n# ℹ 155 more rows\n\n\nThe code c(\"open space\", \"street\") produces what is referred to in R as a vector (sometimes referred to as an atomic vector, especially in error messages). A vector is a one-dimensional sequence of values of the same type (i.e. all numbers, all character strings etc.). For example, a vector might hold several strings of text (as in the vector c(\"open space\", \"street\")) or a series of numbers such as c(1, 2, 3). There is lots we could learn about vectors, but for now it’s only necessary to know that we can create vectors with the c() or combine function.\nIf we wanted to re-use a vector of values several times in our code, it might make sense to store the vector as an object. For example:\n\n\n\nR Console\n\n# Create vector of location types we are interested in\nlocation_types &lt;- c(\"open space\", \"street\")\n\n# Filter the data\nfilter(agg_assault_data, location_category %in% location_types)\n\n\n# A tibble: 165 × 5\n   date                longitude latitude location_type location_category\n   &lt;dttm&gt;                  &lt;dbl&gt;    &lt;dbl&gt; &lt;chr&gt;         &lt;chr&gt;            \n 1 2019-01-07 21:37:00     -97.8     30.5 green space   open space       \n 2 2019-01-08 14:02:00     -97.7     30.3 green space   open space       \n 3 2019-01-10 01:59:00     -97.6     30.3 green space   open space       \n 4 2019-01-11 17:00:00     -97.7     30.3 green space   open space       \n 5 2019-01-12 15:50:00     -97.8     30.2 green space   open space       \n 6 2019-01-12 16:00:00     -97.8     30.4 green space   open space       \n 7 2019-01-15 07:20:00     -97.7     30.2 green space   open space       \n 8 2019-01-17 00:00:00     -97.7     30.4 green space   open space       \n 9 2019-01-22 22:00:00     -97.7     30.2 green space   open space       \n10 2019-01-29 01:40:00     -97.7     30.2 green space   open space       \n# ℹ 155 more rows\n\n\nFinally, you can filter based on the output of any R function that returns TRUE or FALSE. For example, missing values are represented in R as NA. We can test whether a value is missing using the is.na() function. If we wanted to remove rows from our data that had missing location types, we would filter for those rows that are not NA. We can do this by combining the is.na() function with the ! (not) operator:\n\n\n\nR Console\n\nfilter(agg_assault_data, !is.na(location_type))\n\n\n# A tibble: 5,344 × 5\n   date                longitude latitude location_type location_category\n   &lt;dttm&gt;                  &lt;dbl&gt;    &lt;dbl&gt; &lt;chr&gt;         &lt;chr&gt;            \n 1 2019-01-01 00:00:00     -97.7     30.3 residence     residence        \n 2 2019-01-01 00:00:00     -97.8     30.2 residence     residence        \n 3 2019-01-01 00:27:00     -97.8     30.2 residence     residence        \n 4 2019-01-01 01:00:00     -97.7     30.4 residence     residence        \n 5 2019-01-01 01:12:00     -97.7     30.3 residence     residence        \n 6 2019-01-01 01:20:00     -97.7     30.4 residence     residence        \n 7 2019-01-01 01:35:00     -97.7     30.3 hotel         hotel            \n 8 2019-01-01 01:50:00     -97.7     30.3 residence     residence        \n 9 2019-01-01 02:21:00     -97.8     30.2 residence     residence        \n10 2019-01-01 02:26:00     -97.8     30.2 residence     residence        \n# ℹ 5,334 more rows\n\n\nWe will see lots more examples of how to use filter() in future chapters.\n\n\n\n\n\n\nfilter()\n\n\n\nWhat is a vector (sometimes known as an atomic vector) in R?\n\n A type of object that stores a tibble or data frame A type of object that stores a one-dimensional sequence of values of the same type A type of object that stores a one-dimensional sequence of values that can be of different types There is no such thing as a vector in R\n\nWhich offences (rows) will be returned by the code filter(agg_assault_data, location_type %in% c(\"restaurant\", \"mall\"))?\n\n Offences that occurred in both restaurants and in shopping malls (e.g. at restaurants inside shopping malls) Offences that occurred anywhere except restaurants or shopping malls (e.g. in homes) No offences, because the way the two criteria are combined is illogical Offences that occurred either in restaurants or in shopping malls\n\nWhat does the &lt;= operator mean?\n\n greater than less than less than or equal to greater than or equal to",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Wrangling data</span>"
    ]
  },
  {
    "objectID": "03_data_wrangling/index.html#transforming-values",
    "href": "03_data_wrangling/index.html#transforming-values",
    "title": "3  Wrangling data",
    "section": "3.5 Transforming values",
    "text": "3.5 Transforming values\nIt is often useful to create new columns in our data, or change the values of existing columns. The mutate() function in the dplyr package gives us a way to transform existing columns in our dataset using almost any R function.\n\n\n\n\nFor example, say we wanted to create a new column in our aggravated-assault dataset specifying the day of the week on which each crime occurred. We can do this using the wday() function from the lubridate package (using the label = TRUE argument to produce weekday names, rather than numbers):\n\n\n\nR Console\n\nmutate(agg_assault_data, weekday = wday(date, label = TRUE))\n\n\n# A tibble: 8,696 × 6\n   date                longitude latitude location_type location_category\n   &lt;dttm&gt;                  &lt;dbl&gt;    &lt;dbl&gt; &lt;chr&gt;         &lt;chr&gt;            \n 1 2019-01-01 00:00:00     -97.7     30.3 residence     residence        \n 2 2019-01-01 00:00:00     -97.8     30.2 residence     residence        \n 3 2019-01-01 00:01:00     -97.7     30.3 &lt;NA&gt;          &lt;NA&gt;             \n 4 2019-01-01 00:15:00     -97.7     30.3 &lt;NA&gt;          &lt;NA&gt;             \n 5 2019-01-01 00:27:00     -97.8     30.2 residence     residence        \n 6 2019-01-01 00:30:00     -97.7     30.3 &lt;NA&gt;          &lt;NA&gt;             \n 7 2019-01-01 00:51:00     -97.7     30.3 &lt;NA&gt;          &lt;NA&gt;             \n 8 2019-01-01 01:00:00     -97.7     30.4 residence     residence        \n 9 2019-01-01 01:00:00     -97.7     30.3 &lt;NA&gt;          &lt;NA&gt;             \n10 2019-01-01 01:12:00     -97.7     30.3 residence     residence        \n# ℹ 8,686 more rows\n# ℹ 1 more variable: weekday &lt;ord&gt;\n\n\nWe can also change existing columns. However (as with objects) there is no way to undo this, so you should only replace columns if you are sure you will not need them. For example, if we wanted to remove the time portion of the date variable (which may sometimes be useful, as shown in the next section) using the as_date() function (also from the lubridate package) and at the same time create the weekday variable:\n\n\n\nR Console\n\nmutate(\n  agg_assault_data, \n  date = as_date(date),\n  weekday = wday(date, label = TRUE)\n)\n\n\n# A tibble: 8,696 × 6\n   date       longitude latitude location_type location_category weekday\n   &lt;date&gt;         &lt;dbl&gt;    &lt;dbl&gt; &lt;chr&gt;         &lt;chr&gt;             &lt;ord&gt;  \n 1 2019-01-01     -97.7     30.3 residence     residence         Tue    \n 2 2019-01-01     -97.8     30.2 residence     residence         Tue    \n 3 2019-01-01     -97.7     30.3 &lt;NA&gt;          &lt;NA&gt;              Tue    \n 4 2019-01-01     -97.7     30.3 &lt;NA&gt;          &lt;NA&gt;              Tue    \n 5 2019-01-01     -97.8     30.2 residence     residence         Tue    \n 6 2019-01-01     -97.7     30.3 &lt;NA&gt;          &lt;NA&gt;              Tue    \n 7 2019-01-01     -97.7     30.3 &lt;NA&gt;          &lt;NA&gt;              Tue    \n 8 2019-01-01     -97.7     30.4 residence     residence         Tue    \n 9 2019-01-01     -97.7     30.3 &lt;NA&gt;          &lt;NA&gt;              Tue    \n10 2019-01-01     -97.7     30.3 residence     residence         Tue    \n# ℹ 8,686 more rows\n\n\nYou may sometimes want to change only some values in a column. With a categorical variable, we can change one value to another using the case_match() function from the dplyr package. Look at this code and use the comments (lines starting with #) to understand how it works.\n\n\n\nR Console\n\nmutate(\n  agg_assault_data,\n  location_category = case_match(\n    location_category, \n    # We specify which existing values we want to convert into which new values\n    # by providing the existing value on the left-hand side and the new value\n    # on the right hand side, separated by a tilde (~) character\n    \"open space\" ~ \"public open space\",\n    \"street\" ~ \"street or road\",\n    # If we want to keep the existing values for any values we have not\n    # specified, we can do that using the .default argument\n    .default = location_category\n  )\n)\n\n\n# A tibble: 8,696 × 5\n   date                longitude latitude location_type location_category\n   &lt;dttm&gt;                  &lt;dbl&gt;    &lt;dbl&gt; &lt;chr&gt;         &lt;chr&gt;            \n 1 2019-01-01 00:00:00     -97.7     30.3 residence     residence        \n 2 2019-01-01 00:00:00     -97.8     30.2 residence     residence        \n 3 2019-01-01 00:01:00     -97.7     30.3 &lt;NA&gt;          &lt;NA&gt;             \n 4 2019-01-01 00:15:00     -97.7     30.3 &lt;NA&gt;          &lt;NA&gt;             \n 5 2019-01-01 00:27:00     -97.8     30.2 residence     residence        \n 6 2019-01-01 00:30:00     -97.7     30.3 &lt;NA&gt;          &lt;NA&gt;             \n 7 2019-01-01 00:51:00     -97.7     30.3 &lt;NA&gt;          &lt;NA&gt;             \n 8 2019-01-01 01:00:00     -97.7     30.4 residence     residence        \n 9 2019-01-01 01:00:00     -97.7     30.3 &lt;NA&gt;          &lt;NA&gt;             \n10 2019-01-01 01:12:00     -97.7     30.3 residence     residence        \n# ℹ 8,686 more rows\n\n\nWe could also make changes based on more-complicated sets of criteria using the case_when() function, but we will return to that in a future chapter.\nThe R functions that you use inside mutate() must return the same number of values as there are rows in the dataset. This is true for most R functions (which are referred to as vectorised functions), but there are some – such as mean() and max() – that return a single value. These summarising functions cannot be used inside mutate() (you will see an error message if you try) but are instead used with the next data-wrangling function we will learn about: summarise().",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Wrangling data</span>"
    ]
  },
  {
    "objectID": "03_data_wrangling/index.html#summarising-rows",
    "href": "03_data_wrangling/index.html#summarising-rows",
    "title": "3  Wrangling data",
    "section": "3.6 Summarising rows",
    "text": "3.6 Summarising rows\nSummarising data is often useful in crime analysis. We can use the summarise() function from the dplyr package to produce summaries of different columns in our data. There is an identical function called summarize() so that you do not have to remember whether to use the US or British spelling.\nBy default, summarise() collapses data into a single row, with each column summarised using a function that you specify. For example, suppose you want to find out which police station a specialist squad should be based at to most easily respond to reports of serious assaults. You might do this by working out the weighted centre of all the offence locations, i.e. the means of the longitudes and latitudes for all the crimes. You could then base the squad at the police station that was closest to the weighted centre.\n\n\n\nR Console\n\nsummarise(\n  agg_assault_data, \n  mean_lng = mean(longitude, na.rm = TRUE),\n  mean_lat = mean(latitude, na.rm = TRUE)\n)\n\n\n# A tibble: 1 × 2\n  mean_lng mean_lat\n     &lt;dbl&gt;    &lt;dbl&gt;\n1    -97.7     30.3\n\n\n\n\n\n\n\n\nWhat does the argument na.rm = TRUE do?\n\n\n\n\n\nLots of functions in R have an argument called na.rm that can be set to either TRUE or FALSE. Setting na.rm = TRUE in this case specifies that the mean() function should remove (rm) any missing (NA) values before calculating the mean.\nIf we do not specify this and our data contain any missing values, the mean() function will return NA. Functions in R do this because it is not possible to completely answer the question ‘what is the mean of these values?’ if some of the values are missing.\nThis logic applies in lots of cases. For example, if you create an R object called value with the code value &lt;- 2 and then run the R code value &gt; 1, you will get the answer TRUE. But if you set the object value to be NA using the code value &lt;- NA, when you run the R code value &gt; 1 you will get the answer NA. This is because there is no way to know if the missing value represented by NA is greater than 1 or not. This is why it is often useful to calculate statistics such as a mean value after removing any missing values using the na.rm = TRUE argument.\n\n\n\nsummarise() becomes more useful if we first divide our data into groups, since we then get a summary for each group separately. We can use the .by argument of the summarise() function to specify that we want separate summaries for each unique value of one or more columns in the data. For example, to produce a separate summary for each unique value of location_category, we can use this code:\n\n\n\nR Console\n\nsummarise(\n  agg_assault_data,\n  mean_lng = mean(longitude, na.rm = TRUE),\n  mean_lat = mean(latitude, na.rm = TRUE),\n  .by = location_category\n)\n\n\n# A tibble: 12 × 3\n   location_category mean_lng mean_lat\n   &lt;chr&gt;                &lt;dbl&gt;    &lt;dbl&gt;\n 1 residence            -97.7     30.3\n 2 &lt;NA&gt;                 -97.7     30.3\n 3 hotel                -97.7     30.3\n 4 other                -97.7     30.3\n 5 retail               -97.7     30.3\n 6 healthcare           -97.7     30.3\n 7 open space           -97.7     30.3\n 8 commercial           -97.7     30.3\n 9 education            -97.7     30.3\n10 government           -97.7     30.3\n11 leisure              -97.7     30.4\n12 transportation       -97.7     30.2\n\n\nYou can add multiple grouping variables using the c() (combine) function if you want to generate summary values for groups within groups:\n\n\n\nR Console\n\nsummarise(\n  agg_assault_data,\n  mean_lng = mean(longitude, na.rm = TRUE),\n  mean_lat = mean(latitude, na.rm = TRUE),\n  .by = c(location_category, location_type)\n)\n\n\n# A tibble: 24 × 4\n   location_category location_type     mean_lng mean_lat\n   &lt;chr&gt;             &lt;chr&gt;                &lt;dbl&gt;    &lt;dbl&gt;\n 1 residence         residence            -97.7     30.3\n 2 &lt;NA&gt;              &lt;NA&gt;                 -97.7     30.3\n 3 hotel             hotel                -97.7     30.3\n 4 other             other                -97.7     30.3\n 5 retail            other retail         -97.7     30.3\n 6 healthcare        healthcare           -97.7     30.3\n 7 retail            mall                 -97.8     30.3\n 8 retail            convenience store    -97.7     30.3\n 9 open space        green space          -97.7     30.3\n10 commercial        office               -97.7     30.3\n# ℹ 14 more rows\n\n\n\n3.6.1 Counting rows\nOne very common way of summarising data is to count the number of rows in a dataset that have each unique value of one or more columns. For example, if we have a dataset of crimes in which each row represents a single crime, we might want to count how many crimes happened on each day of the week, or how many crimes of each type are in the dataset. We can use summarise() to do that, together with the n() function (from the same dplyr package as summarise()). For example, if we wanted to count how many rows in the agg_assault_data dataset had each unique combination of location_category and location_type:\n\n\n\nR Console\n\nsummarise(agg_assault_data, n = n(), .by = c(location_category, location_type))\n\n\n# A tibble: 24 × 3\n   location_category location_type         n\n   &lt;chr&gt;             &lt;chr&gt;             &lt;int&gt;\n 1 residence         residence          4385\n 2 &lt;NA&gt;              &lt;NA&gt;               3352\n 3 hotel             hotel               248\n 4 other             other               205\n 5 retail            other retail         30\n 6 healthcare        healthcare           36\n 7 retail            mall                 11\n 8 retail            convenience store    90\n 9 open space        green space         165\n10 commercial        office               28\n# ℹ 14 more rows\n\n\nIn this code, the n() function simply returns the number of rows of data in each group, i.e. the number of rows with each unique combination of values of location_category and location_type.\nSince counting the number of rows in each group a dataset is a very common task, dplyr includes another function called count() that allows you to do the same thing as in the code above, but with slightly less typing:\nSo if you wanted to know how many aggravated assaults had occurred in each location category and type, you could this code instead of using summarise():\n\n\n\nR Console\n\ncount(agg_assault_data, location_category, location_type)\n\n\n# A tibble: 24 × 3\n   location_category location_type         n\n   &lt;chr&gt;             &lt;chr&gt;             &lt;int&gt;\n 1 commercial        construction         11\n 2 commercial        factory/warehouse     2\n 3 commercial        finance               1\n 4 commercial        office               28\n 5 commercial        storage               4\n 6 education         child care           11\n 7 education         college               5\n 8 education         school               25\n 9 government        government           18\n10 healthcare        healthcare           36\n# ℹ 14 more rows\n\n\nNote that the first argument of count() is the dataset, and every subsequent argument specifies another variable that should be used to group the data.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Wrangling data</span>"
    ]
  },
  {
    "objectID": "03_data_wrangling/index.html#arranging-rows",
    "href": "03_data_wrangling/index.html#arranging-rows",
    "title": "3  Wrangling data",
    "section": "3.7 Arranging rows",
    "text": "3.7 Arranging rows\nIt is sometimes useful to be able to place rows in a dataset into a particular order. We can do this using the arrange() function from the dplyr package. For example, we can sort the aggravated-assault data by date:\n\n\n\nR Console\n\narrange(agg_assault_data, date)\n\n\n# A tibble: 8,696 × 5\n   date                longitude latitude location_type location_category\n   &lt;dttm&gt;                  &lt;dbl&gt;    &lt;dbl&gt; &lt;chr&gt;         &lt;chr&gt;            \n 1 2019-01-01 00:00:00     -97.7     30.3 residence     residence        \n 2 2019-01-01 00:00:00     -97.8     30.2 residence     residence        \n 3 2019-01-01 00:01:00     -97.7     30.3 &lt;NA&gt;          &lt;NA&gt;             \n 4 2019-01-01 00:15:00     -97.7     30.3 &lt;NA&gt;          &lt;NA&gt;             \n 5 2019-01-01 00:27:00     -97.8     30.2 residence     residence        \n 6 2019-01-01 00:30:00     -97.7     30.3 &lt;NA&gt;          &lt;NA&gt;             \n 7 2019-01-01 00:51:00     -97.7     30.3 &lt;NA&gt;          &lt;NA&gt;             \n 8 2019-01-01 01:00:00     -97.7     30.4 residence     residence        \n 9 2019-01-01 01:00:00     -97.7     30.3 &lt;NA&gt;          &lt;NA&gt;             \n10 2019-01-01 01:12:00     -97.7     30.3 residence     residence        \n# ℹ 8,686 more rows\n\n\nBy default, arrange() sorts rows in ascending order, i.e. it sorts numeric values from the smallest to the largest, dates from earliest to latest and character values alphabetically. We can instead sort values in descending order by wrapping the name of a column in the desc() function:\n\n\n\nR Console\n\narrange(agg_assault_data, desc(date))\n\n\n# A tibble: 8,696 × 5\n   date                longitude latitude location_type location_category\n   &lt;dttm&gt;                  &lt;dbl&gt;    &lt;dbl&gt; &lt;chr&gt;         &lt;chr&gt;            \n 1 2019-12-31 23:33:00     -97.7     30.4 residence     residence        \n 2 2019-12-31 23:26:00     -97.7     30.4 residence     residence        \n 3 2019-12-31 23:20:00     -97.7     30.3 &lt;NA&gt;          &lt;NA&gt;             \n 4 2019-12-31 23:19:00     -97.7     30.3 &lt;NA&gt;          &lt;NA&gt;             \n 5 2019-12-31 22:39:00     -97.7     30.3 other         other            \n 6 2019-12-31 22:29:00     -97.7     30.3 residence     residence        \n 7 2019-12-31 21:13:00     -97.8     30.2 residence     residence        \n 8 2019-12-31 20:52:00     -97.7     30.3 residence     residence        \n 9 2019-12-31 20:44:00     -97.7     30.3 residence     residence        \n10 2019-12-31 20:19:00     -97.7     30.3 &lt;NA&gt;          &lt;NA&gt;             \n# ℹ 8,686 more rows\n\n\nWe can also sort the data based on multiple columns – the data are sorted first on the first column that you specify, with tied rows then sorted on the subsequent columns in order.\n\n\n\nR Console\n\narrange(agg_assault_data, date, desc(location_type), location_category)\n\n\n# A tibble: 8,696 × 5\n   date                longitude latitude location_type location_category\n   &lt;dttm&gt;                  &lt;dbl&gt;    &lt;dbl&gt; &lt;chr&gt;         &lt;chr&gt;            \n 1 2019-01-01 00:00:00     -97.7     30.3 residence     residence        \n 2 2019-01-01 00:00:00     -97.8     30.2 residence     residence        \n 3 2019-01-01 00:01:00     -97.7     30.3 &lt;NA&gt;          &lt;NA&gt;             \n 4 2019-01-01 00:15:00     -97.7     30.3 &lt;NA&gt;          &lt;NA&gt;             \n 5 2019-01-01 00:27:00     -97.8     30.2 residence     residence        \n 6 2019-01-01 00:30:00     -97.7     30.3 &lt;NA&gt;          &lt;NA&gt;             \n 7 2019-01-01 00:51:00     -97.7     30.3 &lt;NA&gt;          &lt;NA&gt;             \n 8 2019-01-01 01:00:00     -97.7     30.4 residence     residence        \n 9 2019-01-01 01:00:00     -97.7     30.3 &lt;NA&gt;          &lt;NA&gt;             \n10 2019-01-01 01:12:00     -97.7     30.3 residence     residence        \n# ℹ 8,686 more rows\n\n\n\n\n\n\n\n\nArranging rows in data\n\n\n\nIn the R Console, type the code necessary to arrange agg_assault_data in order of latitude, in descending order (from largest to smallest). Once you’ve tried writing the code, if you need help you can click the ‘Solution’ button below.\n\n\nSolution\n\narrange(agg_assault_data, desc(latitude))",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Wrangling data</span>"
    ]
  },
  {
    "objectID": "03_data_wrangling/index.html#saving-data",
    "href": "03_data_wrangling/index.html#saving-data",
    "title": "3  Wrangling data",
    "section": "3.8 Saving data",
    "text": "3.8 Saving data\nOnce we have finished wrangling a particular dataset, it is often useful to save it to a file so that we can use it again in future without going through all the steps of data wrangling again.\nMost R functions that begin with read_ (like read_csv() and read_excel()) have equivalent functions that begin write_ and which save data into a particular file format. In this example, we will use the write_csv() function from the readr package, which is loaded when we load the tidyverse package.\nJust as read_csv() can read files from a variety of different locations, so write_csv() can write files to different locations. Look at this code and use the comments (lines beginning with #) to understand the different options for saving data.\n\n\n\nR Console\n\n# We can write data to a file in the same folder as our R script\nwrite_csv(agg_assault_data, \"fort_worth_agg_assault.csv\")\n\n# Or another folder on your computer ('../../' is short for the parent folder of\n# the parent folder of the current folder)\nwrite_csv(agg_assault_data, \"../../fort_worth_agg_assault.csv\")\n\n\nThere are corresponding write functions for other types of data (which we will come back to when we learn how to handle spatial data), but in this course we will store all non-spatial data in CSV format because it can be read by many different programs.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Wrangling data</span>"
    ]
  },
  {
    "objectID": "03_data_wrangling/index.html#sec-pipe-operator",
    "href": "03_data_wrangling/index.html#sec-pipe-operator",
    "title": "3  Wrangling data",
    "section": "3.9 Stringing functions together",
    "text": "3.9 Stringing functions together\nIn this chapter we have learned how to use the dplyr functions select(), filter(), mutate(), summarise() and arrange() to wrangle data from one format to another. Data wrangling is part of almost all data analysis, so these are skills we will use frequently.\nData wrangling often involves multiple steps. For example, we might want to load some data, select certain columns, filter some rows, mutate some of the variables, summarise the dataset and save the result. We can do each of these steps separately, assigning the result of each step to a new object.\n\n\n\nchapter_03a.R\n\n# Select only the columns we need\nrobbery2 &lt;- select(san_fran_rob, date_time)\n\n# Filter only those offences that occurred in the first quarter of 2019\nrobbery3 &lt;- filter(robbery2, as_date(date_time) &lt;= ymd(\"2019-03-31\"))\n\n# Create a new weekday variable\nrobbery4 &lt;- mutate(robbery3, weekday = wday(date_time, label = TRUE))\n\n# Count how many offences occurred on each weekday\nq1_weekday_counts &lt;- count(robbery4, weekday)\n\n\nThis code works, but involves creating six new objects, even though we only need the final q1_weekday_counts object for our analysis. You may notice that the first argument expected by select(), filter(), mutate() and count() is always the data tibble produced by the previous step. For example, the filter() function on line 5 of this code uses the robbery2 object created by line 2 of the code.\nWhenever you write code that (a) consists of several sequential steps, and (b) each step uses the data produced by the previous step, there is a better way to do it. This method uses the |&gt; (or pipe) operator. The pipe operator works by using the result of the code on the left-hand side of the pipe as the first argument to a function on the right-hand side. So the code x  |&gt; fun1() |&gt; fun2() starts with an object called x, uses that as the input to the function fun1(), then passes the result produced by fun1() and uses that as the input to fun2().\nIt may be useful to read the pipe operator as ‘and then’, since piped code does the first thing and then the second thing with the result and then the third thing with the result of that, and so on. Piped code (sometimes called a pipeline) is a lot like the series of steps in a recipe.\nDelete all the code from the file chapter_03a.R, since we are now going to replace it with a piped version of the same code. This pipeline of code is better because it is shorter than the previous code, is easier to maintain and (once you get used to the pipeline) is easier to understand.\n\n\n\nchapter_03a.R\n\n# Load packages\npacman::p_load(tidyverse)\n\n# Load data\nsan_fran_rob &lt;- read_csv(\"https://mpjashby.github.io/crimemappingdata/san_francisco_robbery.csv\")\n\n# Produce counts of robberies each weekday\nq1_weekday_counts &lt;- san_fran_rob |&gt; \n  select(-offense_type) |&gt; \n  filter(as_date(date_time) &lt;= ymd(\"2019-03-31\")) |&gt; \n  mutate(weekday = wday(date_time, label = TRUE)) |&gt; \n  count(weekday)\n\n\n\n\nRows: 951 Columns: 5\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (1): offense_type\ndbl  (3): uid, longitude, latitude\ndttm (1): date_time\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nYou might not find the pipe operator completely intuitive at the moment, but it will become easier as you see more examples in future chapters.\n\n\n\n\n\n\nWhat about the %&gt;% pipe operator?\n\n\n\n\n\nIf you have learned any R coding before, you might have learned to use the %&gt;% pipe operator from the magrittr package. The %&gt;% pipe operator was introduced several years ago to allow people to construct pipelines of code in R. The %&gt;% operator was so widely used that the team that writes the R programming language decided to provide the |&gt; pipe operator in R itself, to avoid the need to load the magrittr package. You might sometimes see the |&gt; referred to as the native pipe operator.\nYou will still see the %&gt;% pipe operator used in lots of R code examples online. In almost all cases, when you see %&gt;% you can replace it with the R pipe operator |&gt;, since they both work in very similar ways.\n\n\n\nSave the chapter_03a.R file, then restart R to start a new session by clicking on the Session menu and then clicking Restart R. This creates a blank canvas for the next chapter.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Wrangling data</span>"
    ]
  },
  {
    "objectID": "03_data_wrangling/index.html#in-summary",
    "href": "03_data_wrangling/index.html#in-summary",
    "title": "3  Wrangling data",
    "section": "3.10 In summary",
    "text": "3.10 In summary\nIn this chapter, you have learned how to wrangle data in R using functions from packages in the tidyverse suite of packages. You can now construct a complete pipeline of R code to take raw data and transform it into the format(s) we need to effectively map crimes.\n\nDeveloping your data wrangling skills will help you to produce better, faster analysis of crime (and other) data. If you would like to develop your skills further, you might be interested in:\n\nData Wrangling with R by Claudia Engel, a free online book that explains the functions introduced in this tutorial (and some others) in more detail.\nData transformation with dplyr cheat sheet by the team that makes RStudio, which provides a handy two-page guide to the main functions in the dplyr package, which is very useful for reminding you of the code needed to run each of the functions we have used in this tutorial.\nData wrangling with R and RStudio by Garrett Grolemund, a recording of a webinar covering the data-wrangling functions introduced in this tutorial and some other very-useful related functions.\nR for Data Science by Hadley Wickham and Garrett Grolemund, a free online book that is the bible for wrangling data in R.\n\n\n\n\n\n\n\n\nRevision questions\n\n\n\nAnswer these questions to check you have understood the main points covered in this chapter. Write between 50 and 100 words to answer each question.\n\nWhat is data wrangling, and why is it an important step in data analysis?\nDescribe the difference between loading data from a CSV file and an Excel file in R. What additional steps are required when working with Excel files?\nWhat is the purpose of the select() function in the dplyr package? How can it be used to both choose and rename columns? Include a brief example in your explanation.\nExplain why should should never include the function install.packages() in an R script file.\nWhat are the benefits of using the pipe (|&gt;) operator in R?\n\n\n\n\nArtwork by @allison_horst",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Wrangling data</span>"
    ]
  },
  {
    "objectID": "04_your_second_crime_map/index.html",
    "href": "04_your_second_crime_map/index.html",
    "title": "4  Your second crime map",
    "section": "",
    "text": "4.1 Introduction\nIn Chapter 2 we produced a simple map of a particular type of crime in a neighbourhood, but we skipped over a lot of the details of how to do it. In this chapter we will make another crime map, this time focusing more on each step in the process. We will then build on this in Chapter 6 to make a better crime map.\nIn this chapter (and Chapter 6) we will make a map of bicycle thefts in Vancouver in 2020. At the end of Chapter 6, the final map will look something like this.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Your second crime map</span>"
    ]
  },
  {
    "objectID": "04_your_second_crime_map/index.html#handling-spatial-data",
    "href": "04_your_second_crime_map/index.html#handling-spatial-data",
    "title": "4  Your second crime map",
    "section": "4.2 Handling spatial data",
    "text": "4.2 Handling spatial data\nMaps are visual representations of spatial data. Spatial data is special because each row in the data is associated with some geographic feature such as a building, a street or the boundary of a neighbourhood. This adds some quirks that we have to understand to work with spatial data successfully.\nMaps are made up of multiple layers of spatial data that are styled to represent features of interest and then stacked on top of one another to make the finished map. Watch this video to learn more about spatial layers and the different types of data that we can use in maps.\n\n\n\n\n\n\n\nQuiz\n\n\n\nWhich of the following best describes the three ways computers store spatial features?\n\n Points, polygons, and events Points, lines, and polygons Layers, rasters, and vectors Features, base maps, and grid cells\n\nWhat is the purpose of a base map in mapping?\n\n To provide a single layer showing all features on a map To allow individual access to points, lines, and polygons To replace all other map layers To simplify data by combining and styling different features\n\nWhat happens when too much detail is included on a map?\n\n The map becomes easier to interpret The map’s layers automatically simplify The data becomes harder to work with and visualize effectively The map’s scale adjusts to include all details\n\nWhat is a raster layer?\n\n A dataset storing only lines and polygons A grid of cells that simplifies data by representing features with single values A detailed layer that preserves all original data A stack of feature layers\n\nWhy is it common to store data about different types of features in separate files?\n\n Most files cannot handle large amounts of data It is easier to create maps with fewer data files In most spatial data formats, each file is limited to storing points, lines, or polygons, not all three Storing all features in one file makes maps less detailed\n\n\n\nPoints, lines and polygons in spatial data are known as geometric objects or simply geometries. Spatial data is data that has a geometric object (e.g. a pair of co-ordinates representing a crime location) associated with each row.\n\n4.2.1 Representing places on the earth\nWith any spatial data, we need a way of describing where on the earth a particular point (such as the location of a crime or the corner of a building) is located. Watch this video to find out about the different co-ordinate systems we can use to do this.\n\n\n\n\n\n\n\nQuiz\n\n\n\nWhat are co-ordinates used for?\n\n To measure the distance between two locations To specify a location on the Earth’s surface relative to a reference point To calculate the area of a geographic region To determine the altitude of a point on the Earth\n\nWhat is the geographic co-ordinate system based on?\n\n Latitude and altitude Longitude and elevation Latitude and grid distances Latitude and longitude\n\nWhy are degrees of latitude and longitude difficult to use for everyday measurements?\n\n They are too small to measure accurately They are not precise enough for spatial analysis They do not correspond to easily understandable units like metres They only work in polar regions\n\nWhat can happen if you use a co-ordinate system designed for a different part of the globe?\n\n The map may have errors or show distorted features The map will automatically adjust to the correct region The dataset will not load The co-ordinates will convert to the nearest global standard\n\nWhat is a practical way to find out which co-ordinate system a dataset uses if it’s not embedded in the file?\n\n Guess based on the region it represents Check the size of the dataset file Ask the person or organization that provided the data Compare it with another dataset from a different region",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Your second crime map</span>"
    ]
  },
  {
    "objectID": "04_your_second_crime_map/index.html#spatial-data-in-r",
    "href": "04_your_second_crime_map/index.html#spatial-data-in-r",
    "title": "4  Your second crime map",
    "section": "4.3 Spatial data in R",
    "text": "4.3 Spatial data in R\n\n\n\nThere are several packages that handle raster map data from different sources – one of them is the ggspatial package that we have already used to load the base map of Atlanta for the homicide map we made in Chapter 2.\n\nVector data can be handled in R using functions from the sf package. SF stands for ‘simple features’, which is a standard for storing spatial data. SF objects are data frames that have a special column to hold the geometry (point, line or polygon) associated with each row in the data. SF objects also understand what co-ordinate system the geometry are described in. This means SF objects can be transformed between co-ordinate systems and combined together in layers on a map.\nThere a lots of functions in the sf package for handling spatial data. Almost all of these functions begin with the letters st_ (e.g. st_read()), which makes it easy to identify that those functions are designed to be used on SF objects.\n\n\n\n\n\n\nFunctions in the sf package start with the letters st_\n\n\n\nThe names of almost all functions in the sf package start with the letters st_, not the letters sf_.\n\n\n\n4.3.1 Reading spatial data\nTo get started, open a blank R script file (using File &gt; New File &gt; R Script), then save it with the name chapter_04.R. Now add the code needed to load the sf and tidyverse packages:\n\n\n\nchapter_04.R\n\n# Load packages\npacman::p_load(sf, tidyverse)\n\n\nThe special features of spatial data – needing to store geometries, details of the projection used etc. – mean that spatial data is often stored in special file formats. There are lots of spatial-data formats, but fortunately almost all of them can be read by the st_read() function. This means we do not need to learn a different function for each spatial-data format.\nWhile datasets with line or polygon geometries must almost always be stored in specific spatial-data formats, point data can also be stored in common data formats such as Excel and CSV files. The data for this chapter is provided by the Vancouver Police Department in a CSV file (gzipped to reduce the file size). The file is located at:\nhttps://mpjashby.github.io/crimemappingdata/vancouver_thefts.csv.gz\n\n\n\n\n\n\nWhat does the .gz at the end of this URL mean?\n\n\n\n\n\nCSV files can hold very large amounts of data, but at the cost of the size of the file becoming very large. We can reduce the size of a CSV file (or most other types of file) by compressing the file. You may be familiar with .zip compressed files. Gzip is another way of compressing files. Gzipped files have the file extension .gz. read_csv() can automatically decompress gzipped files, so we can treat a gzipped CSV file just the same as an uncompressed CSV file.\n\n\n\n\n\n\n\n\n\nLoading data from a CSV file\n\n\n\nThinking back to what we learned in Section 3.2, add a new line of code to the file chapter_04.R to load this dataset and store it in an object called thefts. If you need help, you can click the ‘Solution’ button below.\n\n\nSolution\n\nSince the data are stored in a regular CSV file, we can use the read_csv() function from the readr package to read the file, and the assignment operator &lt;- to store the data in the object thefts. read_csv() can read directly from a URL, so there is no need to download the data first.\n\n\n\nchapter_04.R\n\nthefts &lt;- read_csv(\"https://mpjashby.github.io/crimemappingdata/vancouver_thefts.csv.gz\")\n\n\n\n\n\n\n\n\n\n\n\nViewing the first few rows of a dataset\n\n\n\nNow that you have stored the data in the thefts object, what code is needed to view the first few rows of data? Type the code into the R Console and press Enter/Return to run it. Click the ‘Solution’ button to check your code.\n\n\nSolution\n\n\n\n\nR Console\n\nhead(thefts)\n\n\n\n\n\n\n\n4.3.2 Reading data files from your computer\nThe data consists of 21,918 rows, each representing one theft. Before we can map this data, we will need to do some minor data wrangling to get it into the format we want.\nSo far we have created datasets by loading data directly from URLs, such as the URL for the Vancouver thefts data we loaded in the previous section. But we can also load datasets that are already stored on our own computers. To do this, we need to know the file path that specifies where a particular file is stored.\nYou might have encountered file paths before, but you may not. A typical file path looks like this on a Mac or Linux computer:\n\n\n\nDo not run this code\n\n/Users/john_smith/Documents/Crime Mapping/vancouver_thefts.csv\n\n\nor like this on Windows:\n\n\n\nDo not run this code\n\nC:\\Users\\john_smith\\Documents\\Crime Mapping\\vancouver_thefts.csv\n\n\nThe important thing to note here is that computers store files such as vancouver_thefts.csv in folders (also called directories), which are themselves often stored inside larger folders, etc. A file path tells a computer where to find a particular file. The file paths above can be read as telling the computer to open the Users directory, then the john_smith directory, then the Documents directory, then the Crime Mapping directory, and finally the file vancouver_thefts.csv.\nYou can find the file path of a particular file by typing file.choose() in the R console. This will open a new window that allows you to search or browse for a particular file. When you choose the file, R will print the file path in the console.\nThe two file paths shown above are called absolute file paths, because they show the full location of a particular file on the computer. But there are two problems with absolute paths: they can be very long so they clutter up your code, and (more importantly) they are only correct for a specific computer. If you write an R script that includes either of the file paths above, then you give that file for me to run, the code will immediately produce an error because there is no directory /Users/john_smith on my computer.\nWe can deal with that problem in a few ways. The first is to use a relative path. This specifies the location of a file not on the computer as a whole, but relative to the directory we are currently working in. This working directory will depend on how your version of RStudio is configured, but you can find out the working directory by typing getwd() in the R console. This will print the absolute path of the working directory for whatever you are currently working on in RStudio.\nImagine our working directory is /Users/john_smith/Documents/Crime Mapping/. If we wanted to open the file /Users/john_smith/Documents/Crime Mapping/vancouver_thefts.csv, we could use the absolute file path:\n\n\n\nDo not run this code\n\nread_csv(\"/Users/john_smith/Documents/Crime Mapping/vancouver_thefts.csv\")\n\n\nHowever, since the file is stored in our current working directory, we could also open the file like this:\n\n\n\nDo not run this code\n\nread_csv(\"vancouver_thefts.csv\")\n\n\nThis works because when we provide a relative path (e.g. one that does not begin with / on Mac or C:\\ on Windows), R treats the path as being relative to the current working directory. Since the file vancouver_thefts.csv is in the working directory, we don’t need to specify anything other than the file name.\nIf we wanted to read from a file that was in a directory within the working directory – e.g. /Users/john_smith/Documents/Crime Mapping/original_data/canada/vancouver_crime.gpkg – then we just need to specify where the file is relative to the working directory:\n\n\n\nDo not run this code\n\nread_sf(\"original_data/canada/vancouver_crime.gpkg\")\n\n\n\n\n\n\n\n\nImportant\n\n\n\nFile paths in your code can make it harder to make sure your code is portable, i.e. that it can be used by other people. This is because the file structure on someone else’s computer is likely to be different from the file structure on your computer.\nYou might think code portability doesn’t matter, because you are not planning to share your code with anyone. But it’s quite possible that you will want to:\n\nRe-use code yourself on a different computer, e.g. if you replace your current computer with a new one.\nSend the code to someone else to get help with a problem.\nSend the code to someone else who has asked for help from you.\n\nTo help keep your code portable:\n\nWhere possible, include in your code any code needed to download the necessary data from the internet. This is what we most-often do in this course.\nIf the data is not available online, you will need to distribute the data along with your code. In that case, tell anyone you send you code to that they should keep the code file and data files in the same directory, then you can just refer to each data file by its name (e.g. read_csv(\"vancouver_thefts.csv\")).\nAvoid using absolute file paths in your code, since these will almost certainly produce an error if anyone tries to run your code on another computer.\n\n\n\n\n\n4.3.3 Cleaning column names\n\n\n\nTypos are one of the most frequent causes of errors in any coding language. One way to avoid typos is to follow some basic rules when we write code. We will learn more about these rules in Chapter 5, but for now we can just learn one rule: column names in datasets should use what is known as snake_case, i.e. they should be all lower case and words should be separated by underscore characters (_). This makes your code easier to read and means you don’t have to remember whether you called a column crime_count, crimecount, CrimeCount or CRIMECOUNT.\n\nAt the moment, the column names in the thefts dataset are upper-case letters. Rather than having to remember this, we can easily convert them to snake case using the clean_names() function from the janitor package. To use a function from a package, we usually first load the package using the pacman::p_load() function. In this case, we probably won’t want to use any other functions from the janitor package, so instead of loading the whole package we will use this one function directly. To do this, we write the function name with the package name added to the front, separated by two colons ::.\n\n\n\nchapter_04.R\n\nthefts &lt;- janitor::clean_names(thefts)\n\n\nAdd the code above to the chapter_04.R file and run that line of code. If you now run head(thefts) in the R Console, you will see that the data has stayed the same but all the column names are now in snake case. clean_names() would also have replaced any spaces with underscores, tried to separate words in the variable names and cleaned up several other potential problems. For this reason it is common to call janitor::clean_names() straight away after loading a dataset so that you can be confident that the column names will be in the format you expect.\nIf we wanted to use the clean_names() function again, we would have to include the package name and :: each time, so if our code was going to make repeated use of the function then it would probably be easier to load the package using the pacman::p_load() function as we have done in previous chapters.\n\n\n4.3.4 Converting our data to an SF object\nAt present, the data in the thefts object is just a regular tibble. We could not use it to make a map because R does not know which columns represent the geometry, or what co-ordinate system the locations are recorded in. We can deal with this by converting the data to an SF object using the st_as_sf() function from the sf package.\nThe data provided by the Vancouver Police use the UTM zone 10N co-ordinate system. UTM is a system for assigning co-ordinates to any location on earth relative to a designated local reference point for the UTM zone covering that part of the planet. The ‘N’ at the end of the zone name 10N refers to the northern hemisphere.\n\n\n\n\n\n\nCo-ordinate reference systems are specific to one part of the globe\n\n\n\nIn almost all cases, co-ordinate reference systems only work for the part of the world that they were designed for. So we should not use the UTM zone 10N co-ordinate system to map data outside the area for which it was designed (broadly speaking, the west coast of North America from Los Angeles to Vancouver, and the part of Canada directly north of Vancouver extending as far as the north pole). If we were to use the UTM zone 10N co-ordinate system for data from another part of the world, we would be very likely to get error messages or strange results.\n\n\n\n\n\n\n\n\n\n\n\nWe can convert the thefts tibble to an SF object using the st_as_sf() function (remember, all functions in the sf package start with st_, which can sometimes make the function names a little confusing). We specify which columns in the data represent the geometry (in this case, the x and y columns), and what co-ordinate system the data uses.\nCo-ordinate systems can be specified in lots of ways (some very complicated), but the easiest is to specify the EPSG code for the relevant system. An EPSG code is a unique reference number for a particular co-ordinate system that R can look up in a database to get the information needed to display the data on a map. The EPSG code for the UTM zone 10N is EPSG:32610.\nAdd this code to the chapter_04.R file in RStudio.\n\n\n\nchapter_04.R\n\nthefts_sf &lt;- st_as_sf(thefts, coords = c(\"x\", \"y\"), crs = \"EPSG:32610\")\n\n\nIf you look at the contents of the thefts_sf object by running head(thefts_sf) in the R Console, you’ll see that there is a new column called geometry. This column contains the co-ordinates of each bike theft. But crucially, it stores those co-ordinates in a format that R recognises represent specific locations on the surface of the earth, which means the co-ordinates can be used to make maps.\n\n\n\n\n\n\nImportant\n\n\n\nIt is important to remember that we should only use st_as_sf() to convert a non-spatial dataset (such as a tibble) into a spatial dataset (an SF object). If we use st_as_sf() on an object that is already an SF object, this can have unexpected results and lead to errors in your code.\nThe easy way to think about this is that if you have loaded a dataset with read_sf() or st_read() then you have already created an SF object, so you don’t need st_as_sf(). If you have loaded a dataset with any other function that reads data (such as read_csv() or read_excel()) then you will need to use st_as_sf() if you want to plot the data on a map. Most importantly, do not use st_as_sf() if you loaded a dataset with read_sf() or st_read().\n\n\n\n\n4.3.5 Finding bike thefts in our data\n\n\n\n\n\n\nChoosing only some rows in a dataset\n\n\n\nIf you look through the contents of the thefts_sf object by running head(thefts_sf) in the R Console, you will see that not all of the rows relate to bicycle thefts. The type column shows that the dataset also includes thefts from vehicles, for example. To choose only those rows containing bicycle thefts, which function from the dplyr package would we used? If you need help, you can think back to Chapter 3 or have a look at the Data transformation with dplyr cheat sheet.\nWhich function from the dplyr package should we use to remove all the rows from our dataset except those for bicycle thefts?\n\n select() filter() mutate() summarise()\n\nAdd code to the chapter_04.R file to create a new object called bike_thefts that contains only the rows in the thefts_sf object that relate to bicycle theft. If you get stuck, you can hit the ‘Hint’ button below to get help, but try to find the answer on your own first! Once you’ve finished the code, click the ‘Solution’ button to check your code.\n\n\nHint\n\nUse the filter() function to choose particular rows in a dataset. The syntax for filter() is filter(dataset, column_name == \"value\"). Replace dataset with the name of the SF object we created from the thefts tibble, column_name with the name of the column containing the offence type and value with the offence type for bicycle theft.\n\n\n\nSolution\n\nThe correct code to store only bicycle thefts in a new object is:\n\n\n\nchapter_04.R\n\nbike_thefts &lt;- filter(thefts_sf, type == \"Theft of Bicycle\")\n\n\n\n\n\nOur data is now ready for us to make our crime map!\nSo far in this chapter we have used several functions – read_csv(), clean_names(), st_as_sf() and filter() to produce an SF object representing the locations of bike thefts. Since we have done this step by step, we have created a different object to store the result produced by each function. But since we only need the final dataset, our code would be a lot easier to read if we used the pipe operator (|&gt;) to run all these functions in one go. Delete the relevant lines from the chapter_04.R script file and replace them with this code:\n\n\n\nchapter_04.R\n\n# Load packages\npacman::p_load(sf, tidyverse)\n\n# Load and wrangle bike theft data\nbike_thefts &lt;- read_csv(\"https://mpjashby.github.io/crimemappingdata/vancouver_thefts.csv.gz\") |&gt;\n  janitor::clean_names() |&gt; \n  st_as_sf(coords = c(\"x\", \"y\"), crs = \"EPSG:32610\") |&gt; \n  filter(type == \"Theft of Bicycle\")",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Your second crime map</span>"
    ]
  },
  {
    "objectID": "04_your_second_crime_map/index.html#producing-maps-in-r",
    "href": "04_your_second_crime_map/index.html#producing-maps-in-r",
    "title": "4  Your second crime map",
    "section": "4.4 Producing maps in R",
    "text": "4.4 Producing maps in R\nNow that we have our data, we can use it to create a map of bicycle theft in Vancouver. Before we start, let’s take another look at our dataset so that we know which columns contain which data.\n\n\n\nR Console\n\nhead(bike_thefts)\n\n\nSimple feature collection with 6 features and 8 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 488371.3 ymin: 5452696 xmax: 494295.1 ymax: 5458232\nProjected CRS: WGS 84 / UTM zone 10N\n# A tibble: 6 × 9\n  type              year month   day  hour minute hundred_block    neighbourhood\n  &lt;chr&gt;            &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;            &lt;chr&gt;        \n1 Theft of Bicycle  2020     1     1     0      0 12XX VENABLES ST Strathcona   \n2 Theft of Bicycle  2020     1     1     0      0 20XX MAPLE ST    Kitsilano    \n3 Theft of Bicycle  2020     1     1    13      0 7XX PACIFIC BLVD Central Busi…\n4 Theft of Bicycle  2020     1     1    20      0 53XX VINE ST     Arbutus Ridge\n5 Theft of Bicycle  2020     1     3    11     55 65XX ANGUS DR    Kerrisdale   \n6 Theft of Bicycle  2020     1     3    14      0 4XX E 10TH AVE   Mount Pleasa…\n# ℹ 1 more variable: geometry &lt;POINT [m]&gt;\n\n\n\n4.4.1 Introduction to ggplot2\n\n\n\nA map is a specialised type of chart, so we can make maps using the ggplot2 package that is widely used to create other types of chart in R. ggplot2 charts are made up of layers, so they’re well suited to making maps.\n\nThe most-basic crime map that we can make simply plots the locations of crimes with no context. However, this almost never makes a good crime map because if there are more than a few crimes it becomes hard to see patterns in the data. But we can use a basic dot map as the foundation on which we can build a better map later on.\nggplot2 plots work by building up a chart using different functions, each of which adds or modifies some part of the chart. Building a plot starts with calling the ggplot() function, with each subsequent function being added to the plot definition using the + operator. Note that while the package is called ggplot2, the function in that package used to create plots is called ggplot(), not ggplot2().\nThe most-important of the ggplot2 functions are those beginning with geom_, which add graphical elements to the chart. If you want to add a layer to your chart showing a scatter plot, you use the geom_point() function, while if you want to make a line chart you use geom_line().\n\n\n\n\n\n\n\n\n\nThere are lots of geom_ functions available for representing data on charts in different ways. For maps, we can use the geom_sf() function that is designed to add spatial data (in the form of an SF object such as our bike_thefts data) to a chart, making it into a map. So to simply plot the points in our bicycle-theft data, we can use the code:\n\n\n\nR Console\n\nggplot() +\n  geom_sf(data = bike_thefts)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ngeom_sf() only works with SF objects\n\n\n\ngeom_sf() only works on SF objects, which is why we needed to convert the original tibble of data to an SF object using st_as_sf(). If you try to use geom_sf() on a dataset that is not stored as an SF object, R will produce an error.\n\n\nBy convention, each function that we add to ggplot() to change the appearance of our map goes on a new line (this makes the code easier to read) and all but the first line is indented by two spaces. RStudio does this indenting automatically if the previous line ends with a + symbol, since RStudio then understands that there is more code to come on the next line.\nThe map above shows the bike-theft data, but it is obviously not a very useful map. Fortunately, we can use the features of the ggplot2 package to build on this basic map.\n\n\n\n\n\n\nAvoid dot maps\n\n\n\nUnless we want to produce a map of only a very small number of crimes (like the Atlanta downtown homicides map we produced in Chapter 2), it is unlikely that a point map will be very useful.\nIn fact, if you find yourself making map with each crime represented by a separate point, you should probably stop and ask yourself if that is really the best way to achieve your goal – it will almost always be better to map the data in another way.\n\n\n\n\n4.4.2 Controlling aesthetics\nWe can change the appearance of the points by specifying various arguments to the geom_sf() function. These arguments are called aesthetics, because they control the aesthetic appearance of the geometric objects (points, lines etc.) that are produced by a geom_ function. There are lots of aesthetics, but some of the most common are:\n\ncolour controls the colour of points and lines (for polygons, it controls the colour of the border around the polygon edge) – you can also use the spelling color for this argument and get an identical result,\nfill controls the colour used to fill polygons or points that use a shape capable of having different colours in the centre and around the edge (fill has no meaning for lines),\nshape controls the shape (circle, triangle, square etc.) of points (it has no meaning for lines or polygons),\nsize controls the size of points and text,\nlinewidth controls the width of lines, including the borders around the edges of polygons, and\nalpha controls the transparency of a layer (alpha = 1 equals fully opaque, alpha = 0 means fully transparent).\n\ncolour and fill can be specified using any one of 657 R colour names or using a hexidecimal (‘hex’) colour code. Values of size don’t relate to any unit of size (e.g. millimetres or points), so it’s easiest to set the size of points and text by trial and error.\nThere are 25 built-in shapes for points in R (shape 16 is the default):\n\n\n\n\n\n\n\n\n\nWe use aesthetics such as colour, fill, etc. to change the appearance of layers on a map by adding the aesthetic as an argument to the geom_*() function that creates the relevant layer. For example, we could change the points on our map to be red squares rather than the default black circles using this code:\n\n\n\nR Console\n\nggplot() +\n  geom_sf(data = bike_thefts, shape = 15, colour = \"red\")\n\n\n\n\n\n\n\n\n\nAs we have said, this basic map is not very useful. We can see that there seems to be a cluster of bike thefts towards the top (north) of the map, but it is difficult to see how important this cluster is because so many of the points overlap. Overlapping points are a particular problem in maps, because if there are multiple crimes at the same location then the points representing those crimes will be exactly on top of one another and it will be impossible to see whether there is one crime at a particular location or 100.\nOne way to deal with this problem is to make the points semi-transparent so that overlapping points appear darker. This often works better if we also make the points slightly smaller at the same time. We can use the alpha and size aesthetics to make the points smaller (relative to the default for points of size = 1) and semi-transparent.\nAdd this code to your script file:\n\n\n\nchapter_04.R\n\n# Create a basic crime map\nggplot() +\n  geom_sf(data = bike_thefts, size = 0.75, alpha = 0.1)\n\n\n\n\n\n\n\n\n\nMaking the points semi-transparent goes some way to making it easier to see where bike theft is most common in Vancouver, but the pattern is not clear and it is not possible to tell which darker points represent a handful of crimes at the same location and which represent hundreds of crimes at the same location.\nTo make our map truly useful, we need to use a different technique. In chapter Chapter 6 we will learn how to identify crime patterns by mapping the density of crime.\nSave the chapter_04.R file, then restart R to start a new session by clicking on the Session menu and then clicking Restart R. This creates a blank canvas for the next chapter.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Your second crime map</span>"
    ]
  },
  {
    "objectID": "04_your_second_crime_map/index.html#in-summary",
    "href": "04_your_second_crime_map/index.html#in-summary",
    "title": "4  Your second crime map",
    "section": "4.5 In summary",
    "text": "4.5 In summary\nIn this chapter we have learned more about each specific step in the process of creating a basic crime map. Concepts such as co-ordinate reference systems and EPSG codes can be hard to understand at first, but you will get used to them as you use them to make more maps, so that by the end of this book you will be confident applying those ideas in a wide range of contexts.\nAt the moment, your script file should look like this.\n\n\n\nchapter_04.R\n\n# Load packages\npacman::p_load(sf, tidyverse)\n\n# Load and wrangle bike theft data\nbike_thefts &lt;- read_csv(\"https://mpjashby.github.io/crimemappingdata/vancouver_thefts.csv.gz\") |&gt;\n  janitor::clean_names() |&gt; \n  st_as_sf(coords = c(\"x\", \"y\"), crs = \"EPSG:32610\") |&gt; \n  filter(type == \"Theft of Bicycle\")\n\n# Create a basic crime map\nggplot() +\n  geom_sf(data = bike_thefts, size = 0.75, alpha = 0.1)\n\n\n\n\n\n\n\n\nImportant\n\n\n\nAt the moment, it isn’t very easy to see patterns on this map, since so many of the points overlap. In Chapter 6, we will learn how to make this map much better by converting it into a density map.\nRemember: whenever you create a dot map, ask yourself if a density map would be more useful.\n\n\n\nYou can find out more about some of the things we have covered in this chapter using these resources:\n\nUnderstand more about the history of trying to develop accurate map projections in this short video: Why all world maps are wrong.\nFind out more about making all sorts of charts (not just maps) with the ggplot2 package in the Data Visualisation chapter of R for Data Science by Hadley Wickham and Garrett Grolemund.\nLearn more about making maps using simple features in Chapter 1 of Spatial Data Science by Edzer Pebesma and Roger Bivand.\n\n\n\n\n\n\n\n\nRevision questions\n\n\n\nAnswer these questions to check you have understood the main points covered in this chapter. Write between 50 and 100 words to answer each question.\n\nWhat is spatial data, and how does it differ from other types of data? Provide examples of spatial features commonly used in crime mapping.\nExplain the role of coordinate systems in mapping. Why is it important to use the correct coordinate system for a specific dataset?\nDiscuss the challenges of using file paths in R scripts and how relative file paths improve code portability.\nWhy is it helpful to use the pipe operator (|&gt; in R) when working with multiple functions? Provide an example from the chapter.\nExplain how the select() function in dplyr is used to manage columns in a dataset. How can it help when working with large spatial datasets?",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Your second crime map</span>"
    ]
  },
  {
    "objectID": "05_code_with_style/index.html",
    "href": "05_code_with_style/index.html",
    "title": "5  Code with style",
    "section": "",
    "text": "5.1 Introduction\nNow that you’re beginning to write code in R, it’s time to introduce a few conventions for how to write code so that it’s easier to read. This is important because “good coding style is like correct punctuation: you can manage without it, butitsuremakesthingseasiertoread”.\nWriting readable code is particularly important because it is much easier to find mistakes in well-written code, and your code will inevitably contain mistakes (everyone’s code contains mistakes!). That means writing readable code isn’t just a matter of style but a key part of successfully using a programming language like R to do work.\nWriting code has been compared to performing in a band:\nYou might be relaxing at this point, thinking “that isn’t a problem for me, because I’m the only person who is going to be working on my code”. If so, think again. It’s been said that in data science that there are always at least two people working on a project: the you who is working on the code now, and the past you who has worked on the same code previously. The problem is that past you does not answer emails. So you can save future you a lot of hassle later by writing readable code.\nThis chapter introduces some basic guidelines on formatting your code. Some sections of this chapter have been adapted from The tidyverse style guide, which provides lots more detail. All the code you see in the chapters in this book was written following this style guide.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Code with style</span>"
    ]
  },
  {
    "objectID": "05_code_with_style/index.html#introduction",
    "href": "05_code_with_style/index.html#introduction",
    "title": "5  Code with style",
    "section": "",
    "text": "I liken the situation to a group of musicians trying to form a band. Each one comes in believing that their way of doing things is best (their “method” or “process”). The band will struggle so long as everyone is trying to do their own thing. It’s impossible to create good music unless everyone in the band agrees on the tempo, the style and who should take lead during a song. Anyone who has ever heard a high school band perform knows this to be true. Unless everyone is on the same page, you aren’t going to accomplish much.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Code with style</span>"
    ]
  },
  {
    "objectID": "05_code_with_style/index.html#sec-projects",
    "href": "05_code_with_style/index.html#sec-projects",
    "title": "5  Code with style",
    "section": "5.2 Organising files in a project",
    "text": "5.2 Organising files in a project\n\n\n\nIn Section 1.4.2 we learned how to create an RStudio project. Projects in RStudio are a way of keeping your analysis organised. When you work inside an project, RStudio will do various things in the background that make it easier to stay organised. For example, RStudio will save the project’s state: if you have to close the project and re-open it later, all the files that you had open before will re-open, too.\n\n\n\n\n\n\nDo everything in RStudio inside a project\n\n\n\nI strongly recommend that you use projects for everything you do in RStudio. There are no real costs to using projects and the benefits can be substantial, so try to get used to using them for all your analysis. Have one RStudio project for each project that you work on, for example you might have one project for each research report that you write or piece of analysis you do on a particular topic. For this purposes of this course, do all your work inside the single project you created in Section 1.4.2.\n\n\nOnce you have created an empty project, you can create some directories within it to organise your files. There are lots of suggestions online for how to organise your files within a project – see this summary of project-organisation methods if you would like to understand the different options available – but the most important thing is that you have separate places to store:\n\nthe original/raw data that you have obtained (e.g. by downloading it from a website or extracting it from a police database),\nthe results of any data wrangling that you do,\nfiles containing the code that you use to wrangle the data and conduct your analysis, and finally,\nthe outputs from your analysis, including tables, charts and reports.\n\nPutting each of these four types of files in a separate directory within the project directory makes it much easier to keep track of them. We can create the necessary directories directly in R using the dir.create() function. You can run this code in the R Console whenever you create a new project:\n\n\n\nR Console\n\n# `dir.create()` does not produce any output unless there is a problem\ndir.create(\"source_data\")\ndir.create(\"analysis_data\")\ndir.create(\"code\")\ndir.create(\"outputs\")\n\n\n\n\n\n\n\n\nInside a project, all paths are relative to the project directory\n\n\n\nBecause we are running these commands from inside an RStudio project, these directories will automatically be created as subdirectories of the project directory we have already created – this saves a lot of moving between directories, and is another reason to use RStudio projects.\nOnce you are working within an RStudio project, RStudio knows to treat any reference to a file or folder as being relative to the folder that contains the RStudio project. So, for example, if your project for analysis of thefts in Vancouver is stored in a folder called /Documents/crime_mapping/vancouver_thefts/ and you run the R code read_csv(\"source_data/vancouver_thefts.csv\"), RStudio knows that you mean to open the file stored at /Documents/crime_mapping/vancouver_thefts/source_data/vancouver_thefts.csv. This can save you lots of time working out how to reference a particular file location in R.\n\n\n\n\n\n\n\n\nWhat about setting a working directory?\n\n\n\n\n\nIf you have learned some R before this course – especially if it was some time ago or taught by someone who themselves learned R a long time ago – you might have learned to start each analysis by setting a working directory using the setwd() function or the Session &gt; Set Working Directory menu option in RStudio. Not only do you not need to do this when you keep your analysis in an RStudio project, you should not set a working directory in R because it can have several unpleasant side effects.\nThe biggest problem with manually setting a working directory in your R code is that if you share your code with someone else then it is likely the setwd() function will produce an error because that person’s computer will have a different directory structure to your computer. So if the first line of your R script is setwd(\"/Users/uctzaaa/Documents/crime_mapping_vancouver_thefts/\") because that is the location of the working directory on your computer, this will produce an error on another computer that does not have a /Users/uctzaaa directory (i.e. it will cause an error on almost every computer in the world). This makes it much harder to get help on your code, for example if you are using a reproducible example (which we will learn about later in this chapter) to show someone else a problem with your code.\nSetting a working directory using the Session &gt; Set Working Directory menu buttons in RStudio has the additional problem that it’s not possible to work out from reading your R code that you have set the working directory that way. So if someone else (or you, in a few weeks when you’ve forgotten) tried to run your code and it produced an error, there would be nothing in the code to tell them that they needed to change the working directory.\nFor more detail on why you should not set a working directory manually, read this blog post on project-oriented workflow.\n\n\n\n\nTyping out the dir.create() function four times is somewhat tedious, and certainly would be if we needed to create (say) 10 directories rather than four. One way to make this code shorter is to use the walk() function from the purrr package, which is part of the tidyverse suite of packages. walk() allows us to run any function several times, each time using a different input, one at a time. The function is called walk() because it ‘walks’ through all the items in a vector of inputs, applying the same function to each of them in turn.\nThe general format of calls to the walk() function is:\nwalk(vector_of_inputs, function_name_to_apply_to_those_inputs)\nSo if we wanted to create the same four directories using walk(), we could run this single line of code in the R Console:\n\n\n\nR Console\n\nwalk(c(\"source_data\", \"analysis_data\", \"code\", \"outputs\"), dir.create)\n\n\nNote that the function name dir.create is not followed by a pair of parentheses when it is used inside walk().\nwalk() is a very powerful function and this is only a very simple example of what it can do, but we will return to it when we need it again.\n\n\n\n\n\n\nWhy didn’t we use a loop instead of walk()?\n\n\n\n\n\nIf you have learned any other programming languages, you might remember that the usual way to apply the same function to a series of values is to use a ‘loop’, which (depending on the language) might be called a ‘for loop’ or a ‘foreach loop’. While you can create ‘for’ loops in R that would do the same thing as the walk() function, it is generally better not to use ‘for’ loops in R. This is both because loops are often slower – which can be particularly important when analysing large spatial datasets – and because functions like walk() can easily be used inside pipelines constructed with the |&gt; operator.\nIf you’re ever tempted to use a ‘for’ loop in R, stop and think about how to achieve the same result using a function like walk().",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Code with style</span>"
    ]
  },
  {
    "objectID": "05_code_with_style/index.html#naming-files",
    "href": "05_code_with_style/index.html#naming-files",
    "title": "5  Code with style",
    "section": "5.3 Naming files",
    "text": "5.3 Naming files\nThe final step in organising your work so that you can analyse data and stay sane is to give every file you create a name that will help you find it later on. To do this, stick to three principles:\n\nmake your file names easily readable by computers,\nmake your file names easily readable by humans, and\nuse file names that will by-default by sorted in a meaningful way.\n\nComputers can increasingly read file names that contain any characters you can think of, but many different characters make referencing your file names in your code more difficult than need be. To keep your code simple, stick to files names that contain only lower-case letters (so you don’t have to remember if each letter is upper-case or lower-case), numbers, hyphens (-) and underscores (_) – that’s it. If the language you are writing in uses letters with accents such as é or ü, its best to not use those accents in file names. So, instead of having files with names like this:\nJohn's raw data.csv\nfigure 4.jpg\nDüsseldorfCrimeData.xlsx\nuse the names:\njohn_raw_data.csv\nfigure_4.jpg\ndusseldorf_crime_data.xlsx\n\nTo make your files easily readable by humans, make sure the file name describes what the file contains or does. So giving a file the name crime_data_atlanta_2020.csv is much more useful than calling the same file data_file_23.csv. If you’ve been using computers for a while (or you’ve been taught by someone who has), you may have been trained to use very short file names because old computers could only handle very short names. Fortunately, modern computers can handle much longer file names, so it’s generally safe to use a few extra characters to describe what your file is or does. File names are also easier to read if words are separated by a hyphen or underscore: police_recorded_robbery_data_2020 is much easier to read than policerecordedrobberydata2020.\nIf your files have a meaningful order, it’s useful to reflect this in the file name. Imagine you have three code files: one for data wrangling, one for carrying out spatial analysis tasks on that wrangled data and a third one for creating charts to show the results of the analysis. You cannot run the analysis code until the data wrangling is complete, so the files have a meaningful order.\nWhen you look at these files in the Windows File Explorer, Mac Finder or RStudio Files tab, they are likely to appear out of order. For example, they might be:\nanalyse_data.R\ncreate_charts.R\nwrangle_data.R\nIf you add a number to the start of the file name to show which order to use the files in, things become clearer:\n01_wrangle_data.R\n02_analyse_data.R\n03_create_charts.R\nNote that the numbers are padded with a zero so that they will still be sorted in the correct order if you end up with more than 10 files in my workflow (I’ve only used one zero for padding because I don’t expect to have more than 100 script files).",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Code with style</span>"
    ]
  },
  {
    "objectID": "05_code_with_style/index.html#organising-your-code",
    "href": "05_code_with_style/index.html#organising-your-code",
    "title": "5  Code with style",
    "section": "5.4 Organising your code",
    "text": "5.4 Organising your code\nIn Section 2.2 we learned about the difference between temporary code that we type in the R Console in RStudio and permanent code that we type in a .R script file. In this section we will learn how to organise the permanent code within a script file so that it is easier to use.\n\n5.4.1 Leaving notes for future you\nWithin each .R file, you can make your code easier to understand in several ways. First, add a comment (one or more lines beginning with # followed by a space) at the top of the file to explain what the code in that file does. This will make it easier for you to know that you’ve found the right file if you are looking for it in a few weeks when you’ve forgotten (which you will) what file contains what code.\n# This code produces a density map of bike thefts in Vancouver in 2020\nComments should usually start with a capital letter and follow normal English rules of punctuation, spacing, etc.\n\n\n5.4.2 Letting your code breathe\nUnless your code is very simple, it will probably consist of several separate tasks that are completed one after another. For example your code might download some data, wrangle it and then plot it on a map. In that case, it can be useful to split your code up into blocks by leaving a blank line between the code needed for each task. For example, if we take the code:\npacman::p_load(tidyverse)\ncrimes &lt;- read_csv(\"crime_data.csv\")\ncrimes &lt;- janitor::clean_names(crimes)\nburglaries &lt;- filter(crimes, type == \"burglary\")\nburglaries &lt;- mutate(burglaries, month = month(date_time))\nggplot() + \n  geom_point(aes(x = lon, y = lat, colour = month)) +\n  theme_void()\nit becomes easier to read if we split the code up into four tasks: loading the necessary packages, reading the data, wrangling the data and plotting the data.\npacman::p_load(tidyverse)\n\ncrimes &lt;- read_csv(\"crime_data.csv\")\n\ncrimes &lt;- janitor::clean_names(crimes)\nburglaries &lt;- filter(crimes, type == \"burglary\")\nburglaries &lt;- mutate(burglaries, month = month(date_time))\n\nggplot(burglaries) + \n  geom_point(aes(x = lon, y = lat, colour = month)) +\n  theme_void()\nSince data wrangling involves several steps and each function uses the result of the previous step, we could use the pipe operator |&gt; to make that code a bit cleaner:\npacman::p_load(tidyverse)\n\ncrimes &lt;- read_csv(\"crime_data.csv\")\n\nburglaries &lt;- crimes |&gt; \n  janitor::clean_names() |&gt; \n  filter(type == \"burglary\") |&gt; \n  mutate(month = month(date_time))\n\nggplot(burglaries) + \n  geom_point(aes(x = lon, y = lat, colour = month)) +\n  theme_void()\nYou can see that in this code, making sure there are no blank lines between lines of code in a pipeline or a ggplot(), and putting a blank line between each pipeline or stack, makes it much easier to see where one task ends and the next begins.\n\n\n5.4.3 Header comments\nIf your code includes very long tasks (e.g. where the code takes up more than a full screen on your computer), you might want to use header comments to divide your code into sections. You can do this by writing a comment that is followed by four of more hyphens (----):\n# Load data ----\n\n… some code …\n\n\n# Wrangle data ----\n\n… some code …\n\n\n# Plot data ----\n\n… some code …\nRStudio will recognise comments that end in four or more hyphens as being headings, and will create a table of contents for your code. You can use this to move between headings by clicking on the Jump To menu at the bottom of the Source panel in RStudio:\n\n\n\n\n\n\n\n\n\nIn general, writing code that is readable is more important than writing the shortest code possible, so don’t be afraid to let your code breathe by using space to separate your code into meaningful chunks.\n\n\n\n\n\n\nComments\n\n\n\nHow do you write a comment in R?\n\n A # symbol, followed by no spaces. Comments should start with a capital letter. e.g. #Load data A # symbol followed by a space. Comments should start with a capital leter. e.g. # Load data A # symbol, followed by no spaces. Comments should be in lower case unless the comment is long enough to contain more than one sentence. e.g. #load data A # symbol followed by a space. Comments should be in lower case unless the comment is long enough to contain more than one sentence. e.g. # load data",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Code with style</span>"
    ]
  },
  {
    "objectID": "05_code_with_style/index.html#naming-objects",
    "href": "05_code_with_style/index.html#naming-objects",
    "title": "5  Code with style",
    "section": "5.5 Naming objects",
    "text": "5.5 Naming objects\nR objects can have any name you like, as long as the name starts with a letter and contains only letters, numbers, dots (.) and underscores (_). That said, you will find coding easier if you follow a few conventions.\n\nUse only lower-case letters in the names of objects, which avoids you having to remember whether a particular letter was upper- or lower-case.\nUse snake case (object_name, with words separated by underscores) for object names rather than camel case (objectName) or kebab case (object-name).\nDon’t use dots in object names.\nDon’t give objects the same names as R functions, because re-using function names makes reading your code more difficult.\n\n\n\n\n\n\n\n\n\n\nJust as crime_data_atlanta_2020.csv is a more-useful file name than data_file_23.csv, you will find it easier to read your code if you give your objects meaningful names. So when you load data into R (e.g. with read_csv()) don’t just call it data (not least because there is a function named data()) but instead give it a name like atlanta_crimes if it contains (for example) crime data from Atlanta.\n\n\n\n\n\n\nNaming objects\n\n\n\nWhich of these would be a good name for an object containing data on homicides in Abu Dhabi?\n\n abu-dhabi-homicides Abu_Dhabi_homicides homicide_data abu_dhabi_homicides",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Code with style</span>"
    ]
  },
  {
    "objectID": "05_code_with_style/index.html#spacing",
    "href": "05_code_with_style/index.html#spacing",
    "title": "5  Code with style",
    "section": "5.6 Spacing",
    "text": "5.6 Spacing\nSpacing out code makes it much easier to read, but (just as in any language) code is easiest to read if spaces are used where people expect them to be by convention.\nMostly in R, we use spaces where we would expect them in English: after commas but not before, outside parentheses but not inside, etc.\n# Good\nread_csv(\"crime_data.csv\", skip = 4)\n\n# Bad\nread_csv(\"crime_data.csv\",skip = 4)\nread_csv(\"crime_data.csv\" ,skip = 4)\nread_csv(\"crime_data.csv\" , skip = 4)\nDon’t put spaces inside parentheses, or between the names of functions and the parentheses:\n# Good\nmean(x, na.rm = TRUE)\n\n# Bad\nmean (x, na.rm = TRUE)\nmean( x, na.rm = TRUE )\nDo put spaces around most operators (==, +, -, &lt;-, etc.), including either side of = when specifying the values of function arguments:\n# Good\nheight &lt;- (feet * 12) + inches\nmean(x, na.rm = TRUE)\n\n# Bad\nheight&lt;-feet*12+inches\nmean(x, na.rm=TRUE)\nAlthough there are some operators that shouldn’t have spaces around them: $, @, [, [[, ^, : and ?.\n\n\n\n\n\n\nSpacing\n\n\n\nWhich of these lines of code has the optimal spacing?\n\n burglary&lt;-filter(crimes,type==\"burglary\") burglary &lt;- filter(crimes, type==\"burglary\") burglary &lt;- filter(crimes, type == \"burglary\") burglary &lt;- filter ( crimes , type == \"burglary\" )",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Code with style</span>"
    ]
  },
  {
    "objectID": "05_code_with_style/index.html#functions",
    "href": "05_code_with_style/index.html#functions",
    "title": "5  Code with style",
    "section": "5.7 Functions",
    "text": "5.7 Functions\nWe’ve now got used to calling functions to do things in R, like calling read_csv() to load data from a CSV file or filter() to choose certain rows from a dataset. We know that we can change the behaviour of functions by using arguments. For example, we can wrap a string of text into shorter lines using the str_wrap() function from the stringr package. str_wrap() needs two arguments: the text to be wrapped into multiple lines and the maximum length of a line of text before the next word is wrapped onto a new line. These arguments are called string and width, so we can call the function as:\nstr_wrap(string = \"some text to be wrapped\", width = 10)\nThe string argument provides the data that the str_wrap() function will work on, while the width argument provides the details of how that work should be done. Since the data argument to a function is typically required (the function makes no sense without it) and is often the first argument, you can omit the name of data arguments to functions. For all other arguments, it is best to give the argument name. So to use str_wrap(), you can write:\nstr_wrap(\"some text to be wrapped\", width = 10)\nIn general, you should keep lines of code to a maximum of 80 characters long, since they can easily fit on most screens and are easy to read. When calling a function, put all of the parameters on a single line if they will fit into 80 characters or less:\ndo_something_simple(\"something\", with = \"only\", short, \"arguments\")\nBut if the function call is longer than 80 characters, use one line each for the function name, each argument, and the closing ), with the arguments indented by two spaces. This makes the code much easier to read.\n# Good\ndo_something_very_complicated(\n  something = \"that\",\n  requires = many,\n  arguments = \"some of which may be long\"\n)\n\n# Bad\ndo_something_very_complicated(\"that\", requires, many, arguments, \"some of which may be long\")\ndo_something_very_complicated(\"that\", requires, many, arguments,\n                              \"some of which may be long\"\n                              )\n\n\n\n\n\n\nImportant\n\n\n\nOne mistake that people often make when splitting a function across multiple lines is to leave the closing parenthesis ) at the end of the last line of the function, like this:\n# Bad\ndo_something_very_complicated(\n  something = \"that\",\n  requires = many,\n  arguments = \"some of which may be long\")\nThe problem with this is that it makes it harder to see where a particular function ends. Instead, put the closing parenthesis on a line on its own, indented with the same number of spaces as the first line of the function:\n# Good\ndo_something_very_complicated(\n  something = \"that\",\n  requires = many,\n  arguments = \"some of which may be long\"\n)\nThis makes it much easier to see where a function call starts and ends.\n\n\nWhen combining multiple functions using the pipe operator (|&gt;), put each function on a single line, with all but the first line indented by two spaces:\na_function() |&gt; \n  another_function() |&gt; \n  and_a_third_function()\nOnce you learn about other types of R code you will need to know how best to style it, but we will learn about those when we need to.\n\n\n\n\n\n\nStyling functions in code\n\n\n\nIf a function is written across several lines of code because it will not fit on one line, where should the closing parenthesis ) at the end of the function be placed?\n\n On a separate line, indented as much as the function name. On a separate line, indented as much as the arguments on the lines above it. At the end of the final function argument, on the same line. It doesn't matter where the closing parenthesis is placed as long as there is one.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Code with style</span>"
    ]
  },
  {
    "objectID": "05_code_with_style/index.html#sec-styler",
    "href": "05_code_with_style/index.html#sec-styler",
    "title": "5  Code with style",
    "section": "5.8 Styling your code automatically",
    "text": "5.8 Styling your code automatically\nYou can get help on styling your R code using the styler package, which can automatically format your code for you. After you install the styler package with the code pacman::p_install(\"styler\"), you can style your code by:\n\nselecting the code you want to style,\nopening the Addins menu at the top of the Source panel in RStudio,\nclicking ‘Style selection’ in the ‘Styler’ section of the list of addins.\n\n\n\n\n\n\n\n\n\n\nRStudio will also try to help style your code as you type, for example by automatically indenting lines.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Code with style</span>"
    ]
  },
  {
    "objectID": "05_code_with_style/index.html#in-summary",
    "href": "05_code_with_style/index.html#in-summary",
    "title": "5  Code with style",
    "section": "5.9 In summary",
    "text": "5.9 In summary\nYou now know how to write your R code so that it is easy to read, which makes it much easier to understand. Understanding code when you read it is important because it allows you to work out what the code is trying to achieve and because it makes it much easier to find and fix problems when your code is not behaving as you want it to.\n\nWriting readable, understandable code is important. To find out more about this, read some of these articles:\n\nWhy coding style matters by Nicholas Zakas.\nThe tidyverse style guide by Hadley Wickham, which is the basis for the rules outlined in this chapter.\n\n\n\n\n\n\n\n\nRevision questions\n\n\n\nAnswer these questions to check you have understood the main points covered in this chapter. Write between 50 and 100 words to answer each question.\n\nWhy is maintaining a consistent coding style important in R? Explain how it benefits both the person writing the code and others who may need to read or edit it later.\nWhat are some best practices for organizing your code different sections? Provide examples of how comments and spacing can make your code easier to navigate.\nDescribe the guidelines for naming objects in R. Why is it important to avoid using names that clash with existing R functions?\nWhat are the key rules for spacing in R code? Provide examples of good and bad practices when spacing around operators, commas, and parentheses.\nExplain how to format a function call that spans multiple lines. Why is it important to place the closing parenthesis on a separate, indented line?\n\n\n\n\nOn Perl from Three Panel Soul. ‘File Organization: Naming’ by Data Carpentry licensed under the Creative Commons Attribution licence. XKCD.com comic ‘Documents’ licensed under the Creative Commons Attribution-NonCommercial license. The tidyverse style guide licensed under the Creative Commons Attribution-ShareAlike licence. Artwork by @allison_horst. Some content on this page contains public sector information licensed under the Open Government Licence v3.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Code with style</span>"
    ]
  },
  {
    "objectID": "06_mapping_crime_patterns/index.html",
    "href": "06_mapping_crime_patterns/index.html",
    "title": "6  Mapping crime patterns",
    "section": "",
    "text": "6.1 Introduction\nIn Chapter 4, we created a map showing bike thefts in the City of Vancouver. But it was hard to see the patterns of theft on this map because too many of the points representing theft locations overlapped. In fact, dot maps are almost never a good way to show patterns of crime. A better way to show where crime is concentrated on a map is to work out the density of crime in each area and then map that density. In this chapter we will learn how to estimate crime density and show it on a map.\nCreate a new R script file and save it as chapter_06.R, then add this code from Chapter 4:\nchapter_06.R\n\n# Load packages\npacman::p_load(ggspatial, sf, sfhotspot, tidyverse)\n\n# Load and wrangle bike theft data\nbike_thefts &lt;- read_csv(\"https://mpjashby.github.io/crimemappingdata/vancouver_thefts.csv.gz\") |&gt;\n  janitor::clean_names() |&gt; \n  st_as_sf(coords = c(\"x\", \"y\"), crs = \"EPSG:32610\") |&gt; \n  filter(type == \"Theft of Bicycle\")\nRun this code. To do that, highlight all the lines of code and then either:",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Mapping crime patterns</span>"
    ]
  },
  {
    "objectID": "06_mapping_crime_patterns/index.html#introduction",
    "href": "06_mapping_crime_patterns/index.html#introduction",
    "title": "6  Mapping crime patterns",
    "section": "",
    "text": "click the Run button at the top of the R script panel, or\npress Ctrl+Enter (Windows) or Command+Return (Mac) on your keyboard.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Mapping crime patterns</span>"
    ]
  },
  {
    "objectID": "06_mapping_crime_patterns/index.html#mapping-crime-density",
    "href": "06_mapping_crime_patterns/index.html#mapping-crime-density",
    "title": "6  Mapping crime patterns",
    "section": "6.2 Mapping crime density",
    "text": "6.2 Mapping crime density\nWhen we speak about the density of crime, we mean the relative concentration of points in each part of the area we are studying, i.e. how many points (representing bike thefts) are there in each part of the map relative to all the other areas of the map.\nTo estimate the density of points in different areas of the map, R uses a technique called kernel density estimation (KDE). To do this, R must:\n\ndivide the map into a grid of cells, each the same size,\ncount the number of points in each cell,\nfor each cell, count the number of points in nearby cells, but give less weight to (i.e. systematically undercount) those cells that are further away,\nfor each cell, total up the count of points in that cell and the (weighted) count of points in nearby cells – this is the estimate of the density of points in that cell.\n\nThis procedure has the effect of producing a smooth surface representing crime density.\n\n\n\n\n\n\n\n\n\n\n\nThere are several ways we can make density maps in R. In this course we will use the sfhotspot package because it makes reasonable default decisions about how density maps should look, while still giving us control over their appearance if we want it. sfhotspot also has other useful functions that we will use in future chapters.\nTo create a density map using sfhotspot, we first use the hotspot_kde() function to convert a dataset of offence locations to an estimate of the density of offences for each cell in a grid. hotspot_kde() automatically chooses how big the cells in the grid should be (but we can set this ourselves if we want to).\nAdd this code to the script file for this chapter and then run that line of code:\n\n\n\nchapter_06.R\n\n# Estimate density of bike thefts\nbike_theft_density &lt;- hotspot_kde(\n  bike_thefts, \n  bandwidth_adjust = 0.5, \n  quiet = TRUE\n)\n\n\n\n\n\n\n\n\nWhy did we specify quiet = TRUE?\n\n\n\n\n\nThe quiet = TRUE argument to the hotspot_*() family of functions (including hotspot_kde()) stops the function from producing progress messages as we go along. If you experience any problems using the hotspot_kde() function, the best place to start in working out what has happened is to run the code again with the quiet = TRUE argument removed.\n\n\n\nAs usual, we can use the head() function to check what the resulting object looks like:\n\n\n\nR Console\n\nhead(bike_theft_density)\n\n\nSimple feature collection with 6 features and 2 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 490447.2 ymin: 5450107 xmax: 491647.2 ymax: 5450307\nProjected CRS: WGS 84 / UTM zone 10N\n# A tibble: 6 × 3\n      n   kde                                                           geometry\n  &lt;dbl&gt; &lt;dbl&gt;                                                      &lt;POLYGON [m]&gt;\n1     0  2.76 ((490447.2 5450107, 490447.2 5450307, 490647.2 5450307, 490647.2 …\n2     0  2.24 ((490647.2 5450107, 490647.2 5450307, 490847.2 5450307, 490847.2 …\n3     0  2.04 ((490847.2 5450107, 490847.2 5450307, 491047.2 5450307, 491047.2 …\n4     0  2.63 ((491047.2 5450107, 491047.2 5450307, 491247.2 5450307, 491247.2 …\n5     0  3.24 ((491247.2 5450107, 491247.2 5450307, 491447.2 5450307, 491447.2 …\n6     0  3.13 ((491447.2 5450107, 491447.2 5450307, 491647.2 5450307, 491647.2 …\n\n\nThe bike_theft_density object created by hotspot_kde() contains three columns: n contains the count of bike thefts in each cell, kde contains the estimate of the density of thefts in each cell, and geometry contains the outline of each grid cell.\nSince hotspot_kde() produces an SF object, we can add it to a map using the geom_sf() function. We can also use the fill aesthetic to specify that the fill colour of each grid cell should be determined based on the values of the kde column in the bike_theft_density object.\n\n\n\nR Console\n\nggplot() +\n  geom_sf(aes(fill = kde), data = bike_theft_density, colour = NA)\n\n\n\n\n\n\n\n\n\nWe have already seen that we can set aesthetics such as colour and shape manually, but the aes() function allows us to specify that the values of different aesthetics should be controlled by columns in the data. The aes() function takes as its arguments pairs of values (combined with an = symbol) where the first value is an aesthetic and the second value is the name of a column in the data. For example, to use the colour of points on a map to represent different types of crime that were stored in a column in the data called type, we could use aes(colour = type). In the map above, we use aes(fill = kde) to specify that the fill colour of the grid cells on the map should be controlled by the kde column in the data. This is called mapping a column to an aesthetic.\n\n\n\n\n\n\nWhen to specify values inside or outside aes()\n\n\n\nWhen should you specify the values of aesthetics inside aes() and when should you do it outside aes()?\n\nIf you want an aesthetic to have a constant value for all the points, lines or other shapes in a layer, control the aesthetic outside aes(). For example, you could use geom_sf(bike_thefts, colour = \"mediumblue\") to make all the shapes in that layer blue.\nIf you want to vary the appearance of shapes according to values in the data, you should control the aesthetic inside aes(). For example, you could use geom_sf(aes(colour = month), bike_thefts) to vary the colour of shapes in a layer according to values of the month column in the data.\n\nIf you specify a constant value for an aesthetic (e.g. colour = \"mediumblue\") this will over-ride any mapping for that aesthetic provided by the aes() function (e.g. aes(colour = month)). If you have used aes() to specify that an aesthetic should be controlled based on a column in the data but find that the aesthetic is not changing based on the data, check you have not also specified a constant value for that aesthetic.\naes() must be the first argument in the geom_*() function.\n\n\nIn this map, instead of seeing each crime as a separate point, we see the density of crime as the filled colour of cells in a grid. By comparing this density map to the point map we produced before, we can see that the density map makes the areas with the highest frequency of thefts slightly easier to identify (although we will improve it much further below).\nYou can also see that our map now has a legend, showing that higher densities of bike thefts are shown on the map in light blue and lower densities are shown in dark blue. The exact values shown in the legend are not particularly meaningful, so we can ignore these for now.\n\n\n\n\n\n\nDensity mapping\n\n\n\nWhat is a key advantage of using KDE over dot maps for crime data?\n\n It helps make spatial patterns in data more apparent. It shows the exact location of each crime point. It allows for interactive maps that users can click on. It removes crime data points from the map.\n\nWhich R function is used to create a density layer from point data?\n\n ggplot() hotspot_density() hotspot_kde() st_point_to_kde()\n\nWhich argument in geom_sf() is used to specify that the fill colour of grid cells should be determined by the KDE values?\n\n colour fill shape size\n\nWhat does the st_transform() function do in spatial data operations?\n\n It combines multiple spatial datasets into one. It filters spatial data to show only relevant points. It calculates the density of points in a specified area. It changes the coordinate reference system of a spatial dataset.\n\n\n\n\n6.2.1 Fine-tuning cell size and bandwidth\nWe can control the appearance of KDE maps in several ways. For example, we can vary the number of cells in the grid and the definition of what cells the kernel density estimation process should consider to be ‘nearby’ for the purposes of calculating weighted counts. Cells are considered to be ‘nearby’ to a particular cell if they are closer to that cell than a distance known as the KDE bandwidth.\nBy default, hotspot_kde() chooses the cell size and the bandwidth automatically. The maps below show how changing these defaults changes the appearance of our map (with the legend and axes removed to make the small maps clearer).\n\n\n\n\n\n\n\n\n\nBy looking at the maps on the right-hand side, you can see that reducing the number of grid cells leads to a map that looks blocky and lacks information. Looking at the maps towards the top of the figure above, you can see that increasing the bandwidth relative to the default makes the density surface smoother. The smoother the surface, the less detail we can see about where crime is most concentrated, until on the top row we can see almost no information at all. On the other hand, if we reduce the bandwidth too much (the bottom row of maps) then almost no ‘nearby’ cells are included in the count and so it becomes more difficult to identify patterns.\nIn most cases, you will not need to change the cell size used in calculating the density of points on a map, but if you do then you can do this using the cell_size argument to hotspot_kde(). A cell size of about 200 metres is often a good choice for maps showing a whole city, such as our map of Vancouver.\nAlthough you can set the bandwidth manually using the bandwidth argument to hotspot_kde(), you will almost never want to do this. Instead, you can vary the bandwidth relative to the automatically chosen default bandwidth by using the bandwidth_adjust argument. For example, if you wanted to see more detail in your map by using a smaller bandwidth, you could use bandwidth_adjust = 0.75 or bandwidth_adjust = 3/4 to set the bandwidth to be three-quarters of the default bandwidth.\n\n\n\n\n\n\nStart with bandwidth_adjust = 0.5\n\n\n\nI recommend using a slightly smaller bandwidth than the default, so that your maps show a bit more detail. Try setting bandwidth_adjust = 0.5 whenever you produce a density layer using hotspot_kde(), but remember to look at the map to see if you are happy with the result. If you are not happy, try varying the value of bandwidth_adjust. It is possible to set a value greater than 1, but it is very unlikely that this would produce the most-informative possible map.\n\n\n\n\n6.2.2 Using colour to show density\nThe final way we can control the appearance of our density layer is to change the colour scheme used to represent density. To do this, we can use another type of function from the ggplot2 package: scales. Functions in the scale_*() family of functions allow us to control how the mapping between a column in the data and an aesthetic (colour, size, etc.) is represented visually. For example, which colour scheme is used to represent the density of thefts on our map.\nThere are many scale_*() functions, but the scale_fill_distiller() function produces several different colour scales that are specifically designed to be effective on maps.\nColour schemes can be divided into three types:\n\nsequential colour schemes are useful for showing values from low to high,\ndiverging colour schemes are useful for showing values relative to a meaningful central point, and\nqualitative colour schemes are useful for showing separate categories that can appear in any order and still be meaningful.\n\nIn crime mapping we’re usually interested in showing how crime varies from low to high, so we need to use a sequential colour palette. There are 18 sequential colour schemes (or palettes) available in scales_fill_distiller(), each with a name:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nChoosing the right type of colour scale\n\n\n\nIt is important to only use the right type of colour scale in the right circumstances, since using the wrong type of scale could end up misleading people reading your map. For example, a diverging colour scale gives the strong impression that the central point in the scale is meaningful.\nIn some circumstances this might be useful, for example if you wanted to show areas in which crime had increased in shades of one colour and areas in which crime had decreased in shades of another colour. In that case, a diverging scale would be appropriate because the central point represents something meaningful: no change in crime. If the central point is not meaningful, use a sequential colour scheme instead.\nIf you want to represent a categorical variable, you should use a categorical colour scale unless the categories have a natural order. For example, if you wanted to show ethnic groups on a map you would use a categorical colour scale, since there is no one order of ethnic groups that is any more meaningful than any other. If you wanted to represent days of the week with colour, then you might want to use a sequential colour scheme since the days of the week have a meaningful order.\n\n\nBy default, scale_fill_distiller() sets the lowest values to have the darkest colour. This often does not work well, but we can change this by setting the argument direction = 1. I recommend doing this in all cases.\n\n\n\n\n\n\n\n\n\nYou can think of all the functions that we can add to ggplot() as being like a stack of pancakes, with each new function being placed on the top of the stack. To change the colour of our map, we just add scale_fill_distiller() to the existing stack. Add this code to the script file chapter_06.R in RStudio:\n\n\n\nchapter_06.R\n\n# Plot density map\nggplot() +\n  geom_sf(aes(fill = kde), data = bike_theft_density, colour = NA) +\n  scale_fill_distiller(palette = \"Oranges\", direction = 1)\n\n\n\n\n\n\n\n\n\n\nYou can find out much more about how colours work in R by reading Working with colours in R and find many different colour palettes you can use with ggplot2 at the Color Palette Finder.\n\n\n\n\n\n\n\nMaking density maps\n\n\n\nWhat R package do we use to make density maps using the hotspot_kde() function?\n\n dplyr ggplot2 sf sfhotspot\n\nWhat is the bandwidth_adjust argument of the function hotspot_kde() used for?\n\n Adjusting the bandwidth of the density layer, relative to the default bandwidth Adjusting the cell size of the density layer, relative to the default cell size Adjusting the colour scheme used to represent density on the map Adjusting the bandwidth of the density layer, relative to a bandwidth of zero\n\nIn what circumstances is it appropriate to use a diverging colour scale, e.g. one that goes from blue to white to red?\n\n To represent variation in a variable from low to high To represent variation in a variable above and below a meaningful mid-point To represent variation in a variable from high to low To represent categories in a categorical variable\n\nWhy is it a bad idea to use a diverging colour scheme for crime density maps?\n\n It highlights low-density areas more clearly. It can mislead the viewer by suggesting a central point of significance. It makes the map look too colorful, which can be distracting. It hides the areas with the highest crime density.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Mapping crime patterns</span>"
    ]
  },
  {
    "objectID": "06_mapping_crime_patterns/index.html#clipping-map-layers",
    "href": "06_mapping_crime_patterns/index.html#clipping-map-layers",
    "title": "6  Mapping crime patterns",
    "section": "6.3 Clipping map layers",
    "text": "6.3 Clipping map layers\nThere is one limitation of the KDE layer on our map that we need to deal with. The area covered by the KDE layer is determined by the area covered by the point data that we provided to hotspot_kde(). More specifically, hotspot_kde() will calculate density values for every cell in the convex hull around the point data, i.e. for the smallest polygon that contains all the points in the data.\nThis can be a problem in some circumstances, because we do not necessarily have crime data for all the areas within the convex hull of the data, even though KDE values will be calculated for those areas. This could be misleading, since it will look like such areas have low crime density, when in fact we do not know what the density of crime in such areas is.\nFortunately, we can easily deal with this problem by clipping the KDE layer to the boundary of the area for which we have crime data. This means we will only show densities for cells for which we actually have data.\n\n\nA. We only have data on bike thefts from the City of Vancouver, so all the bike thefts in the data necessarily occurred within the city. We do not know what the density of crime outside the city is.\n\n\nB. The KDE function only knows the theft locations, not the area in which thefts could have occurred. So the convex hull created by the KDE layer will not necessarily match the area of the data.\n\n\n\n\n\nC. In this case, that means some areas (shaded) will be included in the KDE layer even though they happened outside the area covered by the data, which could be misleading.\n\n\nD. To avoid suggesting we know the density of crimes in areas for which we do not have data, we should clip the KDE layer to the boundary of the area for which we have data.\n\n\n\nTo clip the KDE layer to the boundary of the area for which we have data, we need a new dataset that contains the boundary of the City of Vancouver. Fortunately this dataset is available online in a spatial format known as a GeoJSON file. GeoJSON is a spatial file format, so we can load it in R using the read_sf() function from the sf package.\nRather than add the code needed to load this dataset to the end of our script file, it will be neater to add it to the point in the file where we load the other data. That way, if we come back to the file later, all the code that loads datasets will be in one place and it will be easy to see what data is loaded by that script. Add this code to the chapter_06.R file, after the pipeline that loads and wrangles the bike-theft data and before code that creates the bike_theft_density object.\n\n\n\nchapter_06.R\n\n# Load Vancouver neighbourhood boundaries\nvancouver_nbhds &lt;- read_sf(\"https://mpjashby.github.io/crimemappingdata/vancouver_neighbourhoods.geojson\") |&gt; \n  st_transform(\"EPSG:32610\")\n\n\nYou might have noticed that as well as loading the boundary data with read_sf(), we have transformed the data to use the same co-ordinate system (EPSG:32610) as is used for the bike_thefts object. This is because we can only clip datasets that use the same co-ordinate system.\nBefore we clip the density layer to the city boundary, check that your file chapter_06.R looks like this:\n\n\n\nchapter_06.R\n\n# Load packages\npacman::p_load(sf, sfhotspot, tidyverse)\n\n# Load and wrangle bike theft data\nbike_thefts &lt;- read_csv(\"https://mpjashby.github.io/crimemappingdata/vancouver_thefts.csv.gz\") |&gt;\n  janitor::clean_names() |&gt; \n  st_as_sf(coords = c(\"x\", \"y\"), crs = \"EPSG:32610\") |&gt; \n  filter(type == \"Theft of Bicycle\")\n\n# Load Vancouver neighbourhood boundaries\nvancouver_nbhds &lt;- read_sf(\"https://mpjashby.github.io/crimemappingdata/vancouver_neighbourhoods.geojson\") |&gt; \n  st_transform(\"EPSG:32610\")\n\n# Estimate density of bike thefts\nbike_theft_density &lt;- hotspot_kde(\n  bike_thefts, \n  bandwidth_adjust = 0.5, \n  quiet = TRUE\n)\n\n# Plot density map\nggplot() +\n  geom_sf(aes(fill = kde), data = bike_theft_density, colour = NA) +\n  scale_fill_distiller(palette = \"Oranges\", direction = 1)\n\n\nWe can clip the KDE layer produced by hotspot_kde() to the boundary of the City of Vancouver using the st_intersection() function from the sf package. st_intersection() removes any rows from the dataset provided as the first argument that do not fall within the area covered by the dataset provided as the second argument. We can use st_intersection() to remove any cells in bike_theft_density that are outside the city outline stored in vancouver_nbhds.\nWe will now add the code needed to clip the KDE layer to the city boundary to the chapter_06.R script file. Since we do not need the unclipped version of the KDE layer, we don’t need to separately estimate the density and clip the result. Instead, we can combine the call to st_intersection() with the existing call to hotspot_kde() to form a pipeline using the pipe (|&gt;) operator. To do that, in the chapter_06.R file replace the line that creates the bike_theft_density object with this code:\n\n\n\nchapter_06.R\n\n# Estimate density of bike thefts and clip result\nbike_theft_density_clip &lt;- bike_thefts |&gt; \n  hotspot_kde(bandwidth_adjust = 0.5, quiet = TRUE) |&gt; \n  st_intersection(vancouver_nbhds)\n\n\nWarning: attribute variables are assumed to be spatially constant throughout\nall geometries\n\n\nWe can read this code as saying:\n\ntake the bike_thefts object,  … use it to estimate the density of thefts … and then clip the resulting density object to the boundary stored in the vancouver_nbhds object.\n\nIf you aren’t comfortable with how the pipe operator works, you might want to refer back to Section 3.9.\n\n\n\n\n\n\nst_intersection() requires both objects to use the same co-ordinate system\n\n\n\nThe st_intersection() function requires that both spatial layers have the same co-ordinate system. If the two layers use different co-ordinate systems, you will need to transform one of the layers using st_transform() so that it uses the same co-ordinate system as the other layer.\nIf you do not know which co-ordinate systems the layers use, you can use the st_crs() function to extract the co-ordinate system from one layer and pass that value as the second argument to st_transform(). For example:\nst_transform(bike_theft_density, crs = st_crs(vancouver_nbhds))\n\n\n\n\n\n\n\n\nWarning message: attribute values are assumed to be spatially constant\n\n\n\nst_intersection() produces a warning message:\nWarning: attribute variables are assumed to be spatially \nconstant throughout all geometries\nAs long as you are simply using st_intersection() to remove parts of the data outside a boundary, you can ignore this message.\n\n\nWe can quickly check the effect of clipping the density layer to the city boundary by producing a very basic map that shows both layers in separate colours. Run this code in the R Console:\n\n\n\nR Console\n\nggplot() +\n  geom_sf(data = bike_theft_density, fill = \"blue\") +\n  geom_sf(data = bike_theft_density_clip, fill = \"green\") +\n  geom_sf(data = vancouver_nbhds, colour = \"black\", fill = NA, linewidth = 1)\n\n\n\n\n\n\n\n\n\nFrom this map, we can see that while the bike_theft_density object (shown in blue) contains cells outside the black city boundary, the bike_theft_density_clip object (shown in green) contains only cells inside the boundary.\n\n\n\n\n\n\nQuiz\n\n\n\nWhat is the purpose of the st_intersection() function in spatial data manipulation?\n\n To transform one dataset into another coordinate system. To clip a dataset to the boundary of another spatial layer. To merge multiple spatial layers into one. To calculate the centroid of spatial objects.\n\nWhy is it important to clip a KDE layer to a relevant boundary in crime mapping?\n\n To remove unnecessary layers. To reduce the overall size of the dataset. To improve the map’s aesthetic appearance. To avoid showing incorrect density estimates for areas without data.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Mapping crime patterns</span>"
    ]
  },
  {
    "objectID": "06_mapping_crime_patterns/index.html#sec-base-maps",
    "href": "06_mapping_crime_patterns/index.html#sec-base-maps",
    "title": "6  Mapping crime patterns",
    "section": "6.4 Adding a base map",
    "text": "6.4 Adding a base map\nThe density map we have made is much more effective than a point map at allowing us to identify where the highest number of bike thefts in Vancouver occur. However, it’s still quite difficult to know where those places are, because we cannot easily work out where in the city these places are. We can make this much easier by adding a base map underneath the density layer.\nWe can add a base map using annotation_map_tile() function from the ggspatial package. We can add annotation_map_tile() to a ggplot() stack in the same way that we would add geom_*() functions. Since we want the base map to appear under the density layer, we add the base map layer to the ggplot() stack first. So that we can see the base map underneath the density layer, we will also make the density layer semi-transparent using the alpha argument to geom_sf().\nReplace the existing ggplot() stack in the chapter_06.R file with this code and then run it:\n\n\n\nchapter_06.R\n\nggplot() +\n  annotation_map_tile(zoomin = 0, progress = \"none\") +\n  geom_sf(\n    aes(fill = kde), \n    data = bike_theft_density_clip, \n    alpha = 0.67, \n    colour = NA\n  ) +\n  scale_fill_distiller(palette = \"Purples\", direction = 1)\n\n\n\n\n\n\n\n\n\nThe base maps returned by annotation_map_tile() are available at many different zoom levels, from level 1 that is useful for mapping the whole world in one map, to level 20 that can be used to map a single building. By default, annotation_map_tile() downloads tiles with slightly less detail than we might want, but we can fix this by using the argument zoomin = 0. We could also set a specific zoom level using the zoom argument.\nFor example, these maps show the same area around the UCL Jill Dando Institute with base maps at different zoom levels.\n\n\n\n\n\n\n\n\n\nChoosing the right zoom level is a matter of balancing the level of detail in the map and the clarity of the image. In the maps above, zoom levels less than 12 tend to have pixelated images because they do not contain enough detail, while zoom levels greater than 12 contain too much detail and so the information is hard to read. But if this map covered a smaller or larger area, a different zoom level might be better. In general, setting zoomin = 0 and not setting any value for the zoom argument – so that annotation_map_tile() chooses the zoom level automatically – will produce an acceptable map.\nannotation_map_tile() also gives us access to several different types of base map. The default style (seen in the maps above) is called ‘osm’ because it is the default style used by Open Street Map, the organisation that provides the map data. We can specify which style of base map we want using the type argument to annotation_map_tile().\n\n\n\n\n\n\n\n\n\nYou may want to experiment with different base map styles by using the type argument to annotation_map_tile(), e.g. using annotation_map_tile(type = \"cartolight\", zoomin = 0, progress = \"none\").\nOne final note about the annotation_map_tile() function: you might have noticed that when we have used it above we have always set the argument progress = \"none\". This stops the function from printing a progress bar while it is downloading the map tiles. The progress bar can sometimes be useful, but when you include the output from an R function in a report (as you will learn to do in Chapter 14), the output from the progress bar is likely to interfere with the formatting of the report. To prevent that, we use the progress = \"none\" argument.\n\n\n\n\n\n\nBase maps\n\n\n\nWhat is the purpose of adding a base map to a crime map in R?\n\n To provide information about socio-economic conditions in different places. To provide context on the locations of concentrations of crime. To stop the map from appearing too visually cluttered To make a map more visually attractive.\n\nWhich R package is commonly used for adding base maps to crime maps?\n\n ggplot2 ggmap ggspatial sfhotspot\n\nWhy might you need to be careful in choosing the right base map when creating a crime map?\n\n To ensure that it does not distract from the crime data. To fit the base map to the full extent of the dataset. To remove all geographic details and focus only on crime locations. To add unnecessary elements to the map.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Mapping crime patterns</span>"
    ]
  },
  {
    "objectID": "06_mapping_crime_patterns/index.html#sec-other-layers",
    "href": "06_mapping_crime_patterns/index.html#sec-other-layers",
    "title": "6  Mapping crime patterns",
    "section": "6.5 Adding more layers",
    "text": "6.5 Adding more layers\n\nAdding a base map underneath our density layer makes it easier to understand where in Vancouver the highest densities of bike theft are. But our map could make it easier still to see where clusters of thefts occur. We could, for example, add the names of different neighbourhoods in the city, and show the city limits so that we can tell which areas have no crime because crimes in those areas are not included in our data.\nIn an earlier section I suggested we can think of ggplot charts, including maps, as being like stacks of pancakes – each function we use to amend the appearance of our chart is added to the top of the stack. So to add another layer to our map, we just add another geom_ function to our plot.\n\nAt the same time, we can also add labels to the plot at the centre of each neighbourhood using the geom_sf_label() function. geom_sf_label() works like geom_sf() in that it adds a spatial dataset to a map, but instead of adding the points, lines or polygons themselves, geom_sf_label() instead adds labels for those features.\nWe use the aes() function to specify which column in the vancouver_nbhds dataset we want to use for the label text. If we used head(vancouver_nbhds) to look at the columns in the data, we would see the neighbourhood names are contained in the column called name. Normally, we would use the code aes(label = name) to do this because the label aesthetic is used to set the label text. But in this case we also want to wrap the labels so that they don’t overlap adjacent neighbourhoods. To do this we can use the str_wrap() from the stringr package (part of the tidyverse), so that instead our call to the aes() function becomes aes(label = str_wrap(name, width = 10)).\nTo see how geom_sf_label() works, let’s create a quick map in the R Console. Paste the following code into the Console and run it:\n\n\n\nR Console\n\nggplot() +\n  geom_sf(data = vancouver_nbhds) +\n  geom_sf_label(aes(label = str_wrap(name, width = 10)), data = vancouver_nbhds)\n\n\n\n\n\n\n\n\n\nNow let’s add these labels to the map in our script file. At the same time we will change the style of the labels so that they look better on the map. Do do this, we will add several more arguments to geom_sf_label():\n\nalpha = 0.5 to make the label background semi-transparent so that we can see the density layer underneath it,\ncolour = \"seagreen3\" to slightly reduce the prominence of the label text to avoid distracting attention from the density layer,\nlineheight = 1 to reduce the gap between lines in each label,\nsize = 2.5 to slightly reduce the size of the label text,\nlabel.size = NA to remove the default border around the label background.\n\nReplace the existing ggplot() stack in the chapter_06.R file with this stack and then run that code:\n\n\n\nchapter_06.R\n\n# Plot density map\nggplot() +\n  # Add base map\n  annotation_map_tile(type = \"cartolight\", zoomin = 0, progress = \"none\") +\n  # Add density layer\n  geom_sf(\n    aes(fill = kde), \n    data = bike_theft_density_clip, \n    alpha = 0.75, \n    colour = NA\n  ) +\n  # Add neighbourhood boundaries\n  geom_sf(data = vancouver_nbhds, colour = \"seagreen3\", fill = NA) +\n  # Add neighbourhood names\n  geom_sf_label(\n    aes(label = str_wrap(name, 10)), \n    data = vancouver_nbhds, \n    alpha = 0.5,\n    colour = \"seagreen\", \n    lineheight = 1, \n    size = 2.5,\n    label.size = NA\n  ) +\n  # Set the colour scale\n  scale_fill_distiller(direction = 1)\n\n\n\n\n\n\n\n\n\nFrom this map, we can see that bike theft in Vancouver is heavily concentrated in a handful of neighbourhoods, particularly the Downtown and West End neighbourhoods. This map is much more useful than the first map that we produced in Chapter 4 showing only the point location of each crime, since in this latest map we can see not only the greatest concentrations of bike thefts but how they relate to the different areas of the city.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Mapping crime patterns</span>"
    ]
  },
  {
    "objectID": "06_mapping_crime_patterns/index.html#in-summary",
    "href": "06_mapping_crime_patterns/index.html#in-summary",
    "title": "6  Mapping crime patterns",
    "section": "6.6 In summary",
    "text": "6.6 In summary\nIn this chapter we have learned to produce a density map of crime. This type of map can be very useful in identifying where practitioners should focus efforts to respond to crime. For example, a map like this might help local police to decide where to send officers to carry out extra patrols, while a crime-prevention charity might decide to run events in particular areas to educate people on how best to protect their bikes.\nNow that we have learned all the code for this chapter, your code file should look like this:\n\n\n\nchapter_06.R\n\n# Load packages\npacman::p_load(sf, sfhotspot, tidyverse)\n\n# Load and wrangle bike theft data\nbike_thefts &lt;- read_csv(\"https://mpjashby.github.io/crimemappingdata/vancouver_thefts.csv.gz\") |&gt;\n  janitor::clean_names() |&gt; \n  st_as_sf(coords = c(\"x\", \"y\"), crs = \"EPSG:32610\") |&gt; \n  filter(type == \"Theft of Bicycle\")\n\n# Load Vancouver neighbourhood boundaries\nvancouver_nbhds &lt;- read_sf(\"https://mpjashby.github.io/crimemappingdata/vancouver_neighbourhoods.geojson\") |&gt; \n  st_transform(\"EPSG:32610\")\n\n# Estimate density of bike thefts and clip result\nbike_theft_density_clip &lt;- bike_thefts |&gt; \n  hotspot_kde(bandwidth_adjust = 0.5, quiet = TRUE) |&gt; \n  st_intersection(vancouver_nbhds)\n\n# Plot density map\nggplot() +\n  # Add base map\n  annotation_map_tile(type = \"cartolight\", zoomin = 0, progress = \"none\") +\n  # Add density layer\n  geom_sf(\n    aes(fill = kde), \n    data = bike_theft_density_clip, \n    alpha = 0.75, \n    colour = NA\n  ) +\n  # Add neighbourhood boundaries\n  geom_sf(data = vancouver_nbhds, colour = \"seagreen3\", fill = NA) +\n  # Add neighbourhood names\n  geom_sf_label(\n    aes(label = str_wrap(name, 10)), \n    data = vancouver_nbhds, \n    alpha = 0.5,\n    colour = \"seagreen\", \n    lineheight = 1, \n    size = 2.5,\n    label.size = NA\n  ) +\n  # Set the colour scale\n  scale_fill_distiller(direction = 1)\n\n\n\n\nRows: 21918 Columns: 10\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (3): TYPE, HUNDRED_BLOCK, NEIGHBOURHOOD\ndbl (7): YEAR, MONTH, DAY, HOUR, MINUTE, X, Y\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nWarning: attribute variables are assumed to be spatially constant throughout\nall geometries\n\n\n\n\n\n\n\n\n\nAs you read through this code, you might notice that it’s easier to understand than it otherwise might be because:\n\nThe code is in a logical order: it starts with loading the necessary packages and data, then wrangles that data and finally produces the map.\nThe code includes only the permanent code needed to produce the final map, not any temporary code that we needed to produce the permanent code. If you aren’t sure about the distinction between permanent and temporary code, look back at Section 2.2.\nEach part of the code is annotated with a comment: a line that starts # and explains what the following block of code does.\n\nSave the chapter_06.R file, then restart R to start a new session by clicking on the Session menu and then clicking Restart R. This creates a blank canvas for the next chapter.\n\n\n\n\n\n\nTips for producing effective density maps\n\n\n\n\nDensity layers on maps (e.g. a layer added using geom_sf() to display the result produced by hotspot_kde()) should be made semi-transparent so that readers can see the base map underneath. If a density layer is not semi-transparent then it is likely to be very difficult for readers to see exactly where areas of high density are in the real world. Try setting the argument alpha = 0.7 in the call to the geom_sf() function, then change that value until you are happy with the visibility of both the density layer and the base map underneath.\nDensity layers should almost always only show a single type of crime – avoid calculating a single KDE layer based on data that includes more than one type of crime. There are two problems with combining data for multiple crime types to produce a single density layer. First, different crimes often concentrate in different places, so a combined map might end up showing concentrations inaccurately. Second, since the KDE process is based on the number of points, a density layer produced by combining data for multiple crimes will inevitably be more influenced by whichever crime type is more numerous. Since more minor crimes tend to be more common, this could mean that your density map points people towards areas with lots of minor crimes and away from places where more-serious crimes happen.\nAvoid mapping ‘intangible’ crimes. These are crimes that are only ever recorded by police when officers happen to identify a crime while on patrol, rather than the crime usually being reported by the victim or a third-party. You should avoid mapping these types of crime because they generally reflect geographic concentrations of police patrol more than geographic concentrations of the crimes themselves. The most common intangible offences are drugs possession and weapons possession, which are detected incidentally by officers on patrol much more than they are reported to the police by the public.\n\n\n\n\n\n\n\n\n\nRevision questions\n\n\n\nAnswer these questions to check you have understood the main points covered in this chapter. Write between 50 and 100 words to answer each question.\n\nWhat is kernel density estimation (KDE), and why is it useful for mapping crime patterns? Explain the process briefly.\nWhy are density maps often more effective than point maps for visualizing crime patterns? Provide an example to illustrate your answer.\nWhat are the main factors to consider when adjusting the cell size and bandwidth for a KDE map? How can these adjustments affect the map’s appearance and usefulness?\nWhy is it important to clip density maps to the boundary of the area being analysed? Describe the steps required to achieve this in R.\nHow does the choice of colour scale affect the interpretation of density maps? Provide an example of when to use sequential, diverging, and categorical colour schemes.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Mapping crime patterns</span>"
    ]
  },
  {
    "objectID": "07_map_context/index.html",
    "href": "07_map_context/index.html",
    "title": "7  Giving a map context",
    "section": "",
    "text": "7.1 Introduction\nIn this chapter we will create this map of shootings in the Bronx borough of New York City in 2019.\nYou can see that – unlike the maps we have made so far – this map includes contextual elements such as a title, a legend and a scale bar. In this chapter we will learn how to add each of these elements to a map, and (just as importantly) when you shouldn’t use them.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Giving a map context</span>"
    ]
  },
  {
    "objectID": "07_map_context/index.html#map-choices",
    "href": "07_map_context/index.html#map-choices",
    "title": "7  Giving a map context",
    "section": "7.2 Map choices",
    "text": "7.2 Map choices\nAmong the most important decisions you make when you are creating a map is what information to include and what to leave out. Watch this video to learn more about why this is important and how you can make those decisions.\n\n\n\n\n\n\n\nMap choices\n\n\n\nWhich statement best summarizes the key points of the video?\n\n Map design is mostly about choosing the right colors and symbols Map makers must consider their audience, the purpose of the map, and how it will be used The most detailed maps are always the most effective Maps should include as much information as possible to avoid confusion\n\nWhat is one potential benefit of leaving out information on a map?\n\n It makes the map more colorful It makes the map less useful It allows for more artistic expression It ensures the audience focuses on the main message\n\nWhat is one way that maps can unintentionally mislead their audience?\n\n By using symbols that are too small By including too much detail By making large areas of color seem more important than they actually are By leaving out street names\n\n\n\nThe most important thing to remember when designing a map is to keep in mind the purpose that the map will be used for. Research on how people use maps has repeatedly shown that “the nature of the task or function to be performed by the map user is the single most important factor in determining how [someone] processes the information on the map”.\nAs explained in the video, when you create a crime map you should ask yourself:\n\nWhat will readers be using this map for?\nWhat information do readers need?\nWhat do readers already know?\nIn what context are people going to use this map?\nHow could this map lead the audience to a wrong conclusion?\n\nMaps are powerful communication tools, which can sometimes knowingly or unknowingly mislead the reader. Watch this video to learn more about how maps can be misleading.\n\n\n\n\n\n\n\nDo maps lie?\n\n\n\nAccording to the video, why should we critically evaluate maps?\n\n Maps are always based on outdated data Maps are neutral and require interpretation Maps can be created to deliberately influence or mislead viewers Maps are only useful for navigation, not analysis\n\nWhat is the main difference between Jim’s and Anna’s maps of London house prices?\n\n Jim uses outdated data, while Anna uses current data Jim uses an equal interval classification, while Anna compares London prices to the UK average Jim’s map shows rental prices, while Anna’s map shows purchase prices Jim’s map focuses on urban areas, while Anna’s map focuses on rural areas\n\nWhat three questions should you ask when evaluating a map?\n\n Where was it made? Who paid for it? What colors are used? Is the data current? How is it classified? Is it geographically accurate? What software was used? What is the scale? How many layers does it have? Who made it? Why did they make it? What does it tell us?\n\n\n\nWhenever you make a map, think about your own biases – are your own views on a topic likely to influence the results of your analysis? One way to test your own assumptions about a topic is to test them against other potential assumptions using an approach to crime analysis called hypothesis testing. To find out more about the hypothesis testing approach, read the paper Improving the explanatory content of analysis products using hypothesis testing.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Giving a map context</span>"
    ]
  },
  {
    "objectID": "07_map_context/index.html#sec-visual-hierarchy",
    "href": "07_map_context/index.html#sec-visual-hierarchy",
    "title": "7  Giving a map context",
    "section": "7.3 Visual hierarchy",
    "text": "7.3 Visual hierarchy\nMaps are among the most complex types of data visualisation. Even if we have chosen wisely what to include and what to leave out, there is likely to be lots of information on our map. For all but the simplest maps, there is a risk of readers – especially those in a hurry – might be overwhelmed or mislead by competing pieces of information (such as different layers of data) on a map.\nTo help readers understand what parts of a map they should focus most of their attention on and which are of less importance, we can establish a visual hierarchy. Watch this video to learn more about visual hierarchies in mapping.\n\n\n\n\n\n\n\nVisual hierarchy\n\n\n\nWhat is the main goal of establishing a visual hierarchy in maps?\n\n To make all elements of the map equally visible To make maps look more colorful To guide readers’ attention to the most important elements To eliminate unnecessary details from the map\n\nHow does isolation affect the prominence of an element on a map?\n\n Isolated elements become less visible Isolated elements become more visually prominent Isolation only affects text, not other visual elements Isolation does not impact visual prominence\n\nHow does text size contribute to visual hierarchy?\n\n Larger text makes elements more prominent than smaller text All text should be the same size for consistency Smaller text is always easier to read Text size has no impact on visual hierarchy\n\n\n\nWe have used some of the principles of visual hierarchy in the maps we have already made. For example, in the density map of bike thefts in Vancouver, we used strong colours to represent the data and shades of grey for the base map. This helped readers intuitively understand that they should focus most attention on the data.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Giving a map context</span>"
    ]
  },
  {
    "objectID": "07_map_context/index.html#supporting-elements",
    "href": "07_map_context/index.html#supporting-elements",
    "title": "7  Giving a map context",
    "section": "7.4 Supporting elements",
    "text": "7.4 Supporting elements\nWe can often make our maps much more useful by adding supporting elements that explain the map content, give context or provide extra information. Supporting elements include titles, captions, legends, base maps, scale bars and north arrows.\n\nWe will not need to include every supporting element mentioned in the video in all the maps we make. The visual hierarchy that you establish in your map by use of size, colour, etc. should make it clear which elements are most important. The data should always come first in the visual hierarchy, usually followed by the title and then the legend. Other elements should be lower down the hierarchy. In every map, the supporting elements should be designed so that they do not distract from the data.\n\n\n\nVisual hierarchy of elements in a crime map\n\n\nplace in hierarchy\nmap element\nhow often needed\n\n\n\n\n1st\ndata layers\nalways\n\n\n2nd\ntitle\nvirtually always\n\n\n3rd\nlegend\nusually\n\n\n4th\nbase map\nalmost always\n\n\n5th\nauthor and date\nvirtually always\n\n\n=6th\nscale\nsometimes\n\n\n=6th\nnorth arrow\nsometimes\n\n\n7th\ngrid\nrarely\n\n\n\n\n\nElements that are almost always needed on a crime map are not necessarily highest on the visual hierarchy. For example, the author name is virtually always needed, but is relatively low on the visual hierarchy. This is because it is important information for readers who need it to judge the reliability of a map, or to get in touch to ask questions, but should not distract from the data for those readers who do not need it.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Giving a map context</span>"
    ]
  },
  {
    "objectID": "07_map_context/index.html#creating-and-storing-a-map",
    "href": "07_map_context/index.html#creating-and-storing-a-map",
    "title": "7  Giving a map context",
    "section": "7.5 Creating and storing a map",
    "text": "7.5 Creating and storing a map\nSince we will be adding various elements to a map in this chapter, we will first create a map and save it as an R object. Any map or chart produced using the ggplot() function can be saved as an object using the assignment operator &lt;-. Just as for the result of any other R function, if we save it to an object the result will not be printed to the screen, but we can easily see the plot by simply typing the object name in the R console.\nCreate a new R script file (in RStudio: File &gt; New File &gt; R Script) and save the file in your project directory as chapter_07.R. Paste this code into it and run that code so that we can use it as the basis for the maps we will make during the rest of the chapter.\n\n\n\nchapter_07.R\n\n# Load packages\npacman::p_load(ggspatial, sf, sfhotspot, tidyverse)\n\n# Load shootings data and transform it to use an appropriate co-ordinate system\nshootings &lt;- read_csv(\n  \"https://mpjashby.github.io/crimemappingdata/bronx_shootings.csv\", \n  show_col_types = FALSE\n) |&gt; \n  st_as_sf(coords = c(\"longitude\", \"latitude\"), crs = \"EPSG:4326\") |&gt; \n  st_transform(\"EPSG:6538\")\n\n# Load NYC police precincts data\nprecincts &lt;- read_sf(\"https://mpjashby.github.io/crimemappingdata/nyc_precincts.gpkg\") |&gt; \n  janitor::clean_names() |&gt; \n  # Filter just those precincts that are in the Bronx (40th to 52nd)\n  filter(precinct %in% 40:52) |&gt; \n  st_transform(\"EPSG:6538\")\n\n# Calculate KDE\nshootings_kde &lt;- shootings |&gt; \n  hotspot_kde(\n    grid = hotspot_grid(precincts, cell_size = 100), \n    bandwidth_adjust = 0.33,\n    quiet = TRUE\n  ) |&gt; \n  st_intersection(precincts)\n\n# Create map object\nshootings_map &lt;- ggplot() +\n  annotation_map_tile(type = \"cartolight\", zoomin = 0, progress = \"none\") +\n  geom_sf(aes(fill = kde), data = shootings_kde, alpha = 0.75, colour = NA) +\n  geom_sf(data = precincts, colour = \"grey33\", fill = NA) +\n  geom_sf_label(\n    aes(label = scales::ordinal(precinct)), \n    data = precincts,\n    alpha = 0.5, \n    colour = \"grey33\", \n    size = 2.5, \n    label.size = NA\n  ) +\n  scale_fill_distiller(palette = \"PuBu\", direction = 1)\n\n\n\n\nWarning: attribute variables are assumed to be spatially constant throughout\nall geometries\n\n\nTo view the map in RStudio, we can just type the name of the map object we have just created into the R Console and press Enter/Return.\n\n\n\nR Console\n\nshootings_map\n\n\n\n\n\n\n\n\n\nThe code used to create this map is very similar to the code we used in one of the previous chapters to make a map of bike theft in Vancouver, although there are a few differences.\nLooking at line 22 of the code above, we have used the grid argument to the hotspot_kde() function, specifying hotspot_grid(precincts, cell_size = 100). By default, hotspot_kde() calculates density estimates (KDE values) for every grid cell within the area covered by the crime data. This is fine when at least a few crimes have occurred in all the parts of the area for which we have data. But if crime is heavily concentrated in a few places and there are large areas with no crimes, the density layer will not cover the whole area for which we have data. In this case, it is important to extend the density layer manually to make it clear that the apparent low density of crime in some places is due to a genuine lack of crime there, rather than because we do not have data for those places.\nWe can do this by providing hotspot_kde() with a grid of cells for which KDE values should be calculated, rather than letting hotspot_kde() do this automatically. In the code above, we have used the hotspot_grid() function to create a grid that covers all the precincts in the Bronx (the area we have data for), and then passed the resulting grid to the grid argument of hotspot_kde(). This ensures that KDE values are calculated for every part of the Bronx.\n\nThe only other difference from the Vancouver map is that we have used the ordinal() function from the scales package to convert the precinct numbers to ordinal numbers (1st, 2nd, 3rd, etc.) for the map labels. This is because police precincts in New York City are usually referred to using ordinal numbers (e.g. “the 1st Precinct” rather than “Precinct 1”) and it will be easier for people to read the map if it uses terms they are familiar with.\n\n\n\n\n\n\nDon’t always use ordinal numbers for labels\n\n\n\nWe are using ordinal numbers (1st, 2nd, etc.) for the labels because that is how police precinct numbers are usually expressed in New York City. That does not mean that we should always use wrap map labels in the ordinal() function. How we present map labels (and every other type of information on a map) depends on what is best for the audience we are making the map for.\n\n\nThere are many other functions in the scales package that format numbers in different ways, including comma() to add thousands separators to numbers and dollar() to format numbers as values in dollars or other currencies. There is a full list of scales functions on the package website.\nWe now have a basic map of shootings in the Bronx. This map isn’t good enough on its own, but we can use it to learn how to add supporting elements to a map.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Giving a map context</span>"
    ]
  },
  {
    "objectID": "07_map_context/index.html#ggplot2-themes",
    "href": "07_map_context/index.html#ggplot2-themes",
    "title": "7  Giving a map context",
    "section": "7.6 ggplot2 themes",
    "text": "7.6 ggplot2 themes\nSo far we have learned about several families of functions we can add to a ggplot() stack to help construct a map. For example, we have learned about the geom_*() family of functions for adding data layers to a map and the scale_*() family of functions for controlling how different data values are visually represented. Now we need to learn about another family: themes.\nThemes in ggplot2 affect how the supporting elements of a map appear. By supporting elements, we mean all the parts of the map that don’t depend on the data. Supporting elements include titles, axes and legends.\nThere are many different themes that you can add to a ggplot() stack. Some of these are built into the ggplot2 package, and some are part of other packages that extend what ggplot2 can do. Some organisations (such as the BBC) have their own ggplot2 themes for creating maps and charts that follow their corporate style guide.\nDifferent themes can give maps (and charts) very different appearances, even when they use the same data. For example, these four maps show what happens when different themes are applied to the end of the same ggplot() stack:\n\n\n\n\n\n\n\n\n\nYou can see that the differences between theme_grey() (the default theme) and theme_classic() are quite subtle, but in other cases (such as theme_wsj() – a theme following the house style of the Wall Street Journal newspaper) the differences are quite substantial.\nThree of the themes shown above add x (horizontal) and y (vertical) axis labels to the map. Axis labels are very useful for most types of plot, but usually not for maps. It’s very unlikely that someone looking at this map of shootings in the Bronx will benefit from knowing that the map covers the area from 40.80 degrees north of the equator to 40.92 degrees north. For this reason, when we make a map we will usually add theme_void() to our ggplot() stacks, since this removes unnecessary elements such as axis labels. Removing these elements is useful because it allows readers to focus more on the parts of the map that convey useful information.\n\n\n\n\n\n\nggplot2 themes\n\n\n\nWhat parts of the map are controlled by the function we choose from the theme_*() family?\n\n The data layers on the map, such as the density layer. The non-data elements, like titles and axis labels. The scale of the map. What style of base map will be used in the map.\n\nWhat does the theme_void() function do in a map created using ggplot2?\n\n It removes the title from the map. It changes the color of the map background. It places gridlines on top of the data layers. It removes unnecessary elements like axis labels.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Giving a map context</span>"
    ]
  },
  {
    "objectID": "07_map_context/index.html#sec-titles",
    "href": "07_map_context/index.html#sec-titles",
    "title": "7  Giving a map context",
    "section": "7.7 Titles",
    "text": "7.7 Titles\nA map title is one of the most important ways to add context to a map. Titles can either be descriptive or declarative. Descriptive titles simply state what data are shown on the map. For example, we might give our map the title “Shootings in the Bronx, 2019”. Declarative titles, on the other hand, state what you think the main conclusion should be that readers remember about the map. For example, we might use the title “Shootings are focused in the South Bronx”.\nDeclarative titles are usually more useful than descriptive titles because they help the reader to interpret the map. But writing a good declarative title is harder than writing a descriptive title, because it requires you to think about what is the main point that you want to make with the map. To help you come up with a good declarative title, you might want to try several different titles so that you can choose the one that communicates your message most clearly.\nWe can add a title to our map using the labs() (short for ‘labels’) function from the ggplot2 package. We can use labs() to add labels to various different parts of a map or plot, but for now we will just use the argument title to set the title.\nTo see how this works, run the following code in the R Console. Since we have stored the rest of the ggplot() stack in the shootings_map object, this code is equivalent to adding labs() to the end of the original stack. This code also adds theme_void() to the stack.\n\n\n\nR Console\n\nshootings_map + \n  labs(title = \"Shootings are focused in the South Bronx\") +\n  theme_void()\n\n\n\n\n\n\n\n\n\nSometimes our preferred title might be too long to fit on a map. In this case, we can break the title across two or more lines. We can do this manually by adding the characters \\n (the character code for a new line) at the point where we want the text to start a new line. Alternatively, we can use the str_wrap() function from the stringr package to wrap the text automatically into lines of a given maximum length (specified using the wrap argument).\nWhen you use a declarative title for your map, it is often useful to provide a subtitle containing descriptive information. Adding a subtitle is very easy using the subtitle argument to the labs() function.\n\n\n\nR Console\n\nshootings_map + \n  labs(\n    title = \"Shootings are focused in the South Bronx\",\n    subtitle = \"Fatal and non-fatal shootings recorded by NYC Police, 2019\"\n  ) +\n  theme_void()\n\n\n\n\n\n\n\n\n\n\n7.7.1 Using captions to add author and other information\nThe labs() function has another argument that we can use to add text to our map for context. The caption argument is used to add information such as the author, date and source of the data to a map or chart. We can put any information we like into the caption, using str_wrap() or the new-line character \\n if necessary to stop the text overflowing the map.\nRun this code in the R Console and see where on the map the new information appears.\n\n\n\nR Console\n\nshootings_map + \n  labs(\n    title = \"Shootings are focused in the South Bronx\",\n    subtitle = \"Fatal and non-fatal shootings recorded by NYC Police, 2019\",\n    caption = str_glue(\n      \"Author: Joe Bloggs, Date produced: {today()},\\n\",\n      \"Data: https://data.cityofnewyork.us/d/833y-fsy8\\n\",\n      \"Map data from OpenStreetMap\"\n    )\n  ) +\n  theme_void()\n\n\n\n\n\n\n\n\n\nYou will have seen that the caption appears below the map, in smaller text. The positioning and size of the caption help to move it down the visual hierarchy, since information like the date the map was made is not as important as the (declarative) map title.\nThe code for the caption also uses the str_glue() function from the stringr package. str_glue() glues together any number of character strings separated by commas – in this case, we have split the caption into two separate character strings (on separate lines) so that the lines of code do not become too long to easily read.\nstr_glue() can also include the values of R objects and the results of R functions that are placed inside braces ({}). So the code {today()} runs the function today() from the lubridate package (part of the tidyverse) and glues the result (the current date) into the text.\n\n\n\n\n\n\nAcknowledging the data you use\n\n\n\nWhen you use data to make maps, the data provider will often require you to acknowledge the source of that data. This is a legal requirement so it is important that you do this when required.\nThe base maps we use in this course use data from OpenStreetMap, which requires people using its data to acknowledge that they have done so. The easiest way to do this is to add the text “Map data from OpenStreetMap” to the caption of your maps if they use base maps from OpenStreetMap.\n\n\n\n\n7.7.2 Changing the appearance of titles and captions\nWe have added a title, subtitle and caption to our map, but you might not be happy with their appearance. You might want, for example, to move the caption further down the visual hierarchy by making the text smaller and/or a lighter colour, or add some space between the subtitle and the map itself.\nWe can exercise almost complete control over the supporting elements of maps or charts made with ggplot() using the theme() function. theme() allows us to make changes to the settings that are applied by the theme_*() family of functions (such as theme_void()).\n\n\n\n\n\n\ntheme() doesn’t affect data elements\n\n\n\nOne important thing to remember about theme() is that it only controls the non-data elements of a map – nothing you do with the theme() function will have any effect on the data elements of a map (in this case, the layer showing the density of shootings). To change the appearance of data layers within ggplot() maps, use the geom_, and scale_ families of functions as we have learned in Chapter 4 and Chapter 6.\n\n\nThe theme() function has a lot of potential arguments. If you need help using the theme() function (or any function in R) you can view a manual page (including a list of arguments) for the function by:\n\ntyping a question mark followed by the function name without parentheses (e.g. ?theme) into the R console,\ntyping the function name without parentheses into the search box in the Help panel in RStudio, or\nclicking on the function name anywhere in your R script file to place the cursor on the function name, then pressing F1 on your keyboard.\n\nTry opening the manual page for theme() now to see the list of possible arguments it can take. Fortunately, we will not need most of these arguments most of the time – ggplot() has default values built in for every value that can be changed using theme(), and these defaults will be reasonable in almost all cases.\nTo reduce the visual prominence of the map caption, we can change the value of the plot.caption argument to theme(). Since the caption is a text element (rather than a polygon, line, etc.), we can use the helper function element_text() to do this. The following code changes the colour of the caption text to a lighter grey and makes the text smaller relative to the default using the helper function rel() (for relative sizing) – 0.7 means the text will be 70% as big as it would have been by default.\n\n\n\nR Console\n\nshootings_map +\n  labs(\n    title = \"Shootings are focused in the South Bronx\",\n    subtitle = \"Fatal and non-fatal shootings recorded by NYC Police, 2019\",\n    caption = str_glue(\n      \"Author: Joe Bloggs, Date produced: {lubridate::today()},\\n\",\n      \"Data: https://data.cityofnewyork.us/d/833y-fsy8\\n\",\n      \"Map data from OpenStreetMap\"\n    )\n  ) +\n  theme_void() +\n  theme(\n    plot.caption = element_text(colour = \"grey67\", size = rel(0.7))\n  )\n\n\n\n\n\n\n\n\n\nThe helper function element_text() has arguments to control the appearance of text in different ways. As well as colour (or color, either is fine) and size:\n\nfamily controls the font used, e.g. Times New Roman or Helvetica,\nface controls the style of the font, i.e. ‘plain’, ‘italic’, ‘bold’ or ‘bold.italic’,\nhjust controls the horizontal justification of the text, where 0 means left aligned, 0.5 means centred and 1 means right aligned,\nvjust controls the vertical justification, and\nangle controls the angle (in degrees) of the text (0 means horizontal),\nlineheight controls the space between lines if you have created a value that has more than one line (e.g. using \\n or str_wrap()).\n\nThe margin argument controls the space around the text. It is easiest to specify the value of margin using the helper function margin() designed for that purpose. You specify the top, right, bottom and left margin separately in that order – to remember the order, think ‘trouble’. By default, margins are specified in points (the same units that are commonly used to specify font sizes).\n\n\n\nR Console\n\nshootings_map +\n  labs(\n    title = \"Shootings are focused in the South Bronx\",\n    subtitle = \"Fatal and non-fatal shootings recorded by NYC Police, 2019\",\n    caption = str_glue(\n      \"Author: Joe Bloggs, Date produced: {lubridate::today()},\\n\",\n      \"Data: https://data.cityofnewyork.us/d/833y-fsy8\\n\",\n      \"Map data from OpenStreetMap\"\n    )\n  ) +\n  theme_void() +\n  theme(\n    # Make the plot subtitle smaller and adjust the margin around it\n    plot.subtitle = element_text(size = rel(0.8), margin = margin(3, 0, 6, 0)),\n    # Make the legend caption smaller, left-aligned and a lighter shade of grey\n    plot.caption = element_text(colour = \"grey67\", size = rel(0.7), hjust = 0)\n  )\n\n\n\n\n\n\n\n\n\nNow that we have finished setting the text elements for our map, we can save it as a new object that we can use as the basis for the other objects we want to add. Make sure you run this code, since we will need it for the rest of the code below.\n\n\n\nR Console\n\nshootings_map_titled &lt;- shootings_map +\n  labs(\n    title = \"Shootings are focused in the South Bronx\",\n    subtitle = \"Fatal and non-fatal shootings recorded by NYC Police, 2019\",\n    caption = str_glue(\n      \"Author: Joe Bloggs, Date produced: {lubridate::today()},\\n\",\n      \"Data: https://data.cityofnewyork.us/d/833y-fsy8\\n\",\n      \"Map data from OpenStreetMap\"\n    )\n  ) +\n  theme_void() +\n  theme(\n    # Make the plot subtitle smaller and adjust the margin around it\n    plot.subtitle = element_text(size = rel(0.8), margin = margin(3, 0, 6, 0)),\n    # Make the legend caption smaller, left-aligned and a lighter shade of grey\n    plot.caption = element_text(colour = \"grey67\", size = rel(0.7), hjust = 0)\n  )\n\n\n\n\n\n\n\n\nCreate your maps using a single ggplot() stack\n\n\n\nIn this chapter we will store the map we are creating in an object several times as we go through the process of explaining how to add context to a map. When you write your own code, you should not do this. Instead, you should create the map from start to finish in a single block of code. Doing that will make sure your code is easy to read and that you don’t have to keep track of more objects than necessary. For an example of this, see the final section of this chapter.\n\n\n\n\n\n\n\n\nTitles, subtitles and captions\n\n\n\nWhat is the main purpose of adding a title to a map in R?\n\n To make the map look more professional. To provide a reference for the map's source To display additional geographic information To explain the main conclusion that the audience should draw from the map\n\nWhich R function is used to add a title to a map created with ggplot2?\n\n ggtitle() labs() title() ggmap()\n\nWhich of the following is an example of a declarative title for a crime map?\n\n Residential burglary in Philadelphia, 2024 Philadelphia crime map Residential burglary in Philadephia is concentrated in Kensington Where residential burglaries are most concentrated in Philadephia\n\nWhy is it important to acknowledge the source of data used in crime maps?\n\n It meets legal requirements. It helps reduce the size of the data. It improves the accuracy of the map. It ensures the map looks visually appealing.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Giving a map context</span>"
    ]
  },
  {
    "objectID": "07_map_context/index.html#sec-legends",
    "href": "07_map_context/index.html#sec-legends",
    "title": "7  Giving a map context",
    "section": "7.8 Legends",
    "text": "7.8 Legends\nLegends are important for all but the simplest crime maps because they help readers to interpret the points, lines and polygons used to represent data on a particular map. Except for point maps containing only a small number of crimes (such as the map of homicide in downtown Atlanta that we produced in Chapter 2), crime maps will almost always need a legend to help users interpret them.\nProducing a legend manually could be quite complicated, but fortunately ggplot() produces legends automatically. ggplot() will add a legend to a map or chart whenever one or more layers of data are represented using an aesthetic property such as size, shape, colour or fill. In our current map, the density of shootings is represented using the fill colour of the polygons produced by the hotspot_kde() function, with darker colours representing more shootings.\nOur map already has a legend, but we may want to adjust its appearance by:\n\nchanging the default legend title from “kde” to something more meaningful,\nmoving the legend down the visual hierarchy by making it smaller (at the moment it is almost as visually prominent as the data),\nremoving the potentially confusing raw density values.\n\nWe can change the default legend title by once again using the labs() function. Since we want to change the title of the legend, you might reasonably think that we would do this using something like labs(legend = \"density\") but unfortunately that code would do nothing at all. Instead, we have to set the legend title using the aesthetic (colour, size, shape, etc.) that the legend represents. This makes it possible to specify multiple titles if there are separate legends for different layers that use different aesthetics. For example if a map used lines of different colours to show streets of different types and filled polygons to show the density of crime, it would be possible to have separate legends explaining each aesthetic. In this case, we’ve specified that the kde column in the data should control the fill aesthetic, so we can set the title for that legend using fill = \"title we want\".\nRun this code in the R Console and check to see that the legend title has changed.\n\n\n\nR Console\n\nshootings_map_titled +\n  labs(fill = \"kernel density\\nof shootings\")\n\n\n\n\n\n\n\n\n\nTo make the legend smaller, we can use theme() in the same way as we did to change the appearance of the caption. We use the legend.title argument to format the legend title and the legend.text argument to format the labels for each value in the legend, again using the element_text() helper function. In both cases, we will set the text size relative to the default text size using the rel() helper function.\nWe will also make the colour bar in the legend (called the key by ggplot()) slightly smaller using the legend.key.width argument. To do this we will use the helper function unit(), which allows us to specify the size using any of several common units. In this case, we will specify the key size in lines (1 line = the height of one line of text) so that it is relative to the text size we have chosen.\nRun this code and check how the appearance of the legend changes.\n\n\n\nR Console\n\nshootings_map_titled +\n  labs(fill = \"kernel density\\nof shootings\") +\n  theme(\n    # Make the legend colour bar smaller\n    legend.key.width = unit(0.8, \"lines\"),\n    # Make the legend text smaller\n    legend.text = element_text(size = rel(0.7)),\n    legend.title = element_text(size = rel(0.8))\n  )\n\n\n\n\n\n\n\n\n\nFinally, we want to remove the raw density values, since these are difficult to interpret and might distract readers from the key message that darker colours on the map represent higher densities of shootings.\nBy default, ggplot() sets the label for each legend key based on the data. To specify our own labels, we can use the labels argument to the scale_fill_distiller() function that we previously used to set the colour scheme of the density layer on the map.\nWhen we set colour bar labels manually, we have to also specify where on the colour bar we want those labels to appear. We do this using the breaks argument to scale_fill_distiller(), making sure the number of values we supply to the breaks argument is the same as the number of labels we’ve given to the labels argument (otherwise R will produce an error).\nIn this case, we want to add two labels (“higher” and “lower”), one at either end of the colour bar. We could look at kde column of the shootings_kde object to find the minimum and maximum values, but that would introduce the risk of us accidentally entering the wrong values. Instead, we can use the pull() function to extract the kde column from the shootings_kde dataset and then use the range() function to find the minimum and maximum values. Putting this together, we get breaks = range(pull(shootings_kde, \"kde\")). You can read this code starting from the ‘inside’ and working ‘outwards’: the code says “take the shootings_kde object, pull() the ‘kde’ column out of that object and then calculate the range() of numbers in that column.\nRun this code to see how the legend labels change.\n\n\n\nR Console\n\nshootings_map_titled +\n  # Specify colours used to show density of crime\n  scale_fill_distiller(\n    palette = \"PuBu\", \n    direction = 1, \n    # Specify label positions as the minimum and maximum KDE values\n    breaks = range(pull(shootings_kde, \"kde\")), \n    labels = c(\"lower\", \"higher\")\n  ) +\n  labs(fill = \"kernel density\\nof shootings\") +\n  theme(\n    # Make the legend colour bar smaller\n    legend.key.width = unit(0.8, \"lines\"),\n    # Make the legend text smaller\n    legend.text = element_text(size = rel(0.7)),\n    legend.title = element_text(size = rel(0.8))\n  )\n\n\n\n\n\n\n\n\n\nNow that we have finished formatting the legend, we can again store the map as an object that we can build on further.\n\n\n\nR Console\n\nshootings_map_legend &lt;- shootings_map_titled +\n  # Specify colours used to show density of crime\n  scale_fill_distiller(\n    palette = \"PuBu\", \n    direction = 1, \n    # Specify label positions as the minimum and maximum KDE values\n    breaks = range(pull(shootings_kde, \"kde\")), \n    labels = c(\"lower\", \"higher\")\n  ) +\n  labs(fill = \"kernel density\\nof shootings\") +\n  theme(\n    # Make the legend colour bar smaller\n    legend.key.width = unit(0.8, \"lines\"),\n    # Make the legend text smaller\n    legend.text = element_text(size = rel(0.7)),\n    legend.title = element_text(size = rel(0.8))\n  )\n\n\n\n\n\n\n\n\nLegends\n\n\n\nWhat is the role of a legend on a map?\n\n To display the map’s title. To explain the meaning of different symbols or colors on the map. To display the coordinates of map locations. To add gridlines to the map.\n\nWhat is one advantage of using str_glue() to format text on in maps made with R?\n\n It combines multiple strings into one caption, including dynamic values. It helps with data filtering. It allows you to change map colors. It removes unwanted data points.\n\nWhich ggplot2 function is commonly used to add a legend to a map?\n\n geom_sf() theme_void() scale_fill_*() labs()\n\nWhich of the following statements best describes how map legends should be designed?\n\n geom_sf() theme_void() scale_fill_*() labs()",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Giving a map context</span>"
    ]
  },
  {
    "objectID": "07_map_context/index.html#sec-scale-bars",
    "href": "07_map_context/index.html#sec-scale-bars",
    "title": "7  Giving a map context",
    "section": "7.9 Scales and north arrows",
    "text": "7.9 Scales and north arrows\nThe final elements we can add to our map are a scale bar and a north arrow, which can both be added using functions from the ggspatial package. If you check the chapter_07.R code file, you will see that you have already loaded this package.\n\n7.9.1 Scale bars\nTo add a scale bar, we can add a call to the annotation_scale() function to our existing ggplot() object.\n\n\n\nR Console\n\nshootings_map_legend +\n  # Add scale bar\n  annotation_scale()\n\n\n\n\n\n\n\n\n\nThe default scale bar is a little too visually dominant for the low place it should have in the visual hierarchy of our map, and the default placement in the bottom-left corner happens to overlap with the highest density of shootings. We can change the scale bar using arguments to the annotation_scale() function:\n\nwidth_hint = 1/5 changes the (approximate) proportion of the map width across which the scale bar stretches,\nstyle = \"ticks\" changes the style of the scale bar to the less visually prominent line-and-tick-marks style, and\nlocation = \"br\" moves the scale bar to the bottom-right corner of the map.\n\n\n\n\nR Console\n\nshootings_map_legend +\n  # Add scale bar\n  annotation_scale(width_hint = 1/5, style = \"ticks\", location = \"br\")\n\n\n\n\n\n\n\n\n\n\n\n7.9.2 North arrows\nWe can add a north arrow using the annotation_north_arrow() function. The default arrow is too obtrusive to fit its position in the visual hierarchy, so we will change its appearance using the arguments:\n\nlocation = \"tr\" to move the north arrow to the top-right corner, since we have put the scale bar in the bottom-right where the north arrow would be placed by default,\nheight = unit(1.5, \"lines\") to make the arrow smaller, and\nstyle = north_arrow_minimal(text_size = 8) to use a smaller style of arrow, at the same time reducing the font size (measured in points) of the N symbol.\n\n\n\n\nR Console\n\nshootings_map_legend +\n  # Add scale bar\n  annotation_scale(width_hint = 1/5, style = \"ticks\", location = \"br\") +\n  # Add north arrow\n  annotation_north_arrow(\n    location = \"tr\", \n    height = unit(1.5, \"lines\"), \n    style = north_arrow_minimal(text_size = 8)\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNot all maps need north arrows\n\n\n\nIf a map is going to be used for navigation (e.g. a road map or a nautical chart), it is vital that the map includes a north arrow. But if the map is going to be used to understand concentrations of crime in an area, a north arrow is much less important.\nYou may choose to omit including a north arrow in your crime maps. However, you must include a north arrow if (for whatever reason) north is not at a the top of the map. People expect north to be at the top of a map, so if your map does not follow your convention then it is important to make sure your readers can work that out.\nMost of the crime maps in the rest of this book do not have north arrows, because those maps follow the convention of having north at the top of the map.\n\n\n\n\n\n\n\n\nScale bars and north arrows\n\n\n\nWhich of the following is a recommended practice when adding a north arrow or scale bar to a crime map?\n\n They should be placed in the center of the map to ensure the map is balanced. They should be made as large as possible to dominate the map. They should be placed in a corner where they do not interfere with the map’s data. They should be included in every map, regardless of the map’s scale or orientation.\n\nWhat is the purpose of a scale bar in a crime map?\n\n To show the geographical boundaries of the map. To provide a reference for the actual size of features on the map. To indicate the direction of the map. To highlight the most significant data points.\n\nWhen should a north arrow be included on a crime map?\n\n Only when the map has multiple layers. Always, as it is necessary for orientation in every map. Only for maps showing detailed geographical features. When north is not at the top of the map.\n\n\n\nWe now have the complete code needed to make the full map. Copy this code into the chapter_07.R file, replacing the existing ggplot() stack.\n\n\n\nchapter_07.R\n\nshootings_map &lt;- ggplot() +\n  # Add base map\n  annotation_map_tile(type = \"cartolight\", zoomin = 0, progress = \"none\") +\n  # Add density layer\n  geom_sf(aes(fill = kde), data = shootings_kde, alpha = 0.75, colour = NA) +\n  # Add precinct boundaries\n  geom_sf(data = precincts, colour = \"grey33\", fill = NA) +\n  # Add precinct labels\n  geom_sf_label(\n    aes(label = scales::ordinal(precinct)), \n    data = precincts,\n    alpha = 0.5, \n    colour = \"grey33\", \n    size = 2.5, \n    label.size = NA\n  ) +\n  # Add scale bar\n  annotation_scale(width_hint = 1/5, style = \"ticks\", location = \"br\") +\n  # Specify colours used to show density of crime\n  scale_fill_distiller(\n    palette = \"PuBu\",\n    breaks = range(pull(shootings_kde, \"kde\")), \n    labels = c(\"lower\", \"higher\"),\n    direction = 1\n  ) +\n  labs(\n    title = \"Shootings are focused in the South Bronx\",\n    subtitle = \"Fatal and non-fatal shootings recorded by NYC Police, 2019\",\n    caption = str_glue(\n      \"Author: Joe Bloggs, Date produced: {lubridate::today()},\\n\",\n      \"Data: https://data.cityofnewyork.us/d/833y-fsy8\\n\",\n      \"Map data from OpenStreetMap\"\n    ),\n    fill = \"kernel density\\nof shootings\"\n  ) +\n  theme_void() +\n  theme(\n    # Make the legend colour bar smaller\n    legend.key.width = unit(0.8, \"lines\"),\n    # Make the legend text smaller\n    legend.text = element_text(size = rel(0.7)),\n    legend.title = element_text(size = rel(0.8)),\n    # Make the plot subtitle smaller and adjust the margin around it\n    plot.subtitle = element_text(size = rel(0.8), margin = margin(3, 0, 6, 0)),\n    # Make the legend caption smaller, left-aligned and a lighter shade of grey\n    plot.caption = element_text(colour = \"grey67\", size = rel(0.7), hjust = 0)\n  )\n\n\nThis code looks quite complicated, but you can understand each part of it using the comments included in the code.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Giving a map context</span>"
    ]
  },
  {
    "objectID": "07_map_context/index.html#saving-maps",
    "href": "07_map_context/index.html#saving-maps",
    "title": "7  Giving a map context",
    "section": "7.10 Saving maps",
    "text": "7.10 Saving maps\nUntil now, the maps we have produced have appeared in the RStudio Plots panel. But it is often useful to save a map as an image file so that you can share it with others or embed it into a report or presentation. You can save plots created with ggplot() using the ggsave() function.\nggsave() can create image files in many different formats, including PNG, JPEG and PDF. ggsave() will determine which type of file to create according to the file extension of the file name that you specify. So ggsave(\"bronx_shootings_2019.pdf\", plot = shootings_map) produces a PDF file, while ggsave(\"bronx_shootings_2019.jpg\", plot = shootings_map) produces a JPEG image file.\nYou can specify the size of the image that will be saved using the height and width arguments. Note that for historical reasons these values are in inches by default, but you can change this to either centimetres (using units = \"cm\"), millimetres (using units = \"mm\") or pixels (using units = \"px\").\nTo share our map with others, lets save it as an A4-size PDF. Add this code to the chapter_07.R script file.\n\n\n\nchapter_07.R\n\nggsave(\n  \"bronx_shootings_2019.pdf\", \n  plot = shootings_map, \n  width = 210,\n  height = 297,\n  units = \"mm\"\n)\n\n\nSince we are working inside a project in RStudio, the file will by default be saved in the package directory we created in Section 1.4.2. You can view this directory in the RStudio Files panel: open the Files panel (in the bottom-right of the RStudio window) and click the small icon that shows a letter R inside a translucent blue box.\nSave the chapter_07.R file, then restart R to start a new session by clicking on the Session menu and then clicking Restart R. This creates a blank canvas for the next chapter.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Giving a map context</span>"
    ]
  },
  {
    "objectID": "07_map_context/index.html#in-summary",
    "href": "07_map_context/index.html#in-summary",
    "title": "7  Giving a map context",
    "section": "7.11 In summary",
    "text": "7.11 In summary\nIn this chapter we have learned about the importance of understanding the purpose for which people will use a map when making decisions about map design. We have also learned about how establishing a visual hierarchy on our map can help steer readers towards the most-important elements and how to add titles, legends and scale bars to maps in R.\n\nYou can find out more about the topics we have covered in this chapter:\n\nFor a short summary of research into how people read maps and what that tells us about how to design a map, see Cartography, visual perception and cognitive psychology by Amy Griffin.\nFor a more-detailed explanation of how visual hierarchy can be applied to maps, see Visual Hierarchy and Layout.\nFor more examples of how maps can mislead, read How to lie with maps by Alan Smith.\n\n\n\n\n\n\n\n\nRevision questions\n\n\n\nAnswer these questions to check you have understood the main points covered in this chapter. Write between 50 and 100 words to answer each question.\n\nWhat is visual hierarchy in mapping, and why is it important to establish one when designing a map? Provide an example of how visual hierarchy affects map readability\nWhy is it important to add titles, legends, and other supporting elements to a crime map? How do these elements enhance the map’s usefulness and clarity?\nWhat is the difference between a descriptive and a declarative map title? Provide an example of each for a map showing crime patterns.\nWhy is it important to acknowledge data sources and authorship on your map? How can the caption argument in labs() be used to include this information?\nWhat factors should you consider when deciding whether to include a scale bar or north arrow on your map? How do these elements contribute to the map’s context?",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Giving a map context</span>"
    ]
  },
  {
    "objectID": "08_handling_bugs/index.html",
    "href": "08_handling_bugs/index.html",
    "title": "8  Handling bugs in your code",
    "section": "",
    "text": "8.1 Introduction\nIn this chapter we will learn how to deal with bugs in the R code that we write. It’s inevitable that you will make mistakes in the R code that you write. Everyone types the wrong command every now and again, just as everyone sometimes clicks the wrong button in other pieces of software. Learning how to identify and fix bugs is part of the process of learning to code.\nIn this chapter we will look at examples of errors using a dataset of frauds in Kansas City in the United States. We can load this dataset with this code:\nR Console\n\n# Load packages\npacman::p_load(tidyverse)\n\n# Load data\nfrauds &lt;- read_csv(\"https://mpjashby.github.io/crimemappingdata/kansas_city_frauds.csv.gz\")\n\n\nRows: 6 Columns: 6\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (2): offense_code, offense_type\ndbl  (3): uid, longitude, latitude\ndttm (1): date\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nThis is what the first few lines of the dataset look like:\nuid\noffense_code\noffense_type\ndate\nlongitude\nlatitude\n\n\n\n\n9362175\n26B\ncredit card/automated teller machine fraud\n2015-02-14 11:05:00\n-94.5715\n39.1086\n\n\n9362176\n26B\ncredit card/automated teller machine fraud\n2015-02-14 11:05:00\n-94.5715\n39.1086\n\n\n9362201\n26A\nfalse pretenses/swindle/confidence game\n2015-02-14 13:30:00\n-94.6568\n39.2453\n\n\n9362202\n26A\nfalse pretenses/swindle/confidence game\n2015-02-14 13:30:00\n-94.6568\n39.2453\n\n\n9362213\n26C\nimpersonation\n2015-02-14 15:00:00\n-94.5886\n38.9956\n\n\n9362214\n26C\nimpersonation\n2015-02-14 15:00:00\n-94.5886\n38.9956\nIf you were to try to run this code:\nR Console\n\nselect(frauds, offense_category)\nYou can see that R produces an error message saying something like:\nError in `select()`:\n! Can't select columns that don't exist.\n✖ Column `offense_category` doesn't exist.\nIn this case it is fairly easy to identify that one of the columns you have tried to select does not exist. Maybe you mis-remembered the name of the column. To find out what the correct column name is, you can either print the first few rows of the object (by typing the code head(frauds) into the R console) or use the names() function to print a list of column names present in the data:\nR Console\n\nnames(frauds)\n\n\n[1] \"uid\"          \"offense_code\" \"offense_type\" \"date\"         \"longitude\"   \n[6] \"latitude\"\nFrom this, we can see that the column we want to select is called offense_type, not offense_category.\nOther errors will be harder to identify and fix. In this chapter we will go through the process of debugging – identifying, understanding and fixing errors in your code. Sometimes fixing issues with your code can feel like a bit of a roller coaster, but (like most things) it becomes much easier with practice, and if you approach errors in a systematic way.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Handling bugs in your code</span>"
    ]
  },
  {
    "objectID": "08_handling_bugs/index.html#introduction",
    "href": "08_handling_bugs/index.html#introduction",
    "title": "8  Handling bugs in your code",
    "section": "",
    "text": "Why did we run this code in the R Console?\n\n\n\n\n\nNormally when we load packages and data, we do it in an R script file. We do that because this code is needed as part of the complete script that produces a particular map. In this chapter we won’t be creating a map, just looking at examples of errors, so we won’t need to keep any of the code for later.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Handling bugs in your code</span>"
    ]
  },
  {
    "objectID": "08_handling_bugs/index.html#errors-warnings-and-messages",
    "href": "08_handling_bugs/index.html#errors-warnings-and-messages",
    "title": "8  Handling bugs in your code",
    "section": "8.2 Errors, warnings and messages",
    "text": "8.2 Errors, warnings and messages\nWhen something is not quite right, or just when there is an issue in your code that you should be aware of, R has three ways of communicating with you: messages, warnings and errors.\n\n8.2.1 Messages\nMessages are usually for information only and typically don’t require you to take any action. For example, the function get_crime_data() from the crimedata package issues a message to tell you what data it is downloading. It does this because downloading data sometimes takes a few seconds and without a message you might wonder if the code was working.\nBy default, messages are displayed in RStudio in dark red text, although this might vary if you have changed any settings in the Appearance tab of the RStudio Options dialogue. You can generate a message for yourself using the message() function. This is useful if you are writing code and you want to remind yourself of something, or record a particular value. For example, if your code is likely to take a long time to run, you might want to record the time your code started running by generating a message on the first line of your code:\n\n\n\nR Console\n\nmessage(str_glue(\"Code started: {now()}\"))\n\n\nCode started: 2025-02-04 21:00:56.416129\n\n\nWhen R prints a message about your code, any code underneath the code that generated the message will still run. For example, if you run these three lines of code (the second of which prints a message), the line after the message will run as expected:\n\n\n\nR Console\n\n2 * 2\nmessage(\"This is a message. It might be important so make sure you understand it.\")\n2 / 2\n\n\n\n\n[1] 4\n\n\nThis is a message. It might be important so make sure you understand it.\n\n\n[1] 1\n\n\n\n\n8.2.2 Warnings\nWarnings are generated when there is a potential problem with your code, but the problem was not serious enough to stop your code running entirely. For example, ggplot2 functions like geom_point() will send a warning if some of the rows in your data contain missing values (e.g. if some crimes have missing co-ordinates).\nWarnings are important and you should take time to read and understand them, but it is possible that having done so it will be safe to not take any action. Whether it is safe to take no action will often depend on exactly what you are trying to do, which is why it is important that you understand each warning that you see. For example, if you already know that some rows in your data contain missing values and are happy to plot the remaining values, it will be safe to ignore the warning produced by geom_point(). But if your dataset should not have any missing values in it, you will need to investigate why geom_point() is warning you about missing values and whether those values have been accidentally introduced by some part of your code.\n\n\n\n\n\n\nIt is not safe to ignore warnings\n\n\n\nIt is not safe to ignore warnings unless you are sure why they occurred and certain that you don’t need to take any action. If your code produces a warning, it is not safe to ‘leave it until later’ and carry on writing the rest of your code: work out straight away if you need to take any action.\nOne particularly dangerous scenario is where your code produces warnings but still produces what looks like a reasonable result. In these circumstances it can be tempting to ignore the warnings and assume that everything is fine, since the code still produced roughly what you were expecting. However, it’s possible that the plausible answer is nevertheless wrong because of whatever problem is generating the warning in R. Do not assume that warnings are safe to ignore just because they don’t stop your code running.\n\n\nWarnings are displayed in the same font as messages, but with the text Warning: at the start. You can generate your own warning messages using the warning() function:\n\n\n\nR Console\n\nwarning(\"Something might be wrong. Check to make sure.\")\n\n\nWarning: Something might be wrong. Check to make sure.\n\n\nAs with messages, warnings will not stop your code running. This means that if the warning signalled a genuine problem with your code, the results of the lines underneath the warning might not be reliable. That is why it is important to understand warnings when you see them.\n\n\n8.2.3 Errors\nErrors are generated when there is something wrong with your code or the results it produces that means the code cannot continue running any further. An error might occur, for example, because the function read_csv() could not open the requested file (maybe because the file does not exist). In this case, it would make no sense for the rest of the code to run because it probably depends on the data that read_csv() was supposed to load but could not.\nIt would make sense to be able to generate your own errors using the error() function, but this is one of those times when the function to do something in R has a different name from that you might be expecting. In fact, you can generate an error using the stop() function:\n\n\n\nR Console\n\nstop(\"Something is defintely wrong. Don't go any further until you've fixed it.\")\n\n\nError: Something is defintely wrong. Don't go any further until you've fixed it.\n\n\nThere are two types of errors in R: syntax errors and logical errors. Syntax errors happen when R cannot read your code because of a mistake in how it has been typed out. For example, if you forget to put a comma between the arguments of a function, you will get this error:\n\n\n\nR Console\n\nmessage(\"A\" \"B\")\n\n\nError in parse(text = input): &lt;text&gt;:1:13: unexpected string constant\n1: message(\"A\" \"B\"\n                ^\n\n\nThis error is caused by a missing comma between \"A\" and \"B\".\nWhen you run R code, R reads all the code and checks if it can be interpreted as valid R code. If not, R will produce a syntax error. Because all of your code is checked for syntax errors before any code is actually run, a syntax error anywhere in your code will stop all of your code running. Syntax errors are typically fairly easy to fix, because they are usually caused by typos.\nThe second type of error that can happen is a logical error. This happens when R is able to interpret what you have written, but something is wrong with what you have asked it to do. These are called logical errors because there is usually some problem with the logic of what you are asking R to do. Like syntax errors, logical errors can be caused by typos, but logical errors can also have many other causes.\nThere is a saying in programming that a computer will do exactly what you tell it to do, which may not be the same thing as what you wanted it do. Logical errors happen when you have told R to do something that it cannot do. For example, you might be asking R to multiply together a numeric value and a character value (e.g. 3 * \"A\"), which is illogical.\nSince every step in your code depends on the steps that went before it, it is only possible to identify a logical error during the process of running the code. This means that a lot of your code might run successfully before an error occurs.\nLogical errors are typically harder to fix than syntax errors are, because fixing a logical error involves understanding (a) what you have asked R to do and (b) the current state of everything in your code at the moment when the error occurs. Fortunately, there are lots of ways to identify and fix logical errors.\nNow we know what errors, warning and messages are, we need to find out how to deal with them when they happen.\n\n\n\n\n\n\nMessages, warnings and errors\n\n\n\nWhich of the following terms is used in R to describe the way of communicating something for information only?\n\n A message A warning An error A condition\n\nWhich of these statements is true?\n\n It is generally okay not to worry about warnings produced by R code – in most cases you won't need to take any action anyway While you will often not need to take any action in response to a warning produced by your R code, it is still important to understand what caused the warning just in case you need to take action",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Handling bugs in your code</span>"
    ]
  },
  {
    "objectID": "08_handling_bugs/index.html#finding-problems",
    "href": "08_handling_bugs/index.html#finding-problems",
    "title": "8  Handling bugs in your code",
    "section": "8.3 Finding problems",
    "text": "8.3 Finding problems\nIf an error or warning has a simple cause, such as the example of the incorrect column name in the previous section, you can just fix the problem and re-run the code. For problems that are more difficult to handle, you will need to follow a step-by-step process to find and fix them. Think of this as like being a mechanic fixing a car – first you work out what the problem is, then you fix it.\nIf you code only has one line, it will probably be obvious where any problem lies. But most of your code does several things to achieve a particular goal, such as making a map. The first task in dealing with a problem is therefore to work out exactly which function or other piece of code has caused it. For example, take this data-wrangling code to reduce the size of a dataset and sort it in date order:\n\n\n\nR Console\n\nfrauds |&gt; \n  select(offense_type, date, longitude, latitude) |&gt; \n  filter(offence_type == \"impersonation\") |&gt; \n  arrange(date)\n\n\nError in `filter()`:\nℹ In argument: `offence_type == \"impersonation\"`.\nCaused by error:\n! object 'offence_type' not found\n\n\nThis code produces a fairly complicated error message. As is often the case, the most useful part of the error message is the last part:\n! object 'offence_type' not found\nThis suggests the error is on line 3 of the code, since that is the only line containing reference to the object offence_type. To check this, we can comment out that line by placing a # symbol at the start of the line. Do this and re-run the code above – it should now run without a problem.\nNow we know the problem is on the line filter(offence_type == \"impersonation\"), we can look at that line in more detail. Can you spot the problem with that line?\nThe error message in this case has been caused by a typo – the code offence_type == \"impersonation\" uses the British spelling of the word ‘offence’ but in the dataset the variable is spelled using the American English ‘offense’ (you can see the US spelling in the line of code above the line that is causing the error). Fix the typo and re-run the code above in RStudio.\nSometimes it will not be as clear as this where to start in looking for the problem. In particular, some errors can be caused by a problem on one line of code, but only actually have a negative effect on a subsequent line of code. For example, if you run this code you will see an error:\n\n\n\nR Console\n\nfrauds |&gt; \n  select(offense_code, longitude, latitude) |&gt; \n  filter(offense_code == \"26E\") |&gt; \n  arrange(date)\n\n\nError in `arrange()`:\nℹ In argument: `..1 = date`.\nCaused by error:\n! `..1` must be a vector, not a function.\n\n\nThe error message produced suggests the problem is with the arrange() function, but everything is correct with that function since arrange() is a correct function name and we already know that date is a column in the tibble named frauds. So the problem must lie elsewhere. In cases like this, it can be helpful to comment out all the lines of code except the first one and then uncomment one line at a time until you find the one that causes the problem.\nRe-run the code above, but comment out all the lines except the first one by putting # at the start of each line. Remember to remove the |&gt; from the end of the last uncommented line of code, since otherwise you will find the code does not run as expected. Your code should now look like this:\n\n\n\nR Console\n\nfrauds\n  # select(offense_code, longitude, latitude) |&gt; \n  # filter(offense_code == \"26E\") |&gt; \n  # arrange(date)\n\n\nIf you run this code, it will simply print the first few lines of the dataset. Remove the # comment symbol from the second line of the code and run it again, remembering to replace the pipe operator (|&gt;) at the end of line 1 and remove the |&gt; at the end of line 2.\n\n\n\nR Console\n\nfrauds |&gt; \n  select(offense_code, longitude, latitude)\n  # filter(offense_code == \"26E\") |&gt; \n  # arrange(date)\n\n\nAgain, the code does not produce an error – the result is the same as before, except now the dataset has only three columns.\nRepeat the process again by removing the comment symbol from the start of line three, replacing the pipe operators as before.\n\n\n\nR Console\n\nfrauds |&gt; \n  select(offense_code, longitude, latitude) |&gt; \n  filter(offense_code == \"26E\")\n  # arrange(date)\n\n\n# A tibble: 0 × 3\n# ℹ 3 variables: offense_code &lt;chr&gt;, longitude &lt;dbl&gt;, latitude &lt;dbl&gt;\n\n\nThere is still no error message, but you can see the problem: there are no rows left in the data. That’s because there are no rows in the data for which the column offense_code has the value “26E”. If we use the count() function from the dplyr package to list all the unique values of the offense_code column, we’ll see the only values are “26A”, “26B”, and “26C”:\n\n\n\nR Console\n\ncount(frauds, offense_code)\n\n\n# A tibble: 3 × 2\n  offense_code     n\n  &lt;chr&gt;        &lt;int&gt;\n1 26A              2\n2 26B              2\n3 26C              2\n\n\nUncommenting one line at a time until you find an error or output that is not what you expected is a useful way to isolate problems, but it will not always work. In particular, it will not work if the problem is caused by some code that should have been included but is missing from your code entirely. For example, if you try to run the function st_transform() on a tibble without first changing it into an SF object:\n\n\n\nR Console\n\nfrauds |&gt; \n  select(offense_code, longitude, latitude) |&gt; \n  filter(offense_code == \"26A\") |&gt; \n  sf::st_transform(\"EPSG:3603\")\n\n\nError in UseMethod(\"st_transform\"): no applicable method for 'st_transform' applied to an object of class \"c('tbl_df', 'tbl', 'data.frame')\"\n\n\nIn these cases it is particularly useful to check every argument that you have used in a function to track down the error. We will look at this later in this chapter.\n\n8.3.1 Errors caused by data problems\nMany logical errors will be caused by problems with the code you have written, such as when you try to use a function (e.g. st_intersection()) that only works on an SF object but specify that it should use an object of another type. But sometimes logical errors are caused not by your code, but by a mis-match between the structure that your data actually has and the structure you think your data has. We have already seen an example of this in this chapter, in the code that tried to refer to a column called offense_category in a dataset that did not have a column with that name.\nErrors caused by a mismatch between the data you think you have and the data you actually have can be particularly frustrating, because there is no way to identify them from just looking at your code. For this reason, it is often important when trying to identify problems in your code to look at the data that is used as the input for your code, and the data that is produced by each step in your code. We used this technique to find the error in our code above that was caused by filter() removing all the rows from a dataset because we had told filter() to only keep rows containing a value that was not present in the dataset. There were no obvious problems with the code we had written, so the only way to find the cause of this problem was to view the dataset returned by the filter() function.\nFinding data problems is one of the reasons why we have used the head() function so often in previous chapters to look at the data at each step in writing a block of code. Looking at the results of a particular block of code before moving on to writing the next block can often save you from frustrating errors later on.\nhead() only shows us the first few rows of a dataset, which will not always be enough to identify a problem if the problem is caused by values that are only present in a few rows in the data. For small datasets, we can use the View() function (note the capital letter) to open the entire dataset in a new tab in RStudio.\nFor bigger datasets, this will not work. In that case, we can use the sample_n() or sample_frac() functions from the dplyr package to return a random sample of rows from the data. This can be useful to let us look at a representative sample of a large dataset. sample_n() returns a specific number of rows, e.g. sample_n(frauds, 10) returns 10 rows at random from the frauds dataset. sample_frac() returns a specific proportion of the dataset, e.g. sample_frac(frauds, 0.1) returns a sample of 10% of rows from the data.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Handling bugs in your code</span>"
    ]
  },
  {
    "objectID": "08_handling_bugs/index.html#understanding-a-problem",
    "href": "08_handling_bugs/index.html#understanding-a-problem",
    "title": "8  Handling bugs in your code",
    "section": "8.4 Understanding a problem",
    "text": "8.4 Understanding a problem\nSo far, we have tried two ways to deal with errors:\n\nreading a simple error message that makes the problem obvious,\ncommenting out all the code and then uncommenting one line a time until the error appears.\n\nSometimes you will encounter an error that is still not simple to solve. In this case, it is still important to identify the line of code causing the problem, either by working it out from the text of the error message or commenting out lines of code in turn.\nOnce you know what line is causing the problem, you should focus on understanding exactly what that line of code does. In particular:\n\nWhat data do any functions on that line expect to work on?\nWhat are the values of any arguments to the functions on that line?\nWhat value do the functions on that line produce?\n\nYou can get a lot of help in understanding each function by referring to its manual page. You can access the manual page for a function by:\n\ntyping a question mark followed by the function name without parentheses (e.g. ?mutate) into the R console,\ntyping the function name without parentheses into the search box in the Help panel in RStudio, or\nclicking on the function name anywhere in your R code to place the cursor on the function name, then pressing F1 on your keyboard.\n\nAny of these options opens up a manual page for that function in the Help panel in RStudio. For example, this is the manual page for the str_wrap() function from the stringr package. You can load it by typing ?str_wrap in the R console.\n\n\n\nAll manual pages have the same format.\n\nDescription\n\nThis section gives a short description of what the function does. If multiple related functions are described in a single manual page, this section will explain the differences between them. For example, the manual page for the mutate() function from the dplyr package explains the difference between the mutate() function and the closely related transmute() function.\n\nUsage\n\nThis section shows a single example of how the function works. If there are any optional arguments to the function, this section will show what the default values of those optional arguments are. For example, the manual page for the str_wrap() function from the stringr package show that the default value of the width argument is width = 80.\n\nArguments\n\nThis section gives a list of arguments and the values they can take. It is particularly important to note the type of value expected. So the st_transform() function from the sf package expects an SF object as its first argument – if you provide another sort of object (such as a tibble), this will cause an error.\n\nValue\n\nThis section explains the type of value that the function will return, and whether this value might be of a different type depending on the values of particular arguments. For example, the mean() function in base R returns the arithmetic mean of a vector of numbers. However, if any of the numbers is NA then mean will return NA unless the argument na.rm = TRUE is used. In that case, mean() will ignore the missing values and return the mean of the values that are present.\n\nExamples\n\nThis section gives more examples of how the function can be used.\n\n\nChecking the manual page for a function can often help you understand why a particular piece of code is not working. If you have set any optional arguments for a function that is producing an error, it may help to reset those arguments to their default values (as shown in the Usage section of the manual page) one by one to understand what effect this has on your code.\nBy reading the error message, isolating the error by commenting out and then reading the manual page, you will be able to fix almost all the errors you will come across in writing R code. Occasionally, however, you will find an error that you just can’t understand. In that case, you will need to get some help from others.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Handling bugs in your code</span>"
    ]
  },
  {
    "objectID": "08_handling_bugs/index.html#how-to-fix-some-common-errors",
    "href": "08_handling_bugs/index.html#how-to-fix-some-common-errors",
    "title": "8  Handling bugs in your code",
    "section": "8.5 How to fix some common errors",
    "text": "8.5 How to fix some common errors\nThere are some mistakes that it is common for people to make when writing code. As you get more experience in writing R code and dealing with error messages, you are likely to start to recognise some simple errors (especially those caused by typos) and know how to fix them quickly. One useful way to quickly find help on common errors is to check if the error (and the corresponding solution) appears in Appendix C. Use the information in Appendix C to answer these questions.\n\n\n\n\n\n\nFixing common errors\n\n\n\nWhat might cause the error message could not find function \"blah\"?\n\n You have loaded packages in the wrong order You have mis-typed the name of the blah object You have not loaded the package that contains the blah() function You have tried to use a generic function with a type of object the function does not know how to use\n\nWhat might cause the error message non-numeric argument to binary operator?\n\n You have tried to use a function as if it is an object You have tried to use a mathematical operator such as + or - with a non-numeric value such as a character value You have used an argument name in a function that does not understand it You have either mis-typed the function name or the package containing that function is not loaded",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Handling bugs in your code</span>"
    ]
  },
  {
    "objectID": "08_handling_bugs/index.html#sec-reprex",
    "href": "08_handling_bugs/index.html#sec-reprex",
    "title": "8  Handling bugs in your code",
    "section": "8.6 Getting external help using a reproducible example",
    "text": "8.6 Getting external help using a reproducible example\nIf you cannot fix an error using any of the techniques we have already covered, it is probably time to get some help from others. Fortunately, one of the big benefits of using R is that there is a friendly, welcoming community of R coders online who are willing to help fix problems. Almost everyone in this community will remember when they were knew to using R and so will be gentle with people who are asking their first question.\n\nOne of the things that makes it much more likely that you will find help with your problem in the R community is if you phrase your plea for help in a way that makes it easier to help you. We can do this by providing a reproducible example of our problem (also sometimes called a reprex or a minimum working example).\nProducing a reprex makes it much easier for someone to understand your issue. This not only makes it easier for someone to help you, but also shows that you know it will make it easier for them and that you value their time:\n\nImagine that you’ve made a cake, and for some reason it’s turned out absolutely awful – we’re talking completely inedible. Asking a question without a reprex is like asking, “Why didn’t my cake turn out right?” – there are hundreds of possible answers to that question, and it’s going to take a long time to narrow in on the exact cause for your inedible cake creation.\nAsking a question with a reprex is like asking, “My cake didn’t turn out, and here’s the recipe I used and the steps that I followed. Where did I go wrong?” Using this method is going to significantly increase the likelihood of you getting a helpful response, faster!\n\nTo make a reprex, we have to do two things:\n\nRemove everything from our code that does not contribute to causing the error. We do this by removing each line from our code in turn and only keeping those lines that are necessary to produce the specific error you are asking for help with – this is why a reproducible example is sometimes called a minimum working example.\nMake sure that someone trying to help us can reproduce the issue on their own computer even if they don’t have access to the particular dataset we are using. We do this by replacing our own dataset with a publicly available one, preferably one of the datasets that are built into R for exactly this purpose.\n\n\n\nTo practice making a reproducible example, let’s create a new R script that we know will produce an error. Create a new R script file (File &gt; New File &gt; R Script in RStudio), save it as chapter_08.R, then paste the following code into it:\n\n\n\nchapter_08.R\n\n# Load packages\npacman::p_load(tidyverse)\n\n# Load a dataset from your computer and wrangle it\nroad_deaths &lt;- read_csv(\"road_deaths_data.csv\") |&gt; \n  janitor::clean_names() |&gt; \n  rename(ksi_drivers = drivers, ksi_pass_front = front, ksi_pass_rear = rear) |&gt; \n  select(-petrol_price, -van_killed) |&gt; \n  mutate(\n    law = as.logical(law),\n    ksi_driver_rate = ksi_drivers / (kms / 1000)\n  )\n\n# Make a time-series chart of two continuous variables, coloured by a \n# categorical variable, then add a trend line\nroad_deaths + \n  ggplot(aes(x = month_beginning, y = ksi_driver_rate)) + \n  geom_point(aes(colour = law)) + \n  geom_smooth() +\n  scale_x_date(date_breaks = \"2 years\", date_labels = \"%Y\") +\n  scale_y_continuous(labels = scales::comma_format(), limits = c(0, NA)) +\n  scale_colour_brewer(type = \"qual\") +\n  labs(\n    x = NULL, \n    y = \"drivers killed or seriously injured per 1,000km travelled\", \n    colour = \"after seat belts made mandatory\"\n  ) +\n  theme_minimal() +\n  theme(\n    axis.line.x = element_line(colour = \"grey90\"),\n    axis.ticks = element_line(colour = \"grey90\"),\n    panel.grid.major.x = element_blank(),\n    panel.grid.minor.x = element_blank(),\n    legend.position = \"bottom\"\n  )\n\n\nAt the moment, you can’t run this code because you don’t have the road_deaths_data.csv file. Download that file by clicking this link and saving the file into the same directory (folder) as the chapter_08.R file.\n\nDownload the road deaths data file\n\nNow run all the code in the chapter_08.R file. You should see this error:\n\n\nError in `fortify()`:\n! `data` must be a &lt;data.frame&gt;, or an object coercible by `fortify()`,\n  or a valid &lt;data.frame&gt;-like object coercible by `as.data.frame()`, not a\n  &lt;uneval&gt; object.\nℹ Did you accidentally pass `aes()` to the `data` argument?\n\n\nAs you can see, this error is not easy to decipher, so we might need help to deal with it.\n\n8.6.1 Reproducible code\nThe first step in producing a reprex is to remove every line from our code that isn’t necessary to produce the error. To do that, we start with the last line of code and remove it, then re-run the code. If the code produces the same error, we know that the error wasn’t caused or affected by anything on the line that we have removed. In that case, we don’t need to include that line of code in the reprex. On the other hand, if the error message disappears and the code runs successfully, or the error message changes to a different error, we know that the line of code we removed influenced the error in some way. In that case, we need to include that line of code in the reprex.\nFollowing this process line by line, it is actually possible to remove a lot of the original code and still produce the same error. In fact, we only need to keep five lines of the original code:\n\n\n\nchapter_08.R\n\n# Load packages\npacman::p_load(tidyverse) \n\n# Load a dataset from your computer and wrangle it\nroad_deaths &lt;- read_csv(\"road_deaths_data.csv\") |&gt; \n  janitor::clean_names() |&gt;\n  rename(ksi_drivers = drivers, ksi_pass_front = front, ksi_pass_rear = rear) |&gt;\n  select(-petrol_price, -van_killed) |&gt;\n  mutate(\n    law = as.logical(law),\n    ksi_driver_rate = ksi_drivers / (kms / 1000)\n  )\n\n# Make a time-series chart of two continuous variables, coloured by a \n# categorical variable, then add a trend line\nroad_deaths +  \n  ggplot(aes(x = month_beginning, y = ksi_driver_rate)) +  \n  geom_point(aes(colour = law)) +  \n  geom_smooth() +\n  scale_x_date(date_breaks = \"2 years\", date_labels = \"%Y\") +\n  scale_y_continuous(labels = scales::comma_format(), limits = c(0, NA)) +\n  scale_colour_brewer(type = \"qual\") +\n  labs(\n    x = NULL, \n    y = \"drivers killed or seriously injured per 1,000km travelled\", \n    colour = \"after seat belts made mandatory\"\n  ) +\n  theme_minimal() +\n  theme(\n    axis.line.x = element_line(colour = \"grey90\"),\n    axis.ticks = element_line(colour = \"grey90\"),\n    panel.grid.major.x = element_blank(),\n    panel.grid.minor.x = element_blank(),\n    legend.position = \"bottom\"\n  )\n\n\nThat means we can remove:\n\nThe comments (lines 1, 4, 14 and 15, above).\nThe code that wrangles the data in ways that don’t affect the error (lines 6–12).\nThe code that adds a trend line to the chart (line 19).\nThe code that fine-tune the appearance of the chart (lines 20–35).\n\nWe cannot remove the code that loads necessary packages (line 2), loads the data (line 5) or produces the basic unformatted chart (lines 16–18), because if we remove any of those then the error message either changes or disappears.\nThis leaves us with the following code, which produces the same error message but is much easier for someone to check for errors because it is much shorter. Because we have removed the data-wrangling code, we have had to change the name of the argument on line 17 of the code above from y = ksi_driver_rate to y = drivers, since the column ksi_driver_rate is no longer in the data (because we have removed line 7 of the code above). We also have to remove the pipe operator (|&gt;) at the end of line 5 and the + operator at the end of line 18, since they are no longer needed because we have removed the lines of code that follow them.\n\n\n\n\n\n\nIf the error message changes, keep that line of code\n\n\n\nIf we forgot to change y = ksi_driver_rate to y = drivers, or to remove |&gt; on line 7, then the code would still produce an error, but it would be a different error. The purpose of producing a reprex is to find the minimum code that still produces the same error we are interested in. If you remove a line of code and the error message you see changes, put that line of code back.\n\n\n\n\n\nR Console\n\npacman::p_load(tidyverse)\n\nroad_deaths &lt;- read_csv(\"road_deaths_data.csv\")\n\nroad_deaths +\n  ggplot(aes(x = month_beginning, y = ksi_driver_rate)) +\n  geom_point(aes(colour = law))\n\n\nError in `fortify()`:\n! `data` must be a &lt;data.frame&gt;, or an object coercible by `fortify()`,\n  or a valid &lt;data.frame&gt;-like object coercible by `as.data.frame()`, not a\n  &lt;uneval&gt; object.\nℹ Did you accidentally pass `aes()` to the `data` argument?\n\n\nYou can see that even though we have removed 80% of the previous code, this shorter code still produces the same error message. That means the problem must be on line of these 5 lines of code. This makes the problem much easier to find and fix, since we only have to understand what is happening (on what might be going wrong) on 5 lines of code, not in the 35 lines of our original code.\n\n\n\n\n\n\nBut wouldn’t this map look very different?\n\n\n\n\n\nIf this shortened code ran successfully rather than producing an error, the resulting chart would look very different to the original chart we wanted. But that does not matter, because what we are interested in is isolating the lines of code that produce the specific error we want to find and fix.\n\n\n\n\n\n8.6.2 Reproducible data\nOur shortened code would make a great reproducible example except for one thing: the data file road_deaths_data.csv only exists on your computer. This means the example is not actually reproducible, since anyone trying to run this code on their computer to identify the error would find that they instead got a different error saying that the file road_deaths_data.csv was not found.\nYou could deal with this by uploading your dataset to a website and then having the read_csv() function read it from that URL. But you might not want to share your data (perhaps it is sensitive in some way), or your dataset might be too large to post online. For this reason, many R packages come with toy datasets that can be used in learning or in testing for errors. You can see a list of all the toy datasets available in the packages you have loaded by typing data() in the R console. This will produce a file that gives the name and description of each available dataset.\nTo use one of these toy datasets, you just use the the name of the dataset as you would use any other R object (like the road_deaths object we created above). One commonly used toy dataset is the mpg dataset from the ggplot2 package, which contains fuel economy data for 38 models of car.\nThe data in this dataset are on a completely different topic to the data we were trying to use, but this does not matter as long as the data contains variables of the same type (numeric, character, etc.) as the original data. We can see what variables are in the mpg dataset using the head() function as usual.\n\n\n\nR Console\n\nhead(mpg)\n\n\n# A tibble: 6 × 11\n  manufacturer model displ  year   cyl trans      drv     cty   hwy fl    class \n  &lt;chr&gt;        &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt;      &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; \n1 audi         a4      1.8  1999     4 auto(l5)   f        18    29 p     compa…\n2 audi         a4      1.8  1999     4 manual(m5) f        21    29 p     compa…\n3 audi         a4      2    2008     4 manual(m6) f        20    31 p     compa…\n4 audi         a4      2    2008     4 auto(av)   f        21    30 p     compa…\n5 audi         a4      2.8  1999     6 auto(l5)   f        16    26 p     compa…\n6 audi         a4      2.8  1999     6 manual(m5) f        18    26 p     compa…\n\n\nFrom this, we can see that there is:\n\na year variable that we can use as a substitute for the month_beginning variable in our original code,\na variable called hwy that is numeric and so can be substituted for the drivers variable in our code, and\na categorical variable called trans that we can substitute for the law variable in our data.\n\nThis means we can use this data instead of our own data, knowing that anyone on any computer with the ggplot2 package installed can run the code and should get the same result. It also makes our code slightly shorter still, since we no longer need to load the data from a file with read_csv(). All we need to do is load the ggplot2 package and replace the reference to the road_deaths object on line 5 of our shortened code to a reference to the built-in mpg object:\n\n\n\nR Console\n\npacman::p_load(ggplot2)\n\nmpg +\n  ggplot(aes(x = year, y = hwy)) + \n  geom_point(aes(colour = trans))\n\n\nError in `fortify()`:\n! `data` must be a &lt;data.frame&gt;, or an object coercible by `fortify()`,\n  or a valid &lt;data.frame&gt;-like object coercible by `as.data.frame()`, not a\n  &lt;uneval&gt; object.\nℹ Did you accidentally pass `aes()` to the `data` argument?\n\n\nWe have now managed to reduce our original 37 lines of code down to 4 lines, as well as making the example reproducible by using a widely available toy dataset. The shorter code still produces the same error while being much easier to read, so we are much more likely to get help quickly than if we had just sent someone our original code.\nMost of the time, the act of producing a reprex will be enough for us to find and fix the error without any external help. Can you see the problem with our code that is making this error happen? If not, we will reveal it at the end of this chapter.\n\n\n8.6.3 Checking your reprex is reproducible\nNow that you have the minimum code needed to reproduce the error, it’s almost time to share it with people who can help you. But before you do that, it’s worth checking that the code is truly reproducible. To do this we will use the reprex package, which is part of the tidyverse suite of packages you already have installed.\n\n\n\n\nTo use the reprex package, first put your code in a separate R document in the Source panel in RStudio. Open a new R script in RStudio now and paste the final three-line reprex code above into it. Once you’ve done that, select all the code in that document. Now click the Addins button in RStudio and scroll down until you can choose Reprex selection.\nAfter a few seconds, some code should appear in the RStudio Viewer panel showing your code and the error message that it produces. This code has also been copied to your computer clipboard so that you can paste it into an email or web form when you are asking for help.\nIf the error message that you see along with the code in the Viewer panel is not the error message you were expecting, your example is not yet reproducible. For example if you tried to run the Reprex selection command on the original code that we started this section with, we would get an error message 'road_deaths_data.csv' does not exist in current working directory.\nOnce your reprex produces the same error as the code you originally had the issue with, you’re ready to share it to get help.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Handling bugs in your code</span>"
    ]
  },
  {
    "objectID": "08_handling_bugs/index.html#sources-of-help",
    "href": "08_handling_bugs/index.html#sources-of-help",
    "title": "8  Handling bugs in your code",
    "section": "8.7 Sources of help",
    "text": "8.7 Sources of help\nIf you are being taught R by a formal instructor, or you have friends or colleagues who can help, they will probably be the first people that you go to for help.\nIf this doesn’t work, or if you are the most proficient R user that you know, you might need another place to turn to. Fortunately, R has a large community of volunteers who will help you. Before you ask people online for help, it’s important to check that someone hasn’t already asked the same question and had it answered. Duplicate questions increase the workload of the volunteers who answer questions and slow everything down, so if your question has frequently been answered already it’s possible your question will just be ignored.\nTo find out if there is an answer to your question, the easiest thing to do is to search the error message online. Google, or another search engine of your choice, is definitely your friend. If you search online for the error message that was produced by our reprex code, you will see that there are over 100 pages discussing this error message and how to fix it.\nLet’s imagine, though, that there were no relevant hits when we searched for the error message, or that none of the results was useful. In that case, we need to pose a new question to the R community. The place to find the largest slice of that community is probably the website Stack Overflow. This is a website for people who are writing code in any programming language imaginable to get help. It is part of the larger Stack Exchange Network of question-and-answer websites covering everything from travel to veganism.\nTo ask a new question on Stack Overflow, go to stackoverflow.com/questions/ask and create an account or log in. You will now be asked to complete a short form with your question. Questions are more likely to get an answer faster if you:\n\nGive the question a specific title. Over 20 million questions have been asked on Stack Overflow since it launched, so a generic title like ‘Help’, ‘R error’ or even ‘ggplot error’ will not help other people find your question. Look at some recent questions about R on Stack Overflow to get some ideas on what title to give for your question.\nIn the body of your question, briefly (2–3 lines should do it) explain what you were trying to do, then paste the reprex output that the Reprex selection addin copied to your clipboard into the question body box underneath your brief explanation. You will see that Stack Overflow recognises the format of your code and shows you a preview of it underneath the question box.\nFinally, add the tag r to the Tags box so that people know your question is about coding in R. This is crucial – if you do not tag your question as being about R, it is very unlikely that volunteers who know about R will be able to find your question.\n\nSubmit your question and wait for an answer. As soon as someone answers your question, or comments on it to ask for more detail, you will get an email alert. Many questions are typically answered within a few hours. Hopefully this will help you get to the final stage of the emotional roller coaster of debugging:",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Handling bugs in your code</span>"
    ]
  },
  {
    "objectID": "08_handling_bugs/index.html#getting-help-from-online-tutorials-and-blogs",
    "href": "08_handling_bugs/index.html#getting-help-from-online-tutorials-and-blogs",
    "title": "8  Handling bugs in your code",
    "section": "8.8 Getting help from online tutorials and blogs",
    "text": "8.8 Getting help from online tutorials and blogs\nOne of the nice things about learning R is that there is a large online community of people who write about how to get things done using R. The R-Bloggers website hosts thousands of tutorials on different R topics, written by hundreds of different authors, while institutions such as the UCLA Statistical Consulting group provide detailed tutorials on statistical analysis using R. There are also many free online books and websites on how to get different things done in R, many of which are linked as further reading throughout this book. One example of this is the R Graph Gallery that gives examples of many different types of visualisations you can make in R.\nMany online tutorials are very helpful. But the R language has developed over time, so some older tutorials may use methods that have since been replaced by better ones. Occasionally, older tutorials might even you bad advice. So how can you know which tutorials you can trust? Generally, online posts from the past 3 years or so are more likely to use methods that are reasonably up to date. You might also want to consider who wrote the tutorial. For example, is the tutorial hosted on the website of an R package you are using? If so, it’s likely to have been written by the author of the package and so is likely to be reliable.\nAppendix D explains some obsolete R functions that you should avoid using if you see them suggested in online tutorials or blog posts.\n\n\n\n\n\n\nSome R functions to avoid\n\n\n\nWhy should you avoid using the attach() function?\n\n It deletes all the objects in your R environment It makes it impossible for you to share your code with anyone else It makes it hard to keep track of which values are stored in which dataset It has been superseded by the if_else() function\n\nWhat function should you generally use instead of %&gt;%?\n\n pluck() mod() pull() |&gt;\n\nWhat happens if you try to run the code as.numeric(\"100 degrees\")?\n\n The numeric value 100 is returned The value NA is returned The text value \"100\" is returned R crashes",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Handling bugs in your code</span>"
    ]
  },
  {
    "objectID": "08_handling_bugs/index.html#coding-with-ai",
    "href": "08_handling_bugs/index.html#coding-with-ai",
    "title": "8  Handling bugs in your code",
    "section": "8.9 Coding with AI",
    "text": "8.9 Coding with AI\nOne topic that we haven’t covered yet is using artificial intelligence (AI) to help us produce R code. The type of AI that can be used to help produce code is called a large language model (LLM), sometimes known as a generative AI model. The best known of these among the public is probably ChatGPT, but there are lots of similar services available (and new ones being released all the time).\n\n8.9.1 Coding with AI: an example\nGenerative AI services can help you to write code, especially code that does very common tasks like loading data. For example, say that I want to load the Vancouver thefts dataset that we used in a previous chapter and convert the column names in the data to snake_case. As a reminder, the code we previously used to do that looked like this:\npacman::p_load(tidyverse)\n\nvancouver_thefts &lt;- read_csv(\"https://mpjashby.github.io/crimemappingdata/vancouver_thefts.csv.gz\") |&gt; \n  janitor::clean_names()\nTo ask ChatGPT to write some code that will do this task, I need to tell it what I want. For example, you might use this prompt text to give instructions to ChatGPT:\n\nShow me the R code needed to load the CSV dataset at https://mpjashby.github.io/crimemappingdata/vancouver_thefts.csv.gz and make the column names into snake case\n\nWhen I give this prompt to ChatGPT version 4o in January 2025 it produced this response:\n# Install required packages if not already installed\nif (!require(\"janitor\")) install.packages(\"janitor\")\nif (!require(\"readr\")) install.packages(\"readr\")\n\n# Load required libraries\nlibrary(janitor)\nlibrary(readr)\n\n# Define the URL of the dataset\nurl &lt;- \"https://mpjashby.github.io/crimemappingdata/vancouver_thefts.csv.gz\"\n\n# Read the dataset directly from the URL\nthefts_data &lt;- read_csv(url)\n\n# Convert column names to snake_case\nthefts_data &lt;- clean_names(thefts_data)\n\n# View the first few rows of the dataset\nhead(thefts_data)\nChatGPT also produces a short explanation of what the code does:\n\nExplanation:\n\nread_csv(): Reads the compressed .csv.gz file directly from the URL. The readr package handles the compressed file format automatically.\nclean_names(): Converts column names to snake_case. This function is provided by the janitor package and is useful for making column names consistent and easier to work with in R.\nhead(): Displays the first few rows of the dataset to confirm that the column names have been converted successfully.\n\n\nThis code works, in that it produces the desired result. However, it takes 12 lines of code and creates three objects to do the same work as our original 3 lines of code (lines 1–7 of the ChatGPT code replicate what pacman::p_load(tidyverse) does on the first line of our original code). This might not seem like a problem, and for this simple example it’s not. However, if you wanted to combine this code with other code written with the help of a large language model, the code could quickly get so complicated that it would be hard to understand.\n\n\n\n\n\n\nWhy did I get a different result when I put the same prompt into ChatGPT?\n\n\n\n\n\nIf you put the text “Show me the R code needed to load the CSV dataset at https://mpjashby.github.io/crimemappingdata/vancouver_thefts.csv.gz and make the column names into snake case” into ChatGPT, it is possible that you will get a slightly different response to the one shown above. This is because large language models are stochastic or non-deterministic models, which means that there is some element of chance involved in exactly what response the model will produce for a particular input. In fact, if you provide exactly the same input to ChatGPT multiple times, it is likely you will get a slightly different answer each time.\n\n\n\n\n\n8.9.2 Why does ChatGPT sometimes produce code that doesn’t work?\nIn the example above, ChatGPT 4o produced code that works (the previous version, ChatGPT 3.5, generated code that produced an error). However, there are lots of examples of large language models producing code that doesn’t work in various ways.\nTo understand why that happens, we need to know just a little bit about how large language models such as ChatGPT work. LLMs work by looking for patterns in very large datasets (e.g. all the pages on the world wide web) and then using those patterns to predict what the most likely answer is to questions or requests that include particular words and phrases. To give a very simple example, an LLM might be able to identify that if the first few words in a sentence are ‘Today was Sunday, which meant that tomorrow …’ then it is likely (but not certain) that the next words would be ‘is Monday’.\nBecause they are trained on very large samples of data, LLMs can produce answers that are sophisticated enough that they almost look like magic. But behind the scenes, what models like ChatGPT are doing is predicting what the most likely combination of words a response will contain, based on the input that it is given. It is important to remember that models such as ChatGPT do not know the correct answer to a question, since they do not actually understand the question in the way that humans do. This means that ChatGPT cannot give you any indication of how confident it is about the answer it has given: it will be equally confident in its answer whether it is right or wrong. This unjustified confidence can easily lead you to accept an answer from ChatGPT that is actually wrong, so always be careful in using the results produced by LLMs.\n\n\n\n\n\n8.9.3 Should I use AI to help me code?\nSo given these issues, does that mean that we shouldn’t use LLMs to help us write code? Not necessarily. Lots of programmers find LLMs useful for helping them write code. However, LLMs like ChatGPT are useful for supporting humans to write effective code, not for replacing humans writing code. Even if you asked an LLM to write every part of your code for you, it is likely you would need to make some changes to the code it produced to make it work.\nThat means that to use an LLM effectively to help you write code, you need to understand the code that the LLM produces. That’s why this course teaches you to understand how to write code to produce crime maps in R, rather than just how to use an LLM to write the code for you.\nWhether to use an LLM to help you write code is up to you. Some people find it saves them time and helps them get started on how to produce a particular bit of code, while others find that they spend just as much time fixing the code produced by an LLM as it would take them to write the code themselves. You might want to experiment with getting help from an LLM so you can see what works for you.\n\n\n8.9.4 Using AI to help map crime\nThere are two further important points to make about using LLMs in the context of crime mapping. First, it is vital that you do not upload any sensitive data to an LLM that is hosted online, since it is likely that the company that runs the LLM will have reserved the right to use any data you submit to improve the model in future. You must make sure that sensitive data such as victim details are not posted online or shared unlawfully.\nThe second important point is that the analysis that we use crime mapping for is usually done to answer important questions such as where crime-prevention initiatives should be focused or where a serial offender might live. It is vital that the answers we produce to these questions are accurate. Part of making sure the results of our analysis are accurate is to make sure that we understand exactly how our code produces a particular result. If you use an LLM to support your coding, it is vital that you still understand what the code does.\nRestart R to start a new session by clicking on the Session menu and then clicking Restart R. This creates a blank canvas for the next chapter.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Handling bugs in your code</span>"
    ]
  },
  {
    "objectID": "08_handling_bugs/index.html#in-summary",
    "href": "08_handling_bugs/index.html#in-summary",
    "title": "8  Handling bugs in your code",
    "section": "8.10 In summary",
    "text": "8.10 In summary\nIn this chapter we have learned how to handle messages, warnings and errors in R. We have learned to take time to understand error messages, to isolate errors so that we can better understand them, to use manual pages for functions to check that every argument is correct, and how to write reproducible examples so that we can get help online. This will help you become a more independent coder. You can the the checklist in Appendix E to help you deal with any problems in your code.\n\nWhat caused the error in our reproducible example?\nThe error in our reproducible example was very simple, but quite difficult to spot. On line 1 of the code below, we try to add the ggplot() function to the mpg object using the + operator when what we wanted to do was pass the mpg object to the ggplot() function using the |&gt; operator. R does not know how to add a dataset to a function in this way, so it produced an error message.\nIf you replaced + with |&gt; on line 1 of the code below, the code would now run normally. Since we have removed almost all of our original code to make a reproducible example, the resulting plot looks nothing like what we wanted. This does not matter – when we are producing a reprex we only care about reliably producing the same error. Now that we have fixed the error, we could go back and fix the original code to produce the chart we wanted.\n\n\n\nR Console\n\npacman::p_load(ggplot2)\n\nmpg + # &lt;---- THE `+` OPERATOR HERE SHOULD BE A `|&gt;` OPERATOR INSTEAD\n  ggplot(aes(x = year, y = hwy)) + \n  geom_point(aes(colour = trans))\n\n\nThe mistake in this code is a very easy one to make, because what + does inside a ggplot() stack is so similar to what |&gt; does in passing the result of one function to the next function in an data-wrangling pipeline. Remember that we only use + to combine functions inside a ggplot() stack, and use |&gt; to combine functions everywhere else.\n\nFor more information on writing reproducible examples, see:\n\nWatch the webinar Creating reproducible examples with reprex by Jenny Bryan.\nRead the Reprex do’s and don’ts on the reprex package website.\nLearn more about how to use Six tips for better coding with ChatGPT\n\n\n\n\n\n\n\n\nRevision questions\n\n\n\nAnswer these questions to check you have understood the main points covered in this chapter. Write between 50 and 100 words to answer each question.\n\nWhat are the key differences between messages, warnings, and errors in R? Provide an example of each and explain how they affect the execution of code.\nExplain the difference between syntax errors and logical errors in R. Why are logical errors often harder to identify and fix?\nWhy is it important to check each line of code when debugging a block of R code? Describe a method for systematically isolating problematic lines.\nWhat role do R manual pages play in understanding and fixing errors? How can you access these pages, and what key sections should you focus on?\nWhy is it dangerous to ignore warnings in your R code? Provide an example of how a warning could indicate a significant problem with your analysis.\n\n\n\n\nArtwork by @allison_horst",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Handling bugs in your code</span>"
    ]
  },
  {
    "objectID": "09_mapping_areas/index.html",
    "href": "09_mapping_areas/index.html",
    "title": "9  Mapping area data",
    "section": "",
    "text": "9.1 Introduction\nLearn how to make maps that show data for areas, as well as how to produce interactive maps.\nIn previous tutorials we have produced maps based on the locations of individual crimes, sometimes known as point data because we know the point in space at which each crime occurred. But sometimes we will want to instead map counts of crimes for different areas. There are different reasons why we might want to do this:\nIn all these cases we need to make maps of crimes for different areas, rather than showing the locations of individual crimes or the density of crimes generated from datasets of individual crime locations.\nMaps that show data for areas are called thematic maps or choropleth maps.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Mapping area data</span>"
    ]
  },
  {
    "objectID": "09_mapping_areas/index.html#introduction",
    "href": "09_mapping_areas/index.html#introduction",
    "title": "9  Mapping area data",
    "section": "",
    "text": "We might want to compare the number of crimes in different areas, such as police districts. We might want to do this to decide which district to allocate extra funds to.\nWe might want to estimate the relative risk of a crime occurring in different places by calculating crime rates using counts of crimes and population data for local areas.\nWe might only have access to counts of crimes for different areas, rather than the location of each crime, perhaps in order to protect the privacy of crime victims.\n\n\n\n\n\nDo not aggregate point-level crime data to counts of crimes for areas unless you have a good reason to do so.\nChoropleth maps have several shortcomings that we will look at in this tutorial. If you have data on the locations of individual crimes then you should typically present those either as individual points (if there are only a few crimes) or using a kernel density map. If you have point-level crime data, you should only use a choropleth map if you need to compare administrative areas or because you want to calculate crime risk and you only have population data for areas.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Mapping area data</span>"
    ]
  },
  {
    "objectID": "09_mapping_areas/index.html#counting-crimes",
    "href": "09_mapping_areas/index.html#counting-crimes",
    "title": "9  Mapping area data",
    "section": "9.2 Counting crimes",
    "text": "9.2 Counting crimes\nSometimes we will want to know how many crimes have occurred in different formal areas within a city. For example, we might want to know how many crimes of a particular type had occurred in each neighbourhood in a city as part of a review of the performance of different police teams, or to decide which neighbourhoods should be given more crime-prevention funding.\nTo calculate counts of crimes for areas, we need\n\na dataset representing the crime locations and\na dataset representing the boundaries of the areas that we want to calculate counts for.\n\nIn this tutorial, we will count how many car-jacking offences occurred in each alcaldía (borough) of Mexico City in 2019. The result of this will be an SF object containing the outlines of each borough boundary and the count of car-jacking offences in each borough:\n\nborough_counts |&gt; \n  head() |&gt; \n  knitr::kable()\n\n\n\n\n\n\n\n\n\n\nmunicip\nnomgeo\ngeom\noffences\n\n\n\n\n9\nMilpa Alta\nPOLYGON ((-98.99723 19.2273…\n13\n\n\n14\nBenito Juárez\nPOLYGON ((-99.13679 19.4015…\n140\n\n\n5\nGustavo A. Madero\nPOLYGON ((-99.1186 19.58446…\n545\n\n\n3\nCoyoacán\nPOLYGON ((-99.13923 19.3568…\n152\n\n\n16\nMiguel Hidalgo\nPOLYGON ((-99.19058 19.4679…\n152\n\n\n8\nLa Magdalena Contreras\nPOLYGON ((-99.20859 19.3361…\n21\n\n\n\n\n\nTo get started, watch this video walk-through of the code we need to write to produce this dataset:\n\n\n9.2.1 Load the data\nFirst, we load the car-jacking data from the URL https://mpjashby.github.io/crimemappingdata/cdmx_car_jacking.gpkg – write the R code needed to load this file into an R object called cdmx_car_jacking. Remember to load any packages that you need first.\n\nlibrary(sf)\nlibrary(tidyverse)\n\ncdmx_car_jacking &lt;- read_sf(\"https://mpjashby.github.io/crimemappingdata/cdmx_car_jacking.gpkg\") \n\nhead(cdmx_car_jacking)\n\nSimple feature collection with 6 features and 3 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: -99.22413 ymin: 19.29605 xmax: -99.04177 ymax: 19.45565\nGeodetic CRS:  WGS 84\n# A tibble: 6 × 4\n  fecha_hechos        longitud latitud                 geom\n  &lt;dttm&gt;                 &lt;dbl&gt;   &lt;dbl&gt;          &lt;POINT [°]&gt;\n1 2019-05-17 17:00:00    -99.1    19.4  (-99.1088 19.39638)\n2 2019-05-17 23:50:00    -99.2    19.3 (-99.22413 19.29605)\n3 2019-05-09 22:00:00    -99.2    19.4 (-99.19087 19.40981)\n4 2019-05-17 17:00:00    -99.1    19.5 (-99.14227 19.45565)\n5 2019-05-17 21:30:00    -99.0    19.4 (-99.04177 19.38836)\n6 2019-05-18 09:00:00    -99.1    19.3 (-99.10008 19.32609)\n\n\nWe also need to load the borough boundaries as an SF object from the URL https://mpjashby.github.io/crimemappingdata/cdmx_alcaldias.gpkg and store it in an object called cdmx_alcaldias. Write the R code needed to do this. Use the notes you have from previous tutorials to help you if you need them, or go back to the tutorial called ‘Giving a map context’ to refresh your memory.\n\ncdmx_alcaldias &lt;- read_sf(\"https://mpjashby.github.io/crimemappingdata/cdmx_alcaldias.gpkg\")\n\n\n\n9.2.2 Count crimes in each borough\nNow that we have loaded our data, we can count the number of offences in each borough and produce an R object that contains the boundaries of the boroughs together with the number of offences in each one. To do this, we will:\n\nIdentify which borough each crime is in using the st_join() function from the sf package.\nCount the number of crimes in each borough using the count() function from the dplyr package.\nJoin the counts of crimes to the borough boundaries using the left_join() function from the dplyr package.\n\nFor this to work, the object containing the polygons must include a column that can act as a unique identifier for each row (i.e. a column where no value appears more than once). In the cdmx_alcaldias object, there are two columns where all the values are unique: municip and namgeo. If there wasn’t a column of unique values, we could create one using mutate() together with the row_number() function (both from the dplyr package):\ncdmx_alcaldias &lt;- mutate(cdmx_alcaldias, unique_id = row_number())\nSince we already have a column of unique values, we can move straight to using the st_join() function. st_join() comes from the sf package and needs two SF objects, one containing points and one containing polygons representing areas. st_join() takes all the columns from the polygons object and adds those values to the points based on which points are inside each polygon. When we use st_join() to join the cdmx_car_jacking object (containing points) and the cdmx_alcaldias object (containing polygons), st_join() takes cdmx_car_jacking, works out which polygon each point falls inside, then adds columns to the data associated with each point based on that polygon.\n\ncar_jackings_in_alcaldias &lt;- st_join(cdmx_car_jacking, cdmx_alcaldias)\n\nhead(car_jackings_in_alcaldias)\n\nSimple feature collection with 6 features and 5 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: -99.22413 ymin: 19.29605 xmax: -99.04177 ymax: 19.45565\nGeodetic CRS:  WGS 84\n# A tibble: 6 × 6\n  fecha_hechos        longitud latitud                 geom municip nomgeo      \n  &lt;dttm&gt;                 &lt;dbl&gt;   &lt;dbl&gt;          &lt;POINT [°]&gt;   &lt;int&gt; &lt;chr&gt;       \n1 2019-05-17 17:00:00    -99.1    19.4  (-99.1088 19.39638)       6 Iztacalco   \n2 2019-05-17 23:50:00    -99.2    19.3 (-99.22413 19.29605)      12 Tlalpan     \n3 2019-05-09 22:00:00    -99.2    19.4 (-99.19087 19.40981)      16 Miguel Hida…\n4 2019-05-17 17:00:00    -99.1    19.5 (-99.14227 19.45565)      15 Cuauhtémoc  \n5 2019-05-17 21:30:00    -99.0    19.4 (-99.04177 19.38836)       7 Iztapalapa  \n6 2019-05-18 09:00:00    -99.1    19.3 (-99.10008 19.32609)       7 Iztapalapa  \n\n\nAs well as producing a joined dataset as expected, this code has produced a message noting that st_join() assumes that co-ordinates are planar even though they are specified as latitudes and longitudes. This message is there to remind us that st_join() uses certain assumptions about the shape of the earth that might make identifying which borough each offence occurred in incorrect in some circumstances. This is largely a problem for maps covering very-large areas or places near the North or South poles. To be on the safe side, we can first transform both the spatial layers into a local co-ordinate system that is appropriate for Mexico City.\nTo identify a suitable co-ordinate system for this data, we can use the suggest_crs() function from the crsuggest package. suggest_crs() takes an SF object that has co-ordinates specified using longitudes and latitudes, and suggests local co-ordinate systems that are suitable for that part of the world.\n\ncrsuggest::suggest_crs(cdmx_car_jacking)\n\n# A tibble: 10 × 6\n   crs_code crs_name                        crs_type crs_gcs crs_units crs_proj4\n   &lt;chr&gt;    &lt;chr&gt;                           &lt;chr&gt;      &lt;dbl&gt; &lt;chr&gt;     &lt;chr&gt;    \n 1 31975    SIRGAS 2000 / UTM zone 21N      project…    4674 m         +proj=ut…\n 2 8036     WGS 84 / TM Zone 21N (ftUS)     project…    4326 us-ft     +proj=tm…\n 3 2203     REGVEN / UTM zone 20N           project…    4189 m         +proj=ut…\n 4 31989    SIRGAS 1995 / UTM zone 20N      project…    4170 m         +proj=ut…\n 5 31974    SIRGAS 2000 / UTM zone 20N      project…    4674 m         +proj=ut…\n 6 8035     WGS 84 / TM Zone 20N (ftUS)     project…    4326 us-ft     +proj=tm…\n 7 2003     Grenada 1953 / British West In… project…    4603 m         +proj=tm…\n 8 31988    SIRGAS 1995 / UTM zone 19N      project…    4170 m         +proj=ut…\n 9 31973    SIRGAS 2000 / UTM zone 19N      project…    4674 m         +proj=ut…\n10 2202     REGVEN / UTM zone 19N           project…    4189 m         +proj=ut…\n\n\nWe can see that sugggest_crs() has suggested 10 co-ordinate systems that are suitable for the part of the world covered by the cdmx_car_jacking dataset. If you look at the crs_units column, you will see that some of those co-ordinate systems use metres (m) and some use feet and inches (us-ft). It is probably easiest to use co-ordinate systems specified in metres. In this case, the first row has a co-ordinate system that uses metres, so we will use that system. You can see that the crs_code (31975) column gives us an EPSG code we can use together with st_transform() (i.e. in the form “EPSG:31975”).\nWrite code that uses the st_transform() function to transform both layers, st_join() to join them and head() to display the first few rows of the result. The result of st_join() should be stored in an object called car_jackings_in_alcaldias.\n\ncdmx_car_jacking &lt;- st_transform(cdmx_car_jacking, \"EPSG:31975\")\ncdmx_alcaldias &lt;- st_transform(cdmx_alcaldias, \"EPSG:31975\")\n\ncar_jackings_in_alcaldias &lt;- st_join(cdmx_car_jacking, cdmx_alcaldias)\n\nhead(car_jackings_in_alcaldias)\n\nSimple feature collection with 6 features and 5 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: -4276576 ymin: 2803532 xmax: -4249036 ymax: 2822940\nProjected CRS: SIRGAS 2000 / UTM zone 21N\n# A tibble: 6 × 6\n  fecha_hechos        longitud latitud               geom municip nomgeo        \n  &lt;dttm&gt;                 &lt;dbl&gt;   &lt;dbl&gt;        &lt;POINT [m]&gt;   &lt;int&gt; &lt;chr&gt;         \n1 2019-05-17 17:00:00    -99.1    19.4 (-4257417 2813506)       6 Iztacalco     \n2 2019-05-17 23:50:00    -99.2    19.3 (-4276576 2804269)      12 Tlalpan       \n3 2019-05-09 22:00:00    -99.2    19.4 (-4267535 2818569)      16 Miguel Hidalgo\n4 2019-05-17 17:00:00    -99.1    19.5 (-4259315 2822940)      15 Cuauhtémoc    \n5 2019-05-17 21:30:00    -99.0    19.4 (-4249036 2809786)       7 Iztapalapa    \n6 2019-05-18 09:00:00    -99.1    19.3 (-4259181 2803532)       7 Iztapalapa    \n\n\nWe can see that this time, no messages are produced because the inputs to st_join() do not use longitude and latitude co-ordinates.\nIf you look at the car_jackings_in_alcaldias object, you will see that there are columns called municip and nomgeo that show which row in the cdmx_alcaldias object covers the location of each offence from the cdmx_car_jacking object. We can use either of these columns to count the number of car jackings in each borough using the count() function from the dplyr package. Running count() on SF objects can sometimes be slow, so before we do this we will convert car_jackings_in_alcaldias to a tibble using the st_drop_geometry() function.\n\ncar_jacking_counts &lt;- car_jackings_in_alcaldias |&gt; \n  st_drop_geometry() |&gt; \n  # Call the column that holds the murder counts `offences`\n  count(nomgeo, name = \"offences\")\n\nhead(car_jacking_counts)\n\n# A tibble: 6 × 2\n  nomgeo                offences\n  &lt;chr&gt;                    &lt;int&gt;\n1 Azcapotzalco               210\n2 Benito Juárez              140\n3 Coyoacán                   152\n4 Cuajimalpa De Morelos       10\n5 Cuauhtémoc                  88\n6 Gustavo A. Madero          545\n\n\nThe object car_jacking_counts has two columns: one showing the name of each borough and one showing the number of car jackings occurring there. Note that this object contains only those boroughs in which at least one car jacking occurred, since we used count() to count car jackings, not boroughs.\nTo create a dataset containing the boundary of each borough and the relevant offence count, we will join car_jacking_counts object to the original cdmx_alcaldias object using the left_join() function.\nleft_join() is one of a family of joining functions in the dplyr package that join two objects together based on the values of particular columns. If we had two datasets, x and y:\n\n\n\nwe could use left_join(x, y) to merge them into one combined dataset. This function has left in the name because it joins the two datasets by adding matching rows from the right-hand dataset (y) to each row in the left-hand dataset (x). This means the combined dataset will include all the rows from x, but only the rows from y that match rows in x. Rows in y that do not match any rows in x will be discarded.\n\n\n\nIn our case, we want to join the cdmx_alcaldias and car_jacking_counts objects so that the combined dataset includes exactly one row for each borough in cdmx_alcaldias, even if no car jackings occurred in that borough (which will mean that borough is not present in the car_jacking_counts dataset). Since we want to keep all the rows from cdmx_alcaldias, we use that object as the first argument to left_join(), i.e.  left_join(cdmx_alcaldias, car_jacking_counts). If there are any rows in the right-hand object car_jacking_counts that do not match any rows in the left-hand object cdmx_alcaldias, those rows will be discarded from the result.\nBy default, left_join() will match the rows in the cdmx_alcaldias and car_jacking_counts objects using all the column names that are present in both datasets. This can sometimes have unexpected results, so it is safer to specify which columns we want to match to be based on using the by argument to left_join(), in this case to specify by = \"nomgeo\" since that is the name of the column that represents the borough names in both cdmx_alcaldias and car_jacking_counts.\n\nSince any boroughs with no car jackings will not be present in the car_jacking_counts dataset, those boroughs will have missing values (i.e.  NA) in place of offence counts in the result produced by left_join(). For that reason, when you use left_join() it’s a good idea to clean up the resulting object using the replace_na() function from the tidyr package (another part of the tidyverse).\nreplace_na() needs one argument, which must be a list created with the list() function. The list() function should have one argument for each column in the data for which we want to replace missing values with the values that we specify. In this case, we want to replace any missing values in the offences column (which contains the counts of car jackings) with the number zero, since any missing values produced by left_join() are the result of those boroughs not having any car jackings and so not being included in the car_jacking_counts dataset.\nTying all this together, we can create a new SF object called borough_counts that contains both the outlines and the count of car jackings in each borough.\n\nborough_counts &lt;- cdmx_alcaldias |&gt; \n  left_join(car_jacking_counts, by = \"nomgeo\") |&gt; \n  # Replace missing (NA) values in the `offences` column with the value 0\n  replace_na(list(offences = 0))\n\nhead(borough_counts)\n\nSimple feature collection with 6 features and 3 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: -4292001 ymin: 2763846 xmax: -4246392 ymax: 2840978\nProjected CRS: SIRGAS 2000 / UTM zone 21N\n# A tibble: 6 × 4\n  municip nomgeo                                                   geom offences\n    &lt;int&gt; &lt;chr&gt;                                           &lt;POLYGON [m]&gt;    &lt;int&gt;\n1       9 Milpa Alta             ((-4249843 2785992, -4249876 2785983,…       13\n2      14 Benito Juárez          ((-4260843 2815313, -4260855 2815285,…      140\n3       5 Gustavo A. Madero      ((-4250891 2839641, -4250798 2839162,…      545\n4       3 Coyoacán               ((-4263007 2809283, -4262814 2809210,…      152\n5      16 Miguel Hidalgo         ((-4265084 2826525, -4265177 2826255,…      152\n6       8 La Magdalena Contreras ((-4272894 2809155, -4272910 2809142,…       21\n\n\nWe now have an SF object containing the boundary of each borough in Mexico City and the number of car jackings that occurred there. We could use this object to make a choropleth map of car jackings, but before we do this we need to understand some of the limitations of this sort of map.\nWe can now put together all the code needed to take the locations of car jackings in Mexico City and produce counts of offences at borough level. When we run all this code together, we can make better use of the pipe operator to create fewer unnecessary intermediate objects as we go.\n\nlibrary(sf)\nlibrary(tidyverse)\n\n# Load the data and transform to an appropriate co-ordinate system\ncdmx_car_jacking &lt;- read_sf(\"https://mpjashby.github.io/crimemappingdata/cdmx_car_jacking.gpkg\")\ncdmx_alcaldias &lt;- read_sf(\"https://mpjashby.github.io/crimemappingdata/cdmx_alcaldias.gpkg\")\n\n# Calculate counts of car jackings\ncar_jacking_counts &lt;- cdmx_car_jacking |&gt; \n  # Join the borough names to the offence locations\n  st_join(cdmx_alcaldias) |&gt; \n  # Convert the SF object to a tibble so that `count()` runs faster\n  st_drop_geometry()  |&gt;  \n  # Call the column that holds the murder counts `offences`\n  count(nomgeo, name = \"offences\")\n\n# Join offence counts back to borough boundaries\nborough_counts &lt;- cdmx_alcaldias |&gt; \n  left_join(car_jacking_counts, by = \"nomgeo\") |&gt; \n  # Replace missing (NA) values in the `offences` column with the value 0\n  replace_na(list(offences = 0))\n\nhead(borough_counts)\n\nSimple feature collection with 6 features and 3 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: -99.32331 ymin: 19.04824 xmax: -98.95239 ymax: 19.59276\nGeodetic CRS:  WGS 84\n# A tibble: 6 × 4\n  municip nomgeo                                                   geom offences\n    &lt;int&gt; &lt;chr&gt;                                           &lt;POLYGON [°]&gt;    &lt;int&gt;\n1       9 Milpa Alta             ((-98.99723 19.22731, -98.99745 19.22…       13\n2      14 Benito Juárez          ((-99.13679 19.40156, -99.13682 19.40…      140\n3       5 Gustavo A. Madero      ((-99.1186 19.58446, -99.11691 19.581…      545\n4       3 Coyoacán               ((-99.13923 19.35687, -99.13772 19.35…      152\n5      16 Miguel Hidalgo         ((-99.19058 19.46794, -99.19066 19.46…      152\n6       8 La Magdalena Contreras ((-99.20859 19.3361, -99.20867 19.335…       21\n\n\n\nSometimes the difference between left_join() and st_join() can be confusing. Remember:\n\nst_join() is used to join datasets based on matching spatial locations.\nleft_join() is used to join datasets based on matching values in one or more columns in the data.\n\n\n\nTidy Animated Verbs licensed under the Creative Commons Zero licence.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Mapping area data</span>"
    ]
  },
  {
    "objectID": "09_mapping_areas/index.html#mapping-areas",
    "href": "09_mapping_areas/index.html#mapping-areas",
    "title": "9  Mapping area data",
    "section": "9.3 Mapping areas",
    "text": "9.3 Mapping areas\nMaps showing counts of crimes, or a wide range of other data, for different areas are extremely common in all types of spatial analysis. Watch this video to find out introduce yourself to some of the issues you should be aware of when mapping areas.\n\nBecause so much of the data that we might use in crime mapping is only available for specific areas, we often have no choice as to what types of area we use to produce our maps. But whether we choose the areas or not, we have to be mindful that data for areas can only tell us about the area as a whole, not individual people or places within that area. Assuming information about areas applies to individual people or places within them is known as the ecological fallacy, a type of logical error that can be dangerous when we try to draw conclusions from choropleth maps. Watch this video to find out more about the ecological fallacy.\n\nThe ecological fallacy is not a problem that we can solve – it is inherent to choropleth maps. For this reason, whenever we plan to show some spatial data using a choropleth map, it is worth stopping and thinking about whether another type of map (such as a density map) might show the data more effectively and without the problems that choropleth maps have.\nNevertheless, in some circumstances a choropleth map will be the best choice either because we only have data for pre-defined areas or because we are specifically interested in comparing areas (e.g. in working out if there is more crime in one neighbourhood or police district than another).",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Mapping area data</span>"
    ]
  },
  {
    "objectID": "09_mapping_areas/index.html#making-a-choropleth-map",
    "href": "09_mapping_areas/index.html#making-a-choropleth-map",
    "title": "9  Mapping area data",
    "section": "9.4 Making a choropleth map",
    "text": "9.4 Making a choropleth map\nIn this section we will make a choropleth map of murders in different districts within the state of Uttar Pradesh in northern India in 2014. Uttar Pradesh is the largest state in India, with more than 200 million residents.\n\ndistrict_murders_map &lt;- ggplot(district_murders_pop) +\n  annotation_map_tile(type = \"cartolight\", zoomin = 0, progress = \"none\") +\n  geom_sf(aes(fill = murder), data = district_murders, alpha = 0.75) +\n  scale_fill_distiller(palette = \"Oranges\", direction = 1) +\n  labs(\n    title = \"Murders in districts in Uttar Pradesh, 2014\",\n    fill = \"number of\\nmurders\"\n  ) +\n  theme_void() +\n  theme(panel.border = element_rect(colour = \"black\", fill = NA))\n\nggsave(\n  here::here(\"inst/tutorials/09_mapping_areas/images/uttar_pradesh_murders.jpg\"), \n  district_murders_map,\n  width = 800 / 150, \n  height = 500 / 150,\n  dpi = 150\n)\n\n\n\n\n\n9.4.1 Loading and joining datasets\nFirst, we load the counts of murders in each district and the district boundaries as separate objects.\n\nlibrary(sf)\nlibrary(tidyverse)\n\nmurders &lt;- read_csv(\"https://mpjashby.github.io/crimemappingdata/uttar_pradesh_murders.csv\")\n\nRows: 75 Columns: 2\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): district\ndbl (1): murder\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ndistricts &lt;- read_sf(\"https://mpjashby.github.io/crimemappingdata/uttar_pradesh_districts.gpkg\")\n\nmessage(\"First few rows of the `murders` object …\")\n\nFirst few rows of the `murders` object …\n\nhead(murders, n = 3)\n\n# A tibble: 3 × 2\n  district  murder\n  &lt;chr&gt;      &lt;dbl&gt;\n1 Agra         178\n2 Aligarh      179\n3 Allahabad    132\n\nmessage(\"First few rows of the `districts` object …\")\n\nFirst few rows of the `districts` object …\n\nhead(districts, n = 3)\n\nSimple feature collection with 3 features and 2 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 77.42308 ymin: 26.7417 xmax: 79.79299 ymax: 28.90105\nGeodetic CRS:  WGS 84\n# A tibble: 3 × 3\n  state         district_name                                               geom\n  &lt;chr&gt;         &lt;chr&gt;                                              &lt;POLYGON [°]&gt;\n1 Uttar Pradesh Agra          ((77.6444 27.23444, 77.65405 27.23463, 77.65336 2…\n2 Uttar Pradesh Bareilly      ((78.97474 28.41527, 78.97981 28.41454, 78.98546 …\n3 Uttar Pradesh Etah          ((79.20766 27.56734, 79.20418 27.56314, 79.19801 …\n\n\nNote that we have used the message() function here to make it easier to decipher the output of this code, since we are printing both the murders and districts objects. We have also used the n = 3 argument to the head() function to print just the first three rows of each dataset.\nWe will need to join these two objects using left_join() to create a choropleth map. From the output above we can see that the murders object contains the count of murders in each district and the name of the district in the district column. The districts object also contains the name of each district, but in a column called district_name. To join the two datasets based on this column, we will have to specify that we want to use those differently named columns using the left_join() argument by = c(\"district_name\" = \"district\"). As in the Mexico City example earlier in this tutorial, we will use left_join() and specify the districts object as the first argument in order to keep all the districts in the resulting object, whether there were any murders in the district or not.\n\ndistrict_murders &lt;- left_join(\n  districts, \n  murders, \n  # We can join two datasets based on columns with different names by specifying\n  # `by = c(\"column_name_in_lefthand_data\" = \"column_name_in_righthand_data\")`\n  by = c(\"district_name\" = \"district\")\n)\n\nhead(district_murders)\n\nSimple feature collection with 6 features and 3 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 77.42308 ymin: 24.80419 xmax: 82.35455 ymax: 28.90105\nGeodetic CRS:  WGS 84\n# A tibble: 6 × 4\n  state         district_name                                        geom murder\n  &lt;chr&gt;         &lt;chr&gt;                                       &lt;POLYGON [°]&gt;  &lt;dbl&gt;\n1 Uttar Pradesh Agra          ((77.6444 27.23444, 77.65405 27.23463, 77.…    178\n2 Uttar Pradesh Bareilly      ((78.97474 28.41527, 78.97981 28.41454, 78…    132\n3 Uttar Pradesh Etah          ((79.20766 27.56734, 79.20418 27.56314, 79…     65\n4 Uttar Pradesh Shahjahanpur  ((80.30972 28.46321, 80.29755 28.46668, 80…     88\n5 Uttar Pradesh Pilibhit      ((79.67865 28.84923, 79.67203 28.83937, 79…     54\n6 Uttar Pradesh Allahabad     ((81.54684 25.1848, 81.55587 25.1857, 81.5…    132\n\n\nLooking at the district_murders object, we can see that each row now contains the name of a district and the number of murders that occurred there. There are no districts in which there were zero murders, so we do not need to replace any NA values with zeros using replace_na() as in the previous example.\n\n\n\n9.4.2 Making a choropleth map\nThe district_murders object we have produced is an SF object, which means we can use geom_sf() to plot the districts.\n\nlibrary(ggspatial)\n\nggplot(district_murders) +\n  annotation_map_tile(type = \"cartolight\", zoomin = 0, progress = \"none\") +\n  geom_sf() +\n  theme_void() +\n  theme(panel.border = element_rect(colour = \"black\", fill = NA))\n\n\n\n\n\n\n\n\nThere are at least two problems with this map:\n\nwe cannot tell how many murders there were in each district, and\nwe cannot see the base map under the polygons representing the districts.\n\nWe can solve these two problems by changing the aesthetics for the geom_sf() layer in our code. Specifically, we can make the district polygons semi-transparent by setting the alpha argument to geom_sf() to a value of less than one, while also using aes(fill = murder) to specify that the fill colour of the polygons should depend on the number of murders in each district. At the same time we will use scale_fill_distiller() to use a colour scheme that is accessible to the widest range of users.\n\nggplot(district_murders) +\n  annotation_map_tile(type = \"cartolight\", zoomin = 0, progress = \"none\") +\n  # Add the districts layer with colour controlled by the number of murders\n  geom_sf(aes(fill = murder), alpha = 0.75) +\n  # Use a better colour scheme\n  scale_fill_distiller(palette = \"Oranges\", direction = 1) +\n  # Add some labels\n  labs(\n    title = \"Murders in districts in Uttar Pradesh, 2014\",\n    fill = \"number of\\nmurders\"\n  ) +\n  theme_void() +\n  theme(panel.border = element_rect(colour = \"black\", fill = NA))",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Mapping area data</span>"
    ]
  },
  {
    "objectID": "09_mapping_areas/index.html#making-an-interactive-map",
    "href": "09_mapping_areas/index.html#making-an-interactive-map",
    "title": "9  Mapping area data",
    "section": "9.5 Making an interactive map",
    "text": "9.5 Making an interactive map\n\nDue to a bug on some versions of Windows, the R code chunks in this section are not interactive. If you would like to run the R code here, please copy and paste it into the R console or a blank R script (.R) file.\n\nSo far in this course, all the maps we have made using ggplot() have been static images. These are useful for lots of different circumstances, and are usually the only choice if we want to produce a written report as a Word document or PDF. But in some circumstances it may be useful to produce a map that you can interact with by zooming in or panning around different areas. This can be useful when readers might want to look at areas of the map in more detail than is possible without zooming in.\nIn the example of murders in Uttar Pradesh, we might want to use an interactive map to zoom in to see which cities are included in each district on the map. To see how this works, explore this map of murder rates in Uttar Pradesh.\n\n\n\n\n\n\nTo make an interactive choropleth map we can use functions from the leaflet package. The functions in the leaflet package work in a similar way to ggplot(), but with a few differences.\nWatch this video that walks through the code needed to produce an interactive map to get started.\n\nTo make maps with leaflet, we use functions to create a stack in a similar way to the ggplot() stacks we have already created. Note that while the concept of a stack is similar, we cannot use functions from the ggplot2 package in a leaflet stack, or vice versa. Also, we construct leaflet stacks using the pipe operator |&gt; rather than the plus operator + that we use in ggplot2.\nWe can create a very basic leaflet map using the leaflet() function to create the stack, the addProviderTiles() function to add a base map and the addPolygons() function to add the district outlines.\nlibrary(leaflet)\n\nleaflet(district_murders) |&gt; \n  addProviderTiles(\"CartoDB.Voyager\") |&gt; \n  addPolygons()\nIn this map we use the CartoDB.Voyager style of base map, but leaflet can use a large number of different base maps that you can view in this online gallery.\nThis map isn’t very useful because it doesn’t show how many murders there were in each district. To do that, we need to specify that each district should be coloured according to the values in the murder column of the district_murders object. With a ggplot() map, we would do this with a function such as scale_fill_distiller(), but with a leaflet map the code we need is slightly more complicated and has two separate stages.\nThe first stage is to create a custom function that converts the values in the murder column of the district_murders objects into colours. To do this, we use the colorNumeric() function from the leaflet package. Yes, this means that we are using a function to create a function, which we will then later use to set an argument of another function – programming languages are very powerful, but that sometimes means they are complicated.\nThe colorNumeric() function (note the spelling of “color”) allows us to specify the colour scheme we want using the palette argument. We also have to set the domain argument to be NULL. The palette argument to the colorNumeric()function accepts the same values as the palette argument to the scale_fill_distiller() function from ggplot:\n\n\n\n\n\n\n\n\n\nTo create the custom colour palette function, we use the &lt;- operator to assign it to a name, just as we would with an object. We can give this custom function any name we like, but in the code below we’ll call it colours_red.\nOnce we have created a custom colour palette function using colorNumeric(), we can use that function to create the appropriate values for the fillColor argument of the addPolygons() function in our existing leaflet stack. Since we want the map colours to be controlled by the murder column in the district_murders object, we specify murder as the only argument to the colours_red() palette function we have created.\nOne slight quirk of leaflet is that in order for colours_red() to have access to the columns in the district_murders object, we need to add a tilde (~) operator before the function name, so that the fillColor argument is fillColor = ~ colours_red(murder) rather than fillColor = ~ colours_red(murder). If you forget add do the ~, you will see an error saying:\nError in colours_red(murder) : object 'murder' not found\n# Create a custom colour palette function\ncolours_red &lt;- colorNumeric(palette = \"Reds\", domain = NULL)\n\nleaflet(district_murders) |&gt; \n  addProviderTiles(\"CartoDB.Voyager\") |&gt; \n  addPolygons(\n    fillColor = ~ colours_red(murder),\n    fillOpacity = 0.75,\n    weight = 2,\n    color = \"black\"\n  )\nThere are three improvements we can make to this interactive map. The first is to add a legend, using the addLegend() function. This function needs three arguments:\n\npal specifies which custom palette function controls the legend colours. This should be the same function as used in the fillColor argument of the addPolygons() function.\nvalues specifies which column in the data the legend will show. Once again, the column name should be preceded by a ~ operator.\ntitle specifies the legend title.\n\nleaflet(district_murders) |&gt; \n  addProviderTiles(\"CartoDB.Voyager\") |&gt; \n  addPolygons(\n    fillColor = ~ colours_red(murder),\n    fillOpacity = 0.75,\n    weight = 2,\n    color = \"black\"\n  ) |&gt; \n  addLegend(pal = colours_red, values = ~ murder, title = \"number of murders\")\nNext, we can add an inset map in the corner of the main map. This is useful for interactive maps because if we zoom in to show only a small area, we will be able to use the inset map to stay aware of the wider context.\nWe can do this by adding the addMiniMap() function to the leaflet stack. We will add the argument toggleDisplay = TRUE to add the ability to minimise the inset map if necessary.\nleaflet(district_murders) |&gt; \n  addProviderTiles(\"CartoDB.Voyager\") |&gt; \n  addPolygons(\n    fillColor = ~ colours_red(murder),\n    fillOpacity = 0.75,\n    weight = 2,\n    color = \"black\"\n  ) |&gt; \n  addLegend(\n    pal = colours_red, \n    values = ~ murder, \n    title = \"number of murders\"\n  ) |&gt; \n  addMiniMap(toggleDisplay = TRUE)\nFinally, we can add labels to the districts, which will appear when we move the pointer on a screen over a particular district. We do this by adding the label argument to the existing addPolygons() function. As with the previous sections, we can specify that the labels should be based on the district_name column in the district_murders data using the ~ operator, by setting label = ~ district_name.\nleaflet(district_murders) |&gt; \n  addProviderTiles(\"CartoDB.Voyager\") |&gt; \n  addPolygons(\n    fillColor = ~ colours_red(murder),\n    fillOpacity = 0.75,\n    label = ~ district_name,\n    weight = 2,\n    color = \"black\"\n  ) |&gt; \n  addLegend(\n    pal = colours_red, \n    values = ~ murder, \n    title = \"number of murders\"\n  ) |&gt; \n  addMiniMap(toggleDisplay = TRUE)\nThis map allows you to move around as well as zooming in and out. This is very useful for maps where people may need to see both overall patterns for a large area and details for a smaller area within it. Obviously, interactive maps only work on screens, not for printed maps.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Mapping area data</span>"
    ]
  },
  {
    "objectID": "09_mapping_areas/index.html#calculating-crime-rates",
    "href": "09_mapping_areas/index.html#calculating-crime-rates",
    "title": "9  Mapping area data",
    "section": "9.6 Calculating crime rates",
    "text": "9.6 Calculating crime rates\nOne use of choropleth maps is to combine crime counts with other data. One reason to do this is to calculate a crime rate, i.e. an expression of the frequency of crime that in some way controls for the population that is exposed to crime. We often use choropleth maps for this because we only have population counts for areas, rather than knowing where each individual person lives.\nBefore we learn about different types of crime rate, lets create a targeting table for murder in districts in Uttar Pradesh: a table that shows the districts with the highest number of murders.\n\ndistrict_murders |&gt; \n  # Remove the `geometry` column to make the table clearer\n  st_drop_geometry() |&gt; \n  # Also remove the `state` column, since it is the same for every row\n  select(-state) |&gt; \n  # Arrange the table so the districts with the most murders are first\n  arrange(desc(murder))\n\n# A tibble: 75 × 2\n   district_name murder\n   &lt;chr&gt;          &lt;dbl&gt;\n 1 Meerut           226\n 2 Aligarh          179\n 3 Agra             178\n 4 Bulandshahr      171\n 5 Ghaziabad        166\n 6 Firozabad        140\n 7 Kanpur Nagar     137\n 8 Muzaffarnagar    137\n 9 Bareilly         132\n10 Allahabad        132\n# ℹ 65 more rows\n\n\n\ndistrict_murders_ranked &lt;- district_murders |&gt; \n  st_set_geometry(NULL) |&gt; \n  arrange(desc(murder)) |&gt;  \n  pluck(\"district_name\")\n\nFrom this, we can see that the highest number of murders in 2014 occurred in the Meerut, Aligarh and Agra districts.\n\n9.6.1 Types of crime rate\nThere are three types of crime rate, each of which measures risk in a different way. The most common, and the type of rate that people almost always mean if they talk about a ‘crime rate’ without specifying another type, is the incidence rate. This is the number of crimes in an area divided by the number of people, households or places against which a crime could occur. For example, the incidence rate of burglary in an area might be calculated as the number of crimes per 1,000 households, while the incident rate of homicide might be expressed as the number of homicides per 100,000 people.\n\\[\n\\textrm{incidence} = \\frac{\\textrm{crimes}}{\\textrm{population}}\n\\]\nIncidence rates are useful for comparing the risk from crime in areas of different sizes. For example, knowing that Meerut had more murders in 2014 than any other district in Uttar Pradesh might be useful in deciding where to send more homicide detectives, but it doesn’t tell us much about the risk of being murdered in Meerut because we don’t know if this district has more murders simply because it has more people.\nIncidence rates are useful for comparing areas, but they tell us a little about how likely an individual is to be a victim of crime because crime is heavily concentrated against a few victims (think back to the law of crime concentration we discussed in a previous tutorial). To understand the average risk of a person being a victim of crime once or more, we calculate the prevalence rate. This is the number of people, households, etc. who were victims at least once, divided by the total number of people, households, etc. Prevalence rates are usually expressed as a percentage, so we might say that 1% of the population has been a victim of a particular crime at least once during a year.\n\\[\n\\textrm{prevalence} = \\frac{\\textrm{victims}}{\\textrm{population}}\n\\]\nThe final type of rate is the concentration rate. This tells us how concentrated crime is, and is usually expressed as a single number, e.g. if the concentration rate for a crime is 2.5 then we can say that everyone who was a victim of that crime at least once was on average victimised 2.5 times. This is particularly useful for understanding how important repeat victimisation is to driving up the frequency of crime in an area. This, in turn, might lead us to focus crime-prevention efforts on work to protect recent victims of crime from being victimised again.\n\\[\n\\textrm{concentration} = \\frac{\\textrm{crimes}}{\\textrm{victims}}\n\\]\n\nAll three types of crime rate are average values for an area as a whole, so we are always at risk of committing the ecological fallacy. Remember that while rates are useful for describing areas, that does not imply everyone in an area faces the same risk from crime.\n\n\n\n9.6.2 Choosing a population measure\nTo calculate a crime rate, we need to be able to measure the population that is at risk from that crime. It is very common for analysts to calculate rates based on the number of people who live in an area, not least because that information is often easily available. However, there are many situations in which the residential population of an area is a poor measure of the population at risk for crime there. For example:\n\nRobberies in a shopping area where most of the victims do not live in the area but instead travel from elsewhere to go shopping. This can lead to vastly inflated crime rates for commercial and entertainment districts that have very small residential populations but very large numbers of people coming into the area to work, shop or visit. Crime rates based on residential population almost always give misleading rates for city centres or business districts.\nAssaults on public transport where victims happen to be passing through a given area at the time they are victimised but are doing so as part of a longer journey through several areas, with the crime potentially occurring in any one of them. These crimes are known as interstitial offences.\nResidential burglaries where the targets of crime are homes, not people, so the crime rate may be influenced by the number of people in each household.\n\nIn all these cases, the residential population is a poor measure of the population at risk from crime. Unfortunately, other measures of population (such as counts of people on public transport or walking along a shopping street) are often expensive or difficult to obtain. This is known as the denominator dilemma: should we use residential population to calculate crime rates just because it is the only data that is available?\n\n\n9.6.3 Calculating murder rates in Uttar Pradesh\nIn the case of murder in Uttar Pradesh, residential population is likely to be an acceptable (although not perfect) denominator to use in calculating an incidence rate. This is because Indian districts are relatively large areas and so we can expect that most murder victims will be killed in the district where they live (because people everywhere tend to spend most of their time close to where they live). However, there will definitely be exceptions to this, with people being murdered in a different district to where they live. Working out the extent of this problem would be an important question for a detailed investigation into murder in Uttar Pradesh, but we will not consider it any further here.\nTo calculate an incidence rate, we need to join population data to the existing district_murders object containing murder counts and district outlines. Write the code needed to download population data from https://mpjashby.github.io/crimemappingdata/uttar_pradesh_population.csv and join it to the district_murders object, saving the result as district_murders_pop.\n\ndistrict_pop &lt;- read_csv(\"https://mpjashby.github.io/crimemappingdata/uttar_pradesh_population.csv\")\n\nRows: 75 Columns: 7\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (4): code, district, headquarters, division\ndbl (3): population, area, density_km2\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ndistrict_murders_pop &lt;- left_join(\n  district_murders, \n  district_pop, \n  by = c(\"district_name\" = \"district\")\n)\n\nhead(district_murders_pop)\n\nSimple feature collection with 6 features and 9 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 77.42308 ymin: 24.80419 xmax: 82.35455 ymax: 28.90105\nGeodetic CRS:  WGS 84\n# A tibble: 6 × 10\n  state        district_name                      geom murder code  headquarters\n  &lt;chr&gt;        &lt;chr&gt;                     &lt;POLYGON [°]&gt;  &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;       \n1 Uttar Prade… Agra          ((77.6444 27.23444, 77.6…    178 AG    Agra        \n2 Uttar Prade… Bareilly      ((78.97474 28.41527, 78.…    132 BR    Bareilly    \n3 Uttar Prade… Etah          ((79.20766 27.56734, 79.…     65 ET    Etah        \n4 Uttar Prade… Shahjahanpur  ((80.30972 28.46321, 80.…     88 SJ    Shahjahanpur\n5 Uttar Prade… Pilibhit      ((79.67865 28.84923, 79.…     54 PI    Pilibhit    \n6 Uttar Prade… Allahabad     ((81.54684 25.1848, 81.5…    132 AH    Allahabad   \n# ℹ 4 more variables: division &lt;chr&gt;, population &lt;dbl&gt;, area &lt;dbl&gt;,\n#   density_km2 &lt;dbl&gt;\n\n\nNow that we have a single dataset containing all the variables we need, we can calculate the incidence of murders per 100,000 people.\n\ndistrict_murders_pop &lt;- district_murders_pop |&gt; \n  mutate(murder_rate = murder / (population / 100000)) |&gt; \n  # At the same time we will remove unnecessary variables to clean up the data\n  select(district_name, murder, population, murder_rate)\n\nhead(district_murders_pop)\n\nSimple feature collection with 6 features and 4 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 77.42308 ymin: 24.80419 xmax: 82.35455 ymax: 28.90105\nGeodetic CRS:  WGS 84\n# A tibble: 6 × 5\n  district_name murder population murder_rate                               geom\n  &lt;chr&gt;          &lt;dbl&gt;      &lt;dbl&gt;       &lt;dbl&gt;                      &lt;POLYGON [°]&gt;\n1 Agra             178    4418797        4.03 ((77.6444 27.23444, 77.65405 27.2…\n2 Bareilly         132    4448359        2.97 ((78.97474 28.41527, 78.97981 28.…\n3 Etah              65    1774480        3.66 ((79.20766 27.56734, 79.20418 27.…\n4 Shahjahanpur      88    3006538        2.93 ((80.30972 28.46321, 80.29755 28.…\n5 Pilibhit          54    2031007        2.66 ((79.67865 28.84923, 79.67203 28.…\n6 Allahabad        132    5954391        2.22 ((81.54684 25.1848, 81.55587 25.1…\n\n\n\n\n9.6.4 Mapping crime rates\nTo create a choropleth map of the murder rate using leaflet, we simply use the code from our previous interactive map but specify that the murder_rate column be used to determine the colour of each district polygon rather than the murder column. We will also change the base map to a different style.\n\nWhenever we map a crime rate, it is important that we are explicit about how that rate was calculated. For example, using a legend title such as “rate of murders” would not give enough information to allow readers to understand how to interpret the map. A much better legend title would be “murders per 100,000 residents” or something similar.\n\nWe we need to use a longer legend title, or other text, it can be useful to break the text over multiple lines. Since leaflet maps are built using the same technologies as web pages, the way we wrap text is slightly different to how we do it for maps created with ggplot2. Rather than using the new-line character (\\n) or the str_wrap() function, we will instead use the HTML new-line separator &lt;br&gt; and wrap the whole legend title in the HTML() function from the htmltools package.\nleaflet(district_murders_pop) |&gt; \n  addProviderTiles(\n    \"Esri.WorldImagery\", \n    # Since the base map layer we have chosen is very colourful, we will make it\n    # partially transparent to reduce its visual prominence\n    options = providerTileOptions(opacity = 0.3)\n  ) |&gt; \n  addPolygons(\n    fillColor = ~ colours_red(murder_rate),\n    fillOpacity = 0.75,\n    label = ~ district_name,\n    weight = 2,\n    color = \"black\"\n  ) |&gt; \n  addLegend(\n    pal = colours_red, \n    values = ~ murder_rate, \n    title = htmltools::HTML(\"murders per&lt;br&gt;100,000 residents\")\n  ) |&gt; \n  addMiniMap(toggleDisplay = TRUE)\nWe now have all the code we need to load the necessary data, calculate crime rates and make an interactive map of murders in districts in Uttar Pradesh.\nlibrary(leaflet)\nlibrary(sf)\nlibrary(tidyverse)\n\n# Load data\nmurders &lt;- read_csv(\"https://mpjashby.github.io/crimemappingdata/uttar_pradesh_murders.csv\")\ndistricts &lt;- read_sf(\"https://mpjashby.github.io/crimemappingdata/uttar_pradesh_districts.gpkg\")\ndistrict_pop &lt;- read_csv(\"https://mpjashby.github.io/crimemappingdata/uttar_pradesh_population.csv\")\n\n# Wrangle data\ndistrict_murders_pop &lt;- districts |&gt; \n  # Add murders data to district boundaries\n  left_join(murders, by = c(\"district_name\" = \"district\")) |&gt; \n  # Add population to district boundaries\n  left_join(district_pop, by = c(\"district_name\" = \"district\")) |&gt; \n  # Calculate murder rate\n  mutate(murder_rate = murder / (population / 100000))\n\n# Create a custom colour palette function\ncolours_red &lt;- colorNumeric(palette = \"Reds\", domain = NULL)\n\n# Create interactive map\nleaflet(district_murders_pop) |&gt; \n  addProviderTiles(\n    \"Esri.WorldImagery\", \n    # Since the base map layer we have chosen is very colourful, we will make it\n    # partially transparent to reduce its visual prominence\n    options = providerTileOptions(opacity = 0.3)\n  ) |&gt; \n  addPolygons(\n    fillColor = ~ colours_red(murder_rate),\n    fillOpacity = 0.75,\n    label = ~ district_name,\n    weight = 2,\n    color = \"black\"\n  ) |&gt; \n  addLegend(\n    pal = colours_red, \n    values = ~ murder_rate, \n    title = htmltools::HTML(\"murders per&lt;br&gt;100,000 residents\")\n  ) |&gt; \n  addMiniMap(toggleDisplay = TRUE)\n\n\n9.6.4.1 Check your understanding\nAnswer the following questions to check your understanding of what we’ve learned so far in this tutorial. If you get a question wrong, you can keep trying until you get the right answer.\n\nquiz(\n  caption = \"\",\n  \n  question(\n    \"What type of rate describes the chances of a person being a victim of a crime at least once in a given period?\",\n    answer(\"prevalence rate\", correct = TRUE),\n    answer(\"incidence rate\"),\n    answer(\"concentration rate\"),\n    correct = random_praise(),\n    incorrect = random_encouragement(),\n    allow_retry = TRUE,\n    random_answer_order = TRUE\n  ),\n  \n  question(\n    \"What type of rate describes the frequency of crime after controlling for some measure of the population at risk?\",\n    answer(\"incidence rate\", correct = TRUE),\n    answer(\"prevalence rate\"),\n    answer(\"concentration rate\"),\n    correct = random_praise(),\n    incorrect = random_encouragement(),\n    allow_retry = TRUE,\n    random_answer_order = TRUE\n  ),\n  \n  question(\n    \"What type of rate describes the average number of times each victim of a particular crime is victimised?\",\n    answer(\"concentration rate\", correct = TRUE),\n    answer(\"incidence rate\"),\n    answer(\"prevalence rate\"),\n    correct = random_praise(),\n    incorrect = random_encouragement(),\n    allow_retry = TRUE,\n    random_answer_order = TRUE\n  )\n  \n)",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Mapping area data</span>"
    ]
  },
  {
    "objectID": "09_mapping_areas/index.html#in-summary",
    "href": "09_mapping_areas/index.html#in-summary",
    "title": "9  Mapping area data",
    "section": "9.7 In summary",
    "text": "9.7 In summary\n\nIn this tutorial we have learned how to calculate the number of points in different areas, join different sources of data together, and map crime counts and crime rates on both static and interactive maps.\nWe have also learned about two disadvantages of choropleth maps: the ecological fallacy and the modifiable areal unit problem.\n\n\nTo find out more about the topics covered in this tutorial:\n\nRead the article Crime seen through a cone of resolution by Paul Brantingham, Delmar Dyreson and Patricia Brantingham for more detail about the ecological fallacy in studying crime.\nRead the article Smallest is Better? The Spatial Distribution of Arson and the Modifiable Areal Unit Problem for an example of the modifiable areal unit problem in studying crime.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Mapping area data</span>"
    ]
  },
  {
    "objectID": "10_place_data/index.html",
    "href": "10_place_data/index.html",
    "title": "10  Using data about places",
    "section": "",
    "text": "10.1 Introduction\nLearn how to find and use extra data about places to improve your maps, including open data and data from OpenStreetMap\nThe purpose of most crime maps is to help people make decisions, be they professionals working out how best to respond to crime problems or citizens holding local leaders to account. We can make it easier for people to make decisions by putting crime data into a relevant context. We have already started to do this by adding base maps, titles, legends and so on to our maps.\nSince crime is concentrated in a few places, readers of our crime maps will often be interested in understanding what features of the environment are related to specific concentrations of crime in particular places. Where patterns of crime are related to particular facilities – such as late-night violence being driven by the presence of bars selling alcohol – it can be useful to highlight specific features on our maps.\nAs an example, imagine you are the manager responsible for security on the metro network in Medellin, Colombia. There are several mountains within Medellin, so the city metro network consists of both railway lines in the valley and cable cars up the mountains. The security manager for the metro company will certainly analyse violence on the company’s stations and vehicles, but may also be interested in which stations are in neighbourhoods that themselves have high levels of violence.\nTo help with this, you might produce a map showing the density of homicides recorded by local police.\nThis is an acceptable crime map: it shows the data in a reasonable way, places the data layer at the top of the visual hierarchy and provides suitable context in the title, legend etc. But it is a much less useful map than it could be because it doesn’t show where the metro stations are and this information is not included in the base map. A much better map would add extra layers of data showing the metro stations and the line connecting them.\nFrom this second map, it is much easier to see that Parque Berrío and Prado stations are closest to an area with relatively high numbers of homicides.\nIn this tutorial we will learn how to find relevant data about places and add extra layers to our maps to help readers understand the context within which crimes occur.\nTo get started, watch this video that walks through the code needed to download data from OpenStreetMap for use on our maps.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Using data about places</span>"
    ]
  },
  {
    "objectID": "10_place_data/index.html#loading-csv-data",
    "href": "10_place_data/index.html#loading-csv-data",
    "title": "10  Using data about places",
    "section": "10.2 Loading CSV data",
    "text": "10.2 Loading CSV data\nThroughout this tutorial, we will use homicides in the Colombian city of Medellin as an example. Data on the locations of homicides in Medellin from 2010 to 2019 is available at https://mpjashby.github.io/crimemappingdata/medellin_homicides.csv.\nIn previous tutorials, we have used the read_csv() function from the readr package to load data from CSV files. The read_csv() function assumes that (as the name ‘comma-separated values’ suggests) the columns in a CSV file are separated by commas (,). But not all countries use commas as the column separator in CSV files: some countries use semi-colons (;) instead. This is usually because those countries also use commas instead of periods as the decimal separator inside numbers (so that the number three-point-one-four is written 3,14 instead of 3.14 as in English). If commas are used as decimal separators in numbers in a file, commas cannot also be used to separate columns from one another – otherwise there would be no way to know if a comma represented the decimal mark in a number or the boundary between two columns.\nIf we try to load a CSV file that uses semi-colon separators using the read_csv() function, all the data on each row will be loaded as a single column:\n\nlibrary(tidyverse)\n\nmedellin_homicides &lt;- read_csv(\"https://mpjashby.github.io/crimemappingdata/medellin_homicides.csv\")\n\nWarning: One or more parsing issues, call `problems()` on your data frame for details,\ne.g.:\n  dat &lt;- vroom(...)\n  problems(dat)\n\n\nRows: 9360 Columns: 1\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): fecha_hecho;longitud;latitud;sexo;edad;modalidad\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nhead(medellin_homicides)\n\n# A tibble: 6 × 1\n  `fecha_hecho;longitud;latitud;sexo;edad;modalidad`                            \n  &lt;chr&gt;                                                                         \n1 2019-04-23T12:30:00Z;-75,56985400000;6,25722555000;Mujer;50;Ahorcamiento o es…\n2 2019-05-12T00:51:00Z;-75,61036800000;6,22280611000;Hombre;24;Arma de fuego    \n3 2019-05-12T02:36:00Z;-75,62039100000;6,26238217000;Hombre;34;Arma de fuego    \n4 2019-05-12T03:03:00Z;-75,56176640000;6,26958878000;Hombre;20;Arma de fuego    \n5 2019-05-12T22:40:00Z;-75,53268500000;6,23627692000;Hombre;34;Arma de fuego    \n6 2019-05-12T19:30:00Z;-75,64195830000;6,19619536500;Hombre;25;Arma de fuego    \n\n\nThis is obviously not what we want, so we need to use a different function to load this data. Fortunately, the readr package has another function that can handle CSV files created using the conventions of countries that use semi colons to separate columns: read_csv2().\nHow should you know when to use read_csv2() rather than read_csv()? If you don’t know whether a file uses commas or semi colons to separate columns, the easiest thing is probably to use read_csv() first. Now load the file and use head() to look at the first few rows: if you see all the data has appeared in a single column that contains several semi colons, then you’ll know to change your code to use read_csv2() instead.\nFor this dataset, if you load it with read_csv2() you should find that the structure of the data is more as you’d expect it to be.\n\nmedellin_homicides &lt;- read_csv2(\"https://mpjashby.github.io/crimemappingdata/medellin_homicides.csv\")\n\nℹ Using \"','\" as decimal and \"'.'\" as grouping mark. Use `read_delim()` for more control.\n\n\nRows: 9360 Columns: 6\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \";\"\nchr  (2): sexo, modalidad\ndbl  (3): longitud, latitud, edad\ndttm (1): fecha_hecho\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nhead(medellin_homicides)\n\n# A tibble: 6 × 6\n  fecha_hecho         longitud latitud sexo    edad modalidad                   \n  &lt;dttm&gt;                 &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;  &lt;dbl&gt; &lt;chr&gt;                       \n1 2019-04-23 12:30:00    -75.6    6.26 Mujer     50 Ahorcamiento o estrangulami…\n2 2019-05-12 00:51:00    -75.6    6.22 Hombre    24 Arma de fuego               \n3 2019-05-12 02:36:00    -75.6    6.26 Hombre    34 Arma de fuego               \n4 2019-05-12 03:03:00    -75.6    6.27 Hombre    20 Arma de fuego               \n5 2019-05-12 22:40:00    -75.5    6.24 Hombre    34 Arma de fuego               \n6 2019-05-12 19:30:00    -75.6    6.20 Hombre    25 Arma de fuego               \n\n\nIn the rest of this tutorial we will use data from different sources to better understand clusters of homicides in the La Candelaria neighbourhood of downtown Medellin.\n\n\n10.2.1 Check your understanding\n\nquiz(\n  caption = \"\",\n  \n  question(\n    \"Which function should you use to load a CSV file of crime locations that uses semi-colons to separate the columns?\",\n    answer(\"`read_csv2()` from the `readr` package\", correct = TRUE),\n    answer(\n      \"`read_csv()` from the `readr` package\",\n      message = \"`read_csv()` is used to load CSV files that use commas to separate columns.\"\n    ),\n    answer(\n      \"`read.csv2()` from the `base` package\",\n      message = \"While the `read.csv2()` function from the `base` package will load CSV files that use semi-colons to separate columns, it is better to use a function from the `readr` package so that the loaded data will be in a tibble rather than a data frame, and so that text values will not be silently changed to categorical values.\"\n    ),\n    answer(\n      \"`read_sf()` or `st_read()` from the `sf` package\",\n      message = \"CSV is not a spatial file format (even when it contains columns that represent co-ordinates), so it is best not to load them with functions from the `sf` package (which expect to handle spatial file formats). While `read_sf()` and `st_read()` can open CSV files, both functions assume all the columns contain text values, meaning you then have to use another function to convert column values to numbers, dates, etc.\"\n    ),\n    correct = random_praise(),\n    allow_retry = TRUE,\n    random_answer_order = TRUE\n  )\n  \n)",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Using data about places</span>"
    ]
  },
  {
    "objectID": "10_place_data/index.html#finding-data",
    "href": "10_place_data/index.html#finding-data",
    "title": "10  Using data about places",
    "section": "10.3 Finding data",
    "text": "10.3 Finding data\nIf you are producing crime maps on behalf of a particular organisation such as a police agency or a body responsible for managing a place, it is likely that they will hold spatial data that is relevant to the local area. For example, many city governments will hold records of local businesses. It will sometimes be necessary to track down which department or individual holds this data, and it may also be necessary to convert data into formats that are useful for spatial analysis.\nSome organisations may also have agreements to share data with others. For example, both universities and public agencies such as police forces in the United Kingdom have agreements with the national mapping agency Ordnance Survey to share a wide variety of spatial data. If you are producing maps on behalf of an organisation, it will often be useful to ask what data they hold that might be relevant, or ask for a specific dataset you think would help improve a map.\n\n10.3.1 Open data\nOpen data is data that is released by organisations or individuals that can be freely used by others. Organisations such as local governments increasingly release data about their areas as open data – almost all of the data we have used so far in this course is open data released by different local and national governments.\nOpen data is extremely useful because you can skip the often lengthy and painful process of getting access to data and wrangling it into a format you can use. This means you can move on much more quickly to analysing data, reaching conclusions and making decisions. Watch this video to find out more about the value of open data.\n\nOpen data is published in a wide variety of formats and distributed in different ways. Some data might only be distributed by an organisation sending you a DVD or memory stick. Most of the time, however, data will be released online.\nMany cities (especially but not only in developed countries) now maintain open-data websites that act as a repository for all their open data. For example, the City of Bristol in England publishes the Open Data Bristol website. Anyone can use this website to download data on everything from population estimates to politicians’ expenses. Many of these datasets can be useful for crime mapping. For example, you can download the locations of CCTV cameras (useful in criminal investigations), street lights (relevant to designing out crime) and the catchment areas of secondary schools (helpful if a crime-prevention strategy includes visits to schools).\nDifferent local governments may use different terms for the same types of information, so it sometimes takes some trial and error to find if a particular dataset is available. Some data might also be held by organisations other than the main local government agency for a particular place. For example, data on the locations of electricity substations (useful if you are trying to prevent metal thefts from infrastructure networks) might be held by a power company. All this means that tracking down a particular dataset might require some detective work.\nTo try to make this process easier, some countries have established national open-data portals such as Open Data in Canada, Open Government India, data.gov.uk in the United Kingdom and data.gov in the United States. There are also international repositories such as the African Development Bank Data Portal, openAfrica, the Open Data Network and Data Portals, which seeks to list all the open data portals run by different governments and other organisations.\n\n\n10.3.2 Citing data\nOrganisations that provide data often do so on condition that users of the data follow certain rules. For example, you can use data on the Open Data Bristol website as long as you follow the conditions of the Open Government Licence. The most-common requirement of an open-data licence is that anyone using the data acknowledges the data source in any maps, reports or other outputs they produce. In the case of the Open Government Licence, users of the data are required to add a declaration to any outputs declaring:\n\nContains public sector information licensed under the Open Government Licence v3.0.\n\nComplying with open-data licences is a legal requirement, so it is important to make sure you understand what obligations you are accepting when you use a particular dataset. You can typically find the conditions for using a dataset on the website that you download the data from. If you are required to add an attribution statement to your maps, a good place to do this is by adding it to any other information you place in the caption argument of the labs() function in a ggplot() stack.\n\n\n10.3.3 Check your understanding\n\nquiz(\n  caption = \"\",\n  \n  question(\n    \"Which one of these statements about open data is true?\",\n    answer(\"We can use open data for any purpose as long as we comply with the requirements of the licence the data is released under.\", correct = TRUE),\n    answer(\n      \"We can use open data for any purpose -- there is no need to acknowledge the source of the data.\",\n      message = \"While we can often use open data for almost any purpose, it is important to comply with the requirements of the licence the data was released under. Most open-data licences include a requriement to acknowledge the source of the data.\"\n    ),\n    answer(\n      \"We can use open data, but only for non-commercial purposes.\",\n      message = \"Most open data licences allow us to use data for both commercial and non-commercial purposes, as long as we comply with the other requirements of the licence -- most often this includes a requriement to acknowledge the source of the data.\"\n    ),\n    answer(\n      \"We can download open data but we cannot use it for any project that will be published online.\",\n      message = \"Organisations usually publish open data to make it easier for other organisations and individuals to use that data to make products and analyse local issues. It would be extremely unusual for an open-data licence to stop people from using the data in a project that was going to be published online.\"\n    ),\n    correct = random_praise(),\n    allow_retry = TRUE,\n    random_answer_order = TRUE\n  )\n  \n)",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Using data about places</span>"
    ]
  },
  {
    "objectID": "10_place_data/index.html#shapefiles",
    "href": "10_place_data/index.html#shapefiles",
    "title": "10  Using data about places",
    "section": "10.4 Shapefiles",
    "text": "10.4 Shapefiles\nIn this course we have used spatial data provided in different formats including geopackages (.gpkg) and geoJSON (.geojson) files, as well as creating spatial objects from tabular data in formats like CSV and Excel files. But there is one spatial-data format that we haven’t yet learned to use: the shapefile.\nThe shapefile format was created by Esri, the company that makes the ArcGIS suite of mapping software. It was perhaps the first spatial format that could be read by a wide variety of mapping software, which meant that lots of providers of spatial data began to provide data in shapefile format. Shapefiles are limited in various ways that mean they are unlikely to be a good choice for storing your own data, but it is important to know how to use them because many spatial datasets are still provided as shapefiles for historical reasons.\nOne of the complications of using shapefiles (and why they’re not a good choice for storing your own data) is that different parts of the data are stored in separate files. So while the co-ordinates of the points, line or polygons are stored in a file with a .shp extension, the non-spatial attributes of each spatial feature (such as the date on which a crime occurred or the name of a neighbourhood) are stored in a separate file with a .dbf extension and details of the co-ordinate reference system are stored in a .prj file – a single dataset might be held in up to 16 separate files on a computer. All the files that make up a shapefile have the same file name, differing only in the file extension (e.g. .shp, .dbf, etc.). For example, if a .shp file is called robberies.shp then it will be accompanied by a file called robberies.dbf and one called robberies.prj, as well as a robberies.shx index file and possibly several others. All these separate files make it more-complicated to manage shapefiles than other spatial file formats such as the geopackage.\nBecause storing spatial data in a shapefile requires multiple different files, shapefile data is usually distributed in a .zip file that contains all the component files. This means that to access a shapefile will have to add a step to our usual routine for downloading and opening a data file. To minimise the hassle associated with using shapefiles, in general we will:\n\ndownload the .zip file if we don’t have a local copy already,\ncreate a temporary directory where we can store the unzipped shapefile, so we can save space on our computers by only permanently keeping the (often much smaller) .zip file,\nunzip the .zip file into the temporary directory,\nload the shapefile data from the temporary directory.\n\nFor example, the routes of metro lines in Medellin are available in shapefile format at:\nhttps://mpjashby.github.io/crimemappingdata/medellin_metro_lines.zip\nTo load the data from this file, we can use the process shown above.\n\nUnfortunately it isn’t possible to run this code within this interactive tutorial because of security restrictions on saving files on your computer from within a tutorial. You can test this code by copying it into a new R Script in RStudio and running the code from there. Note that this code assumes you have already loaded the sf and tidyverse packages.\n\n# Step 1: download the .zip file to a temporary file\nmetro_lines_file &lt;- tempfile(fileext = \".zip\")\ndownload.file(\n  url = \"https://mpjashby.github.io/crimemappingdata/medellin_metro_lines.zip\", \n  destfile = metro_lines_file\n)\n\n# Step 2: create a temporary directory\n# The `tempdir()` function returns a location on your computer that is used for\n# storing temporary files. *Any files stored in this temporary directory will be\n# deleted when you restart your computer*, so it's a useful place to put files\n# that you will only need for a short time so they won't clutter up your\n# computer. Since we want to store the shapefile in a sub-directory of the\n# temporary directory, we will use `str_glue()` to add a relevant sub-directory\n# name to the end of the temporary directory name -- `unzip()` will then\n# create this directory in the background at Step 3.\nmetro_lines_dir &lt;- str_glue(\"{tempdir()}/metro_lines\")\n\n# Step 3: unzip file\nunzip(metro_lines_file, exdir = metro_lines_dir)\n\n# Step 5: load the data\nmedellin_metro_lines &lt;- metro_lines_dir |&gt; \n  str_glue(\"/medellin_metro_lines.shp\") |&gt; \n  read_sf()\n\nNote that although a shapefile consists of several different files, we only need to load the file with the extension .shp – the read_sf() function will find all the data it needs from the other files.\nOnce we have loaded a shapefile into R using read_sf(), we can treat it in the same way as any other spatial dataset – it is only loading shapefiles that is different from other spatial data formats.\n\n\n\nHow did you know the name of the medellin_metro_lines.shp file?\n\n\nIf you need to find out the name of the shapefile within the zip file, you can use the list = TRUE argument to the unzip() function to produce a list of files that are inside the zip file rather than unzip any files. For example, the code unzip(metro_lines_file, list = TRUE) would produce a data frame of file names:\n\n\n\n\n\nName\nLength\nDate\n\n\n\n\nmedellin_metro_lines.dbf\n2299\n2023-02-06 22:46:00\n\n\nmedellin_metro_lines.prj\n145\n2023-02-06 22:46:00\n\n\nmedellin_metro_lines.shp\n16076\n2023-02-06 22:46:00\n\n\nmedellin_metro_lines.shx\n172\n2023-02-06 22:46:00\n\n\n\n\n\nMake sure you run unzip(metro_lines_file, list = TRUE) in the R console rather than in your script file, to minimise the amount of unnecessary output that your script produces.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Using data about places</span>"
    ]
  },
  {
    "objectID": "10_place_data/index.html#data-from-openstreetmap",
    "href": "10_place_data/index.html#data-from-openstreetmap",
    "title": "10  Using data about places",
    "section": "10.5 Data from OpenStreetMap",
    "text": "10.5 Data from OpenStreetMap\n\nOften we can get map data from the organisation we are working for, or from open-data portals run by governments or international organisations. But sometimes they won’t hold the information we need.\nFortunately, there is another source of data: OpenStreetMap (OSM). This is a global resource of map data created by volunteers (and started at UCL), using a mixture of open data from governments, data contributed by charities and data collected by the volunteers themselves. Watch this video to learn a bit more about OpenStreetMap.\n\nWe have already used OSM data in this course: all of the base maps we have used when we create maps with ggplot() are based on data from OpenStreetMap. But we have very little control over which information is and is not included in base maps. Sometimes we need more control over the data, and that means downloading data direct from OSM.\nWe can download OSM data into R using the osmdata package. This package allows us to choose particular features from the billions of features worldwide that are included in the OSM database. To choose features, we must:\n\nspecify the bounding box of the area we want to download data for using the opq() function,\nspecify what type of features we want to download using the add_osm_feature() function,\ndownload the data using the osmdata_sf() function, and\nextract the type of spatial object (points, lines or polygons) that we are interested in.\n\nImagine that in your analysis of homicides in Medellin, you have been asked to consider the question of whether homicides are clustered near to bus stops. To answer this question, we need to know the locations of bus stops in the area we are interested in. This information is not published as open data by the Medellin city authorities. Fortunately we can extract bus-stop locations from OpenStreetMap using the osmdata package.\nTo do this, we first need to calculate the bounding box of the La Candelaria neighbourhood that we are interested in. A bounding box is the smallest rectangle that a particular spatial shape will fit inside. For example, the red rectangle on this map shows the bounding box of the city of Medellin (shown in blue).\n\n\n\nYou can calculate the bounding box of an SF object using the st_bbox() function.\n\nAssuming we have already loaded the neighbourhood boundaries into an object called medellin_comunas, write the code needed to filter that object so that only the boundary for the La Candelaria neighbourhood is included, then calculate the bounding box for that layer and store it in an object called la_candelaria_bbox.\nNote that the medellin_comunas object uses the Colombia MANGA West Zone co-ordinate system (EPSG code 3115), but the osmdata package only works with bounding boxes specified as longitudes and latitudes. This means you will also need to transform the dataset to use the WGS84 co-ordinate system (EPSG code 4326) before you calculate the bounding box.\n\n# You can use the `filter()` function to filter only the rows of data that you \n# want to keep in the data\n\n\n# Remember to use `st_transform()` to make sure the data uses the correct\n# co-ordinate system. You can use the `st_bbox()` function to calculate the \n# bounding box of the 52nd division boundary.\n\n\n# If you need to find out the name of the relevant variable in the \n# `medellin_comunas` object, you can use `head(medellin_comunas)` to see the \n# first few rows.\n\n\n\nla_candelaria_bbox &lt;- medellin_comunas |&gt; \n  filter(nombre == \"LA CANDELARIA\") |&gt; \n  st_transform(\"EPSG:4326\") |&gt; \n  st_bbox()\n\nhead(la_candelaria_bbox)\n\n      xmin       ymin       xmax       ymax \n-75.580293   6.224509 -75.553850   6.265124 \n\n\nThe second thing we need to know is what search terms to use in the add_osm_feature() function to return the locations of bus stops. OpenStreetMap has hundreds of feature categories, all in the format key=value. Sometimes we will only need to search for a particular key (category of feature), such as the highway key that contains all the features that show roads (from motorways to winding lanes leading to farms in the countryside), tracks and paths. In other cases, we will want to search for a particular value (type of feature within a category), such as searching for the value natural=water to search for lakes, rivers, etc.\nThe best place to find out how a feature you are interested in is recorded in the OSM database is to look at the OpenStreetMap Wiki. Bus stops are recorded in OSM using the tag highway=bus_stop.\nNow that we know the bounding box of the area we are interested in and the tag for the type of feature we want, we can download the data from OpenStreetMap.\n\n# Define the bounding box of the area we want to search\nbus_stops &lt;- opq(la_candelaria_bbox) |&gt; \n  # Define the features we want\n  add_osm_feature(key = \"highway\", value = \"bus_stop\") |&gt; \n  # Download those features for that area\n  osmdata_sf()\n\n# Print the result (note the result is not a data frame, so we cannot use the\n# `head()` function)\nbus_stops\n\nObject of class 'osmdata' with:\n                 $bbox : 6.22450932830104,-75.5802925457919,6.26512401936258,-75.5538503495463\n        $overpass_call : The call submitted to the overpass API\n                 $meta : metadata including timestamp and version numbers\n           $osm_points : 'sf' Simple Features Collection with 55 points\n            $osm_lines : NULL\n         $osm_polygons : 'sf' Simple Features Collection with 0 polygons\n       $osm_multilines : NULL\n    $osm_multipolygons : NULL\n\n\nYou’ll see that the object bus_stops has quite complicated structure, but that nested within it is an object called osm_points that is an SF object with 55 rows and another SF object called osm_polygons. Even within a particular type of feature, some places might be represented as points (e.g. a point placed at a bus stop) while others are represented as polygons (e.g. the outline of a bus station).\nWe can use the pluck() function from the purrr package (part of the tidyverse) to extract the parts of the bus_stops object that we want. If we extract the SF object called osm_points and look at the first few rows using bus_stops |&gt; pluck(\"osm_points\") |&gt; head(n = 5), we can see:\n\nbus_stops |&gt; pluck(\"osm_points\") |&gt; head(5)\n\nSimple feature collection with 5 features and 19 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: -75.57696 ymin: 6.228165 xmax: -75.56289 ymax: 6.260205\nGeodetic CRS:  WGS 84\n               osm_id                 name access bench  bin  bus\n847830985   847830985                 &lt;NA&gt;   &lt;NA&gt;    no &lt;NA&gt;  yes\n1561073855 1561073855         La Alpujarra   &lt;NA&gt;  &lt;NA&gt; &lt;NA&gt; &lt;NA&gt;\n2418112357 2418112357                 &lt;NA&gt;   &lt;NA&gt;  &lt;NA&gt; &lt;NA&gt; &lt;NA&gt;\n3135948756 3135948756      Barrio Colombia   &lt;NA&gt;  &lt;NA&gt; &lt;NA&gt;  yes\n3346088239 3346088239 Carrera 33 Calle 29C   &lt;NA&gt;    no &lt;NA&gt; &lt;NA&gt;\n           check_date:shelter covered  highway  lit    local_ref name:en\n847830985                &lt;NA&gt;    &lt;NA&gt; bus_stop &lt;NA&gt;         &lt;NA&gt;    &lt;NA&gt;\n1561073855               &lt;NA&gt;    &lt;NA&gt; bus_stop &lt;NA&gt; La Alpujarra    &lt;NA&gt;\n2418112357               &lt;NA&gt;    &lt;NA&gt; bus_stop &lt;NA&gt;         &lt;NA&gt;    &lt;NA&gt;\n3135948756               &lt;NA&gt;    &lt;NA&gt; bus_stop &lt;NA&gt;         &lt;NA&gt;    &lt;NA&gt;\n3346088239               &lt;NA&gt;      no bus_stop &lt;NA&gt;         &lt;NA&gt;    &lt;NA&gt;\n             network              note          operator public_transport\n847830985       &lt;NA&gt;              &lt;NA&gt;              &lt;NA&gt;         platform\n1561073855      &lt;NA&gt;              &lt;NA&gt;              &lt;NA&gt;             &lt;NA&gt;\n2418112357      &lt;NA&gt;              &lt;NA&gt;              &lt;NA&gt;             &lt;NA&gt;\n3135948756 Metroplus Sentido Norte Sur Metro de Medellín         platform\n3346088239      &lt;NA&gt;              &lt;NA&gt; Metro de Medellín             &lt;NA&gt;\n           shelter source wheelchair                   geometry\n847830985      yes   &lt;NA&gt;       &lt;NA&gt; POINT (-75.57696 6.260205)\n1561073855    &lt;NA&gt;   &lt;NA&gt;       &lt;NA&gt; POINT (-75.57342 6.245391)\n2418112357    &lt;NA&gt;   &lt;NA&gt;       &lt;NA&gt;  POINT (-75.56893 6.24997)\n3135948756     yes   &lt;NA&gt;        yes POINT (-75.56982 6.228716)\n3346088239      no   &lt;NA&gt;       &lt;NA&gt; POINT (-75.56289 6.228165)\n\n\nWe can see from this that most of the fields are blank, but there is a name column and a geometry column that we can use to plot the locations of the bus stops.\nWe also need to check the contents of the osm_polygons layer inside the bus_stops object, to see if it contains details of a any more bus stops that are not included in the osm_points layer.\n\nType the code needed to extract the osm_polygons layer and view the first few rows.\n\n\nTo check this, we can again use the pluck() function:\n\n\nbus_stops |&gt; pluck(\"osm_polygons\") |&gt; head(5)\n\nSimple feature collection with 0 features and 1 field\nBounding box:  xmin: NA ymin: NA xmax: NA ymax: NA\nGeodetic CRS:  WGS 84\n[1] osm_id   geometry\n&lt;0 rows&gt; (or 0-length row.names)\n\n\nIn this case, we can see that there are 0 rows in the osm_polygons object. In cases where we have data contained in both the osm_points and osm_polygons layers, we need to merge the two layers by converting the polygon layer to a point layer using the st_centroid() function and then merging the two layers using the bind_rows() function from the dplyr package. We can put all this together into one piece of code.\n\nbind_rows(\n  pluck(bus_stops, \"osm_points\"), \n  st_centroid(pluck(bus_stops, \"osm_polygons\"))\n)\n\nSimple feature collection with 55 features and 19 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: -75.57967 ymin: 6.226014 xmax: -75.55641 ymax: 6.264754\nGeodetic CRS:  WGS 84\nFirst 10 features:\n               osm_id                          name access bench  bin  bus\n847830985   847830985                          &lt;NA&gt;   &lt;NA&gt;    no &lt;NA&gt;  yes\n1561073855 1561073855                  La Alpujarra   &lt;NA&gt;  &lt;NA&gt; &lt;NA&gt; &lt;NA&gt;\n2418112357 2418112357                          &lt;NA&gt;   &lt;NA&gt;  &lt;NA&gt; &lt;NA&gt; &lt;NA&gt;\n3135948756 3135948756               Barrio Colombia   &lt;NA&gt;  &lt;NA&gt; &lt;NA&gt;  yes\n3346088239 3346088239          Carrera 33 Calle 29C   &lt;NA&gt;    no &lt;NA&gt; &lt;NA&gt;\n3346088240 3346088240          Carrera 34 Calle 29C   &lt;NA&gt;    no &lt;NA&gt; &lt;NA&gt;\n3346088241 3346088241                    Carrera 34   &lt;NA&gt;    no &lt;NA&gt; &lt;NA&gt;\n3346088242 3346088242       Calle 29 Av. Las Palmas   &lt;NA&gt;    no &lt;NA&gt; &lt;NA&gt;\n3346088245 3346088245     Carrera 38 Av. Las Palmas   &lt;NA&gt;   yes   no &lt;NA&gt;\n3346088246 3346088246 Carrera 38 Avenida Las Palmas   &lt;NA&gt;    no &lt;NA&gt; &lt;NA&gt;\n           check_date:shelter covered  highway  lit    local_ref name:en\n847830985                &lt;NA&gt;    &lt;NA&gt; bus_stop &lt;NA&gt;         &lt;NA&gt;    &lt;NA&gt;\n1561073855               &lt;NA&gt;    &lt;NA&gt; bus_stop &lt;NA&gt; La Alpujarra    &lt;NA&gt;\n2418112357               &lt;NA&gt;    &lt;NA&gt; bus_stop &lt;NA&gt;         &lt;NA&gt;    &lt;NA&gt;\n3135948756               &lt;NA&gt;    &lt;NA&gt; bus_stop &lt;NA&gt;         &lt;NA&gt;    &lt;NA&gt;\n3346088239               &lt;NA&gt;      no bus_stop &lt;NA&gt;         &lt;NA&gt;    &lt;NA&gt;\n3346088240               &lt;NA&gt;      no bus_stop &lt;NA&gt;         &lt;NA&gt;    &lt;NA&gt;\n3346088241               &lt;NA&gt;      no bus_stop &lt;NA&gt;         &lt;NA&gt;    &lt;NA&gt;\n3346088242               &lt;NA&gt;      no bus_stop &lt;NA&gt;         &lt;NA&gt;    &lt;NA&gt;\n3346088245               &lt;NA&gt;      no bus_stop &lt;NA&gt;         &lt;NA&gt;    &lt;NA&gt;\n3346088246               &lt;NA&gt;      no bus_stop &lt;NA&gt;         &lt;NA&gt;    &lt;NA&gt;\n             network              note          operator public_transport\n847830985       &lt;NA&gt;              &lt;NA&gt;              &lt;NA&gt;         platform\n1561073855      &lt;NA&gt;              &lt;NA&gt;              &lt;NA&gt;             &lt;NA&gt;\n2418112357      &lt;NA&gt;              &lt;NA&gt;              &lt;NA&gt;             &lt;NA&gt;\n3135948756 Metroplus Sentido Norte Sur Metro de Medellín         platform\n3346088239      &lt;NA&gt;              &lt;NA&gt; Metro de Medellín             &lt;NA&gt;\n3346088240      &lt;NA&gt;              &lt;NA&gt; Metro de Medellín             &lt;NA&gt;\n3346088241      &lt;NA&gt;              &lt;NA&gt; Metro de Medellín             &lt;NA&gt;\n3346088242      &lt;NA&gt;              &lt;NA&gt; Metro de Medellín             &lt;NA&gt;\n3346088245      &lt;NA&gt;              &lt;NA&gt; Metro de Medellín             &lt;NA&gt;\n3346088246      &lt;NA&gt;              &lt;NA&gt; Metro de Medellín             &lt;NA&gt;\n           shelter source wheelchair                   geometry\n847830985      yes   &lt;NA&gt;       &lt;NA&gt; POINT (-75.57696 6.260205)\n1561073855    &lt;NA&gt;   &lt;NA&gt;       &lt;NA&gt; POINT (-75.57342 6.245391)\n2418112357    &lt;NA&gt;   &lt;NA&gt;       &lt;NA&gt;  POINT (-75.56893 6.24997)\n3135948756     yes   &lt;NA&gt;        yes POINT (-75.56982 6.228716)\n3346088239      no   &lt;NA&gt;       &lt;NA&gt; POINT (-75.56289 6.228165)\n3346088240      no   &lt;NA&gt;       &lt;NA&gt; POINT (-75.56364 6.229471)\n3346088241      no   &lt;NA&gt;       &lt;NA&gt;  POINT (-75.5649 6.227743)\n3346088242      no   &lt;NA&gt;       &lt;NA&gt; POINT (-75.56406 6.226646)\n3346088245      no   &lt;NA&gt;       &lt;NA&gt; POINT (-75.56695 6.233343)\n3346088246      no   &lt;NA&gt;       &lt;NA&gt; POINT (-75.56712 6.233176)\n\n\n\n\nWhat does the warning st_centroid assumes … mean?\n\n\nYou might have seen a warning saying st_centroid assumes attributes are constant over geometries of x. You will see this warning when you use the st_centroid() function. It is there to remind you that columns in the original data (which the SF package refers to as the attributes associated with each spatial feature) refer to the polygon as a whole, but in the object produced by st_centroid() it will appear that the columns relate to the centroid point. In many cases this will not be a problem, but it could expose you to the ecological fallacy so it is sometimes useful to be reminded.\n\n\n\nThis code references the bus_stops object twice, which means we cannot use this code within a pipeline in the usual way. This means it will be necessary to save the result produced by osmdata_sf() in an object and then combine the points and polygon centroids in a separate piece of code.\n\nWe now have everything we need to map homicides in La Candelaria in relation to bus stops. Create a map showing a suitable base map, the density of homicides in the La Candelaria neighbourhood, the locations of bus stop as individual points and the boundary of the neighbourhood.\nYou will need to:\n\nCreate an object holding the boundary of the La Candelaria neighbourhood. Remember the boundaries of Medellin neighbourhoods are contained in the medellin_comunas object.\nEstimate the density of homicides in the La Candelaria neighbourhood. The homicide locations are stored in the medellin_homicides object, although you will probably want to extract only those in La Candelaria before estimating the density.\nExtract the bounding box of the neighbourhood boundary and use that to get the locations of bus stops, taking into account that some bus stops might be stored in the OSM database as points and others as polygons.\nCreate a map showing the density of homicides, the locations of bus stops and the boundary of the neighbourhood.\n\n\n\nNow that we have everything we need, we can create a map of homicides in La Candelaria.\n\n\n# There are lots of design decisions you could make in producing a map -- the\n# following code is a minimal map, which you could improve on in several ways\n\n# Load data\nmedellin_comunas &lt;- read_sf(\"https://mpjashby.github.io/crimemappingdata/medellin_comunas.gpkg\") |&gt; \n  janitor::clean_names() |&gt; \n  st_transform(\"EPSG:3115\") |&gt; \n  select(nombre, geom)\nmedellin_homicides &lt;- read_csv2(\"https://mpjashby.github.io/crimemappingdata/medellin_homicides.csv\") |&gt; \n  remove_missing(vars = c(\"longitud\", \"latitud\")) |&gt; \n  st_as_sf(coords = c(\"longitud\", \"latitud\"), crs = \"EPSG:4326\")\n\nℹ Using \"','\" as decimal and \"'.'\" as grouping mark. Use `read_delim()` for more control.\n\n\nRows: 9360 Columns: 6\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \";\"\nchr  (2): sexo, modalidad\ndbl  (3): longitud, latitud, edad\ndttm (1): fecha_hecho\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nWarning: Removed 81 rows containing missing values or values outside the scale\nrange.\n\n# Create neighbourhood boundary\nla_candelaria &lt;- medellin_comunas |&gt; \n  filter(nombre == \"LA CANDELARIA\") |&gt; \n  # This object needs to use the same co-ordinate system as `medellin_homicides`\n  # so we can use `st_intersection()`, so transform it first\n  st_transform(\"EPSG:3115\")\n\n# Estimate homicide density\nla_candelaria_homicide_density &lt;- medellin_homicides |&gt; \n  st_transform(\"EPSG:3115\") |&gt; \n  # Extract only those homicides occurring within the La Candelaria \n  # neighbourhood boundary (otherwise `hotspot_kde()` will be very slow)\n  st_intersection(la_candelaria) |&gt; \n  hotspot_kde(\n    grid = hotspot_grid(la_candelaria, cell_size = 100), \n    bandwidth_adjust = 0.33,\n    quiet = TRUE\n  ) |&gt; \n  st_intersection(la_candelaria)\n\nWarning: attribute variables are assumed to be spatially constant throughout\nall geometries\n\n\nWarning: attribute variables are assumed to be spatially constant throughout\nall geometries\n\n# Get bus stop locations\nbus_stops &lt;- la_candelaria |&gt; \n  # `opq()` needs longitude/latitude co-ordinates, so transform before \n  # calculating the bounding box\n  st_transform(\"EPSG:4326\") |&gt; \n  st_bbox() |&gt; \n  opq() |&gt; \n  # Define the features we want\n  add_osm_feature(key = \"highway\", value = \"bus_stop\") |&gt; \n  # Download those features for that area\n  osmdata_sf()\n\n# Extract bus stop locations as points\nbus_stop_points &lt;- bind_rows(\n  pluck(bus_stops, \"osm_points\"), \n  st_centroid(pluck(bus_stops, \"osm_polygons\"))\n)\n\n# Plot map\nggplot() +\n  annotation_map_tile(type = \"cartolight\", zoomin = 0, progress = \"none\") +\n  geom_sf(\n    aes(fill = kde), \n    data = la_candelaria_homicide_density, \n    alpha = 0.7, \n    colour = NA\n  ) +\n  geom_sf(data = la_candelaria, colour = \"grey40\", fill = NA, linewidth = 1.5) +\n  geom_sf(data = bus_stop_points, colour = \"darkred\") +\n  scale_fill_distiller(\n    direction = 1, \n    breaks = range(pull(la_candelaria_homicide_density, \"kde\")),\n    labels = c(\"lower\", \"higher\")\n  ) +\n  labs(\n    caption = str_glue(\n      \"Contains data from OpenStreetMap\\n\",\n      \"Homicide data: Alcaldía de Medellín (CC-BY-SA)\"\n    ),\n    fill = \"homicide\\ndensity\"\n  ) +\n  # We can add the `fixed_plot_aspect()` function to the `ggplot()` stack to\n  # force the map to be square, rather than a rectangle\n  fixed_plot_aspect() +\n  theme_void()\n\n\n\n\n\n\n\n\nFrom this map, it looks like homicides do not cluster particularly around bus stops. This would probably be welcome information for the city’s public transport managers.\n\nJust as with other sources of map data, you are legally required to cite data from OpenStreetMap if you use it. The code in the exercise above, for example, cites data from two sources:\n\n“Contains data from OpenStreetMap” acknowledges that both the base map and the bus-stop locations were obtained from OpenStreetMap.\n“Homicide data: Alcaldía de Medellín (CC-BY-SA)” acknowledges that the Medellin homicide data were released by the Mayor of Medellin under the Creative Commons Attribution Share-alike (CC-BY-SA) licence.\n\n\n\nThe OpenStreetMap logo is a trademark of the OpenStreetMap Foundation, and is used with their permission. This tutorial not endorsed by or affiliated with the OpenStreetMap Foundation.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Using data about places</span>"
    ]
  },
  {
    "objectID": "10_place_data/index.html#in-summary",
    "href": "10_place_data/index.html#in-summary",
    "title": "10  Using data about places",
    "section": "10.6 In summary",
    "text": "10.6 In summary\n\nIn this tutorial we have learned how to find open data, including data from OpenStreetMap, and add it to our maps to help readers better understand crime patterns. We will be able to use these skills to add data to future maps that we make so that readers can gain more insight into crime patterns or other phenomena that we might be analysing.\n\n\nTo find out more about the skills we have worked on in this tutorial, you may want to read:\n\na paper exploring how open crime data can be used in researching crime, and\na more-detailed introduction to the osmdata package written by Mark Padgham and Robin Lovelace.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Using data about places</span>"
    ]
  },
  {
    "objectID": "11_mapping_hotspots/index.html",
    "href": "11_mapping_hotspots/index.html",
    "title": "11  Mapping hotspots",
    "section": "",
    "text": "11.1 What is a hotspot?\nLearn what crime hotspots are, why they are important and how to map them.\nCrime is heavily concentrated in several different ways. A small number of offenders commit a large proportion of crime (even though most people commit minor offences occasionally) and a small number of people are repeatedly victimised. For most types of crime, a large proportion of crime occurs in a small number of places. A hotspot is a specific location or small area where an unusual amount of criminal activity occurs.\nCrime hotspots can occur in several different forms. Watch this video to understand why hotspots are important in understanding and responding to crime.\nSome places are chronic hotspots – they have more crime than surrounding areas over a sustained period (which may appear to be permanent). Chronic hotspots are often generated by a facility that draws vulnerable people into an area, such as a tourist attraction that draws crowds of people who are vulnerable to pickpocketing. Other places are acute hotspots, in which crime increases in a place that previously experienced no or few crimes. This may be the result of some change in the environment or how it’s managed, such as new management at a bar that ignores drug dealing that the previous owners would have not permitted.\nWhen analysing hotspots, it is best to focus on small areas such as an apartment block, a shopping centre, a park or a single street. Focusing on smaller areas is important because resources to respond to crime are almost always limited, so it is important that those resources are directed where the problem is worst.\nAnalysing larger areas, such as a neighbourhood or a police sector, is much more difficult because larger areas are always made up of many smaller areas, each of which might be quite different from one another. This means that the factors causing one street to be a hotspot might be quite different from the factors that make another street in the same district into a hotspot. Conflating different problems with different causes makes it much harder to find effective ways to reduce crime in any one place.\nThese difficulties can be avoided (at least partly) by keeping hotspots small: in an urban area, a useful rule of thumb is that you should be able to stand in the middle of a hotspot and see the whole hotspot area.\nBeing able to identify hotspots using crime mapping is important because it forms a vital first step in many place-focused responses to crime. As an example of this, watch this video about how police in Philadelphia worked with researchers to use crime mapping to identify where to deploy foot patrols to reduce crime.\nIn this tutorial we will learn how to make maps that could be useful in identifying and responding to hotspots of crime. As an example, we will create this map showing hotspots of robbery in Nottingham, England.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Mapping hotspots</span>"
    ]
  },
  {
    "objectID": "11_mapping_hotspots/index.html#what-is-a-hotspot",
    "href": "11_mapping_hotspots/index.html#what-is-a-hotspot",
    "title": "11  Mapping hotspots",
    "section": "",
    "text": "11.1.1 Check your understanding\n\nquiz(\n  caption = \"\",\n  question(\"Which *one* of these statements is true?\",\n    answer(\"Crime is very geographically concentrated – we can expect half of crime to be concentrated in about 5% of micro places\", correct = TRUE),\n    answer(\"Crime is usually not geographically concentrated at micro places\"),\n    answer(\"Crime is slightly geographically concentrated – we can expect half of crime to be concentrated in about one quarter of micro places\"),\n    answer(\"Crime is extremely geographically concentrated – we can expect half of crime to be concentrated in about 1% of micro places\"),\n    correct = \"That's correct – based on previous studies, we can expect half of crime to be concentrated in about 5% of micro places\",\n    incorrect = \"That's not correct – try re-watching the first video above and then try the question again.\",\n    allow_retry = TRUE,\n    random_answer_order = TRUE\n  )\n)",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Mapping hotspots</span>"
    ]
  },
  {
    "objectID": "11_mapping_hotspots/index.html#showing-the-density-of-risk",
    "href": "11_mapping_hotspots/index.html#showing-the-density-of-risk",
    "title": "11  Mapping hotspots",
    "section": "11.2 Showing the density of risk",
    "text": "11.2 Showing the density of risk\nIn the tutorial on mapping area data we learned how to produce maps showing the incidence rate of crime by dividing the number of crimes by a measure of the population at risk of being targeted. We will often only have population estimates for areas, such as census estimates of the number of people living in an area. But for some crimes we have access to estimates of the people (or, more often, objects) at risk of being a target of a particular crime. In these cases, we can produce better maps of the risk of crime in different areas by producing a dual KDE map that shows the density of crime risk in different places.\nTo create a dual KDE map, we must estimate the density of crime and compare it to an estimate the density of the population at risk. Since the incidence rate is calculated as the number of crimes divided by the number of people or objects at risk, we can calculate the density of risk by dividing the density of crime estimated for each cell in the grid by the density of population estimated for the same cell. The hotspot_dual_kde() function from the sfhotspot package does this for us.\nTo illustrate making a dual KDE map, we will use reports of burglaries in three wards in Nottingham in 2020. Since the essential element of the crime of burglary in England is that an offender enters a building as a trespasser in order to steal something, the best measure of the population at risk of burglary is the number of buildings in each area (the definition of burglary is more complicated than this, but we don’t need to worry about that here).\nBurglary is a good example of why the routine activities approach to thinking about crime that we introduced in a previous tutorial emphasises thinking about targets of crime rather than focusing only on crime victims. In the case of burglary, one person might be the owner of a large number of buildings (e.g. a farm with lots of out-buildings) or lots of people might own a single building (such as a house converted into flats). By thinking about the targets that are attacked by offenders, we can identify that burglary rates should be calculated based on buildings rather than, for example, residential population. Note that if our crime data only included residential burglaries then we would want to use residential buildings as our denominator, but in this case we have data for all burglaries, both residential and non-residential.\n\n11.2.1 Data wrangling\nBefore we can create our dual KDE layer, we have to complete some data wrangling. We will extract the boundaries for the wards of interest from a dataset of boundaries for all wards in Nottingham using filter() as we have done previously. To extract only the burglaries occurring in those three wards from a dataset of all burglaries in Nottingham, we will use st_intersection(). We will also transform both datasets to use the British National Grid (EPSG code 27700), since we will need to do that anyway before calculating the KDE values.\n\nwards &lt;- read_sf(\"https://mpjashby.github.io/crimemappingdata/nottingham_wards.gpkg\") |&gt; \n  st_transform(\"EPSG:27700\") |&gt; \n  filter(ward_name %in% c(\"Castle\", \"Lenton & Wollaton East\", \"Meadows\"))\n\nburglaries &lt;- read_csv(\"https://mpjashby.github.io/crimemappingdata/nottingham_burglary.csv.gz\") |&gt; \n  st_as_sf(coords = c(\"longitude\", \"latitude\"), crs = 4326) |&gt; \n  st_transform(\"EPSG:27700\") |&gt; \n  st_intersection(wards)\n\nRows: 1795 Columns: 5\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (2): location, lsoa_code\ndbl  (2): longitude, latitude\ndate (1): month\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nWarning: attribute variables are assumed to be spatially constant throughout\nall geometries\n\n\n\nst_intersection() can take longer to run than the maximum time limit for running code within this tutorial. If you see an error saying Your code ran  longer than the permitted time limit for this exercise or reached elapsed time  limit, you can continue with the rest of the tutorial as usual.\n\nWe do not have a source of open data for all the buildings in Nottingham, so we will use the osmdata package to get the locations of buildings from OpenStreetMap (OSM). You may remember from a previous tutorial that to do this we need to know which key (and possibly value) the OSM database uses for storing the locations of buildings. The OSM feature key for a building is ‘building’ and it is not necessary to specify a value (since we want to capture all types of building). The osmdata package expects data to use the WGS84 co-ordinate reference system, so we must also make sure any data sources we use are projected using that system (EPSG code 4326).\n\nRun the code needed to download data from OSM for all the buildings in the three wards we are interested in and store it in an object called nottingham_buildings.\n\n# To download OSM data, use:\n#   * `st_transform()` to transform the data to use the WGS84 co-ordinate system\n#   * `st_bbox()` to calculate the bounding box of the wards\n#   * `opq()` to set up the OSM query \n#   * `add_osm_feature()` to specify what type of features to download\n#   * `osmdata_sf()` to download the data as an SF object\n\n\nlibrary(osmdata)\n\nnottingham_buildings &lt;- wards |&gt; \n  st_transform(\"EPSG:4326\") |&gt; \n  st_bbox() |&gt; \n  opq() |&gt; \n  add_osm_feature(key = \"?????\") |&gt;  # &lt;- specify type of data here\n  osmdata_sf()\n\nnottingham_buildings\n\nObject of class 'osmdata' with:\n                 $bbox : 52.9173362670705,-1.21712477980417,52.9596833099217,-1.13020878819132\n        $overpass_call : The call submitted to the overpass API\n                 $meta : metadata including timestamp and version numbers\n           $osm_points : 'sf' Simple Features Collection with 0 points\n            $osm_lines : NULL\n         $osm_polygons : 'sf' Simple Features Collection with 0 polygons\n       $osm_multilines : NULL\n    $osm_multipolygons : NULL\n\n\n\n\nlibrary(osmdata)\n\nnottingham_buildings &lt;- wards |&gt; \n  st_transform(\"EPSG:4326\") |&gt; \n  st_bbox() |&gt; \n  opq() |&gt; \n  add_osm_feature(key = \"building\") |&gt; \n  osmdata_sf()\n\nnottingham_buildings\n\nObject of class 'osmdata' with:\n                 $bbox : 52.9173362670705,-1.21712477980417,52.9596833099217,-1.13020878819132\n        $overpass_call : The call submitted to the overpass API\n                 $meta : metadata including timestamp and version numbers\n           $osm_points : 'sf' Simple Features Collection with 164398 points\n            $osm_lines : 'sf' Simple Features Collection with 44 linestrings\n         $osm_polygons : 'sf' Simple Features Collection with 30465 polygons\n       $osm_multilines : 'sf' Simple Features Collection with 2 multilinestrings\n    $osm_multipolygons : 'sf' Simple Features Collection with 69 multipolygons\n\n\nLooking at the nottingham_buildings object, we can see that OSM contains data on buildings stored as points, polygons and multipolygons (we can ignore the few linestrings tagged as buildings, since it doesn’t make sense for a building to be represented as a single line rather than a point or a polygon).\n\n\nWhat is a multipolygon?\n\n\nOpenStreetMap stores features in several different ways. The most basic types are points, lines and polygons. But there are also multipolygons (and multilines). These are features that represent complex structures such as clusters of buildings that are separate structures but are related to each other. For example, a hospital with several buildings might be represented in OpenStreetMap as a single multipolygon feature.\n\n\n\nLet’s plot these features on a base map to check that OSM has reasonable coverage of the buildings in these three wards.\n\nggplot() +\n  annotation_map_tile(type = \"cartodark\", zoomin = 0, progress = \"none\") +\n  # Add building features stored as points\n  geom_sf(\n    data = pluck(nottingham_buildings, \"osm_points\"), \n    colour = \"green\",\n    size = 0.1\n  ) +\n  # Add building features stored as polygons\n  geom_sf(\n    data = pluck(nottingham_buildings, \"osm_polygons\"), \n    colour = NA,\n    fill = \"blue\"\n  ) + \n  # Add building features stored as multi-polygons\n  geom_sf(\n    data = pluck(nottingham_buildings, \"osm_multipolygons\"), \n    colour = NA,\n    fill = \"darkred\"\n  ) +\n  geom_sf(data = wards, colour = \"red\", fill = NA, linewidth = 1.25) +\n  theme_void()\n\n\n\n\n\n\n\n\n\nRemember that osmdata_sf() gets OSM data for the area covered by the bounding box of the input feature, not the feature boundaries. This means some of the buildings returned by the code above will be outside the wards we are interested in. We will deal with this in a minute.\n\nIt looks like almost all the streets in the three wards we are interested in are lined with buildings in the OSM data, which is what we would expect of streets in an urban area. There are some streets without buildings in the top-left of the map, but these streets are outside our three wards so this does not matter.\nWe can also see from this map that the 164,398 point features in the OSM data (shown as green dots on the map) typically represent the corners of buildings that are also represented as polygons, so we know we can ignore the points layer within the OSM data.\nSince the hotspot_dual_kde() function works on points, we need to convert the polygon and multipolygon layers to points by calculating their centroids, then merge the two layers together. This will generate a warning that st_centroid does not give correct centroids for longitude/latitude data but we can ignore this because the calculated centroids will be good enough for our purposes (if we wanted to, we could transform the data to use the British National Grid, calculate the centroids and then transform it back).\nSince we are only interested in those buildings in three particular wards, we can also at this stage remove any buildings that are outside those wards using st_intersection(), as we have already done for the burglaries object. Since the wards object uses the British National Grid and st_intersection() requires both datasets to use the same co-ordinate system, we will transform the building centroids before clipping them.\n\n\nWhat does the warning st_centroid assumes … mean?\n\n\nYou might have seen a warning saying st_centroid assumes attributes are constant over geometries of x. You will see this warning when you use the st_centroid() function. It is there to remind you that columns in the original data (which the SF package refers to as the attributes associated with each spatial feature) refer to the polygon as a whole, but in the object produced by st_centroid() it will appear that the columns relate to the centroid point. In many cases this will not be a problem, but it could expose you to the ecological fallacy so it is sometimes useful to be reminded.\n\n\n\n\n\nWhat does the warning attribute variables are assumed … mean?\n\n\nst_intersection() produces a warning message whenever it is used:\nWarning: attribute variables are assumed to be spatially constant throughout all\ngeometries\nAs long as you are simply using st_intersection() to remove parts of the data outside a boundary, you can ignore this message.\n\n\n\n\n\n11.2.2 Calculating dual kernel density\nWe now have the object burglaries that contains the locations of each burglary in the three Nottingham wards that we are interested in, and the object nottingham_building_centroids that contains the centroids of each building in those three wards. We can use these layers to estimate the density of burglaries and buildings, then combine these to estimate the density of burglary risk.\nhotspot_dual_kde() works in the same way as hotspot_kde(), except that it requires two datasets. In this case, that means one dataset of crime locations and one dataset of building locations. hotspot_dual_kde() will set the cell size and bandwidth automatically, but we can set them manually using the cell_size, bandwidth_adjust and grid arguments in the same way we have done for hotspot_kde(). In this case, we will use the hotspot_grid() helper function to create a grid based on the boundaries of the wards we are interested in. All the spatial objects we are going to use here have co-ordinates specified using the British National Grid because we have already transformed them, so we do not need to do any transformation here.\n\nburglary_risk &lt;- hotspot_dual_kde(\n  burglaries, \n  nottingham_building_centroids, \n  bandwidth_adjust = 0.25, \n  grid = hotspot_grid(wards, cell_size = 100),\n  quiet = TRUE\n) |&gt; \n  st_intersection(wards)\n\nWarning: attribute variables are assumed to be spatially constant throughout\nall geometries\n\nhead(burglary_risk)\n\nSimple feature collection with 6 features and 4 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 455826.6 ymin: 338782.9 xmax: 456354.2 ymax: 338909\nProjected CRS: OSGB36 / British National Grid\n# A tibble: 6 × 5\n      n    kde ward_code ward_name                                      geometry\n  &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;     &lt;chr&gt;                                     &lt;POLYGON [m]&gt;\n1     0 0.0175 E05012277 Castle    ((456061.5 338809, 456061.5 338799.8, 456006…\n2     0 0.0172 E05012277 Castle    ((456061.5 338809, 456161.5 338809, 456161.5…\n3     0 0.0222 E05012277 Castle    ((456161.5 338809, 456261.5 338809, 456261.5…\n4     0 0.0271 E05012277 Castle    ((456261.5 338809, 456354.2 338809, 456261.5…\n5     0 0.0204 E05012277 Castle    ((455861.5 338909, 455861.5 338888.8, 455826…\n6     0 0.0264 E05012277 Castle    ((455861.5 338909, 455961.5 338909, 455961.5…\n\n\nYou might recall from earlier in this tutorial that the value of the kde column in the object produced by hotspot_dual_kde() is calculated by dividing the density of burglary in each grid cell by the density of buildings in the same grid cell. There are two cases where this will produce a result that is not a finite number:\n\nIf, for a particular cell, the density of burglaries and density of buildings are both zero, dividing one by the other will produce the result NaN, for ‘not a number’.\nIf the density of burglaries is greater than zero but the density of buildings is exactly zero, the result will be Inf, for ‘infinite’.\n\nSince it is not possible to calculate burglary risk in either of those cases, we can exclude these cases from the burglary_risk object by using filter() together with the is.finite() function (R does not count NaN as a finite number):\nburglary_risk &lt;- hotspot_dual_kde(\n  burglaries, \n  nottingham_building_centroids, \n  bandwidth_adjust = 0.25, \n  grid = hotspot_grid(wards, cell_size = 100)\n) |&gt; \n  st_intersection(wards)\n\nburglary_risk_filtered &lt;- filter(burglary_risk, is.finite(kde))\nYou might wonder why we didn’t simply add filter() to the existing pipeline that creates the burglary_risk object. It is because we will need the unfiltered object in the next section. But for now …\nWe can plot the estimate of the density of burglary risk. By controlling for the density of buildings, this map shows us where building owners on average face the highest risk of being burgled. This might be useful in working out, for example, which building owners should be offered visits from a crime-prevention advisor or funding to install crime-prevention measures.\n\nggplot() +\n  annotation_map_tile(type = \"cartolight\", zoomin = 0, progress = \"none\") +\n  # Add burglary risk layer\n  geom_sf(\n    aes(fill = kde), \n    data = burglary_risk_filtered, \n    alpha = 0.8, \n    colour = NA\n  ) +\n  # Add ward boundaries\n  geom_sf(data = wards, fill = NA) + \n  scale_fill_distiller(\n    breaks = range(pull(burglary_risk_filtered, \"kde\")),\n    labels = c(\"lower\", \"higher\"),\n    direction = 1\n  ) +\n  labs(\n      title = \"Burglary risk in south-west Nottingham\",\n      subtitle = str_glue(\n        \"dual kernel density of burglary risk in Castle, Lenton & Wollaton \",\n        \"East and Meadows wards\"\n      ),\n      caption = str_glue(\n        \"Contains public sector information licensed under the Open \",\n        \"Government Licence v3.0\"\n      ),\n      fill = \"density of burglary risk, 2020\"\n  ) +\n  theme_void() +\n  theme(\n    legend.position = \"bottom\",\n    plot.caption = element_text(colour = \"grey40\"),\n    plot.subtitle = element_text(margin = margin(t = 6, b = 6)),\n    plot.title = element_text(colour = \"grey50\", face = \"bold\", size = 16)\n  )\n\n\n\n\n\n\n\n\n\n\n11.2.3 Check your understanding\n\nquiz(\n  caption = \"\",\n\n  question(\"Which *one* of these statements is true?\",\n    answer(\"`hotspot_dual_kde()` requires co-ordinates to use a projected co-ordinate system (i.e. not longitude and latitude).\", correct = TRUE),\n    answer(\"`hotspot_dual_kde()` can work on any co-ordinates, whether they use a geographic co-ordinate system (i.e. longitude and latitude) or a projected system.\"),\n    answer(\"`hotspot_dual_kde()` requires co-ordinates to use a geographic co-ordinate system (i.e. longitude and latitude).\"),\n    answer(\"`hotspot_dual_kde()` does not work with co-ordinates, so it does not matter which type of co-ordinate system a dataset uses.\"),\n    allow_retry = TRUE,\n    random_answer_order = TRUE\n  ),\n  \n  question(\n    \"Why do we usually clip the result of `hotspot_dual_kde()` using `st_intersection()`?\",\n    answer(\"To eliminate any areas that we do not have data for, since displaying KDE values for such areas on a map might be misleading.\", correct = TRUE),\n    answer(\"To make our maps look nicer.\"),\n    answer(\"To transform the co-ordinate system our data uses from the system `hotspot_dual_kde()` uses to the one we need to produce a map.\"),\n    answer(\"To make it easier to see the other layers on our map.\"),\n    allow_retry = TRUE,\n    random_answer_order = TRUE\n  )\n)",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Mapping hotspots</span>"
    ]
  },
  {
    "objectID": "11_mapping_hotspots/index.html#finding-hotspots",
    "href": "11_mapping_hotspots/index.html#finding-hotspots",
    "title": "11  Mapping hotspots",
    "section": "11.3 Finding hotspots",
    "text": "11.3 Finding hotspots\nWe now know how to produce a better map of the density of crime in different areas. But how do we know which areas count as hotspots and which don’t?\nThere are several ways to answer this question. If we were planning a particular activity to respond to a crime problem, we might know what resources we had available to respond. For example, we might know that we have enough funding to provide crime-prevention visits to 100 locations. In that case, we can order the cells in a KDE object according to which have the highest estimates of risk, then count all the premises in each cell until we have reached our limit.\nTo do this, we need to know how many buildings are in each grid cell. We have already learned how to count crimes in areas when we learned about mapping area data. When we want to count crimes in each cell in a grid, we can use the hotspot_count() function from the sfhotspot package to count the number of buildings in each grid cell. We will then be able to combine those building counts to the estimates of burglary risk we have already calculated.\n\nSo that we can join the building counts to the burglary risk estimates, it is important that both layers are based on the same grid. In the code above we created a grid with the code hotspot_grid(wards, cell_size = 100), so to make sure we use the same grid to count buildings we can either use that same code again, or we could have saved the result of that code as an object (maybe called nottingham_wards_grid) and then provided that object to the grid argument of both hotspot_dual_kde() and hotspot_count().\nNote that the results of hotspot_dual_kde() and hotspot_count() will only have the same structure before we wrangle then any further, for example by using filter() as in the previous section. This is why we saved an unfiltered version of the dataset in the burglary_risk object.\n\n\nbuilding_counts &lt;- hotspot_count(\n  nottingham_building_centroids, \n  grid = hotspot_grid(wards, cell_size = 100)\n) |&gt; \n  # Clip the result to the area for which we have data\n  st_intersection(wards)\n\nWarning: attribute variables are assumed to be spatially constant throughout\nall geometries\n\nhead(building_counts)\n\nSimple feature collection with 6 features and 3 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 455826.6 ymin: 338782.9 xmax: 456354.2 ymax: 338909\nProjected CRS: OSGB36 / British National Grid\n# A tibble: 6 × 4\n      n ward_code ward_name                                             geometry\n  &lt;dbl&gt; &lt;chr&gt;     &lt;chr&gt;                                            &lt;POLYGON [m]&gt;\n1     0 E05012277 Castle    ((456061.5 338809, 456061.5 338799.8, 456006.6 3388…\n2     0 E05012277 Castle    ((456061.5 338809, 456161.5 338809, 456161.5 338790…\n3     1 E05012277 Castle    ((456161.5 338809, 456261.5 338809, 456261.5 338786…\n4     1 E05012277 Castle    ((456261.5 338809, 456354.2 338809, 456261.5 338786…\n5     2 E05012277 Castle    ((455861.5 338909, 455861.5 338888.8, 455826.6 3389…\n6     4 E05012277 Castle    ((455861.5 338909, 455961.5 338909, 455961.5 338830…\n\n\nThe building_counts and burglary_risk objects have the same structure: each row represents a cell in the same grid, and the rows are in the same order (because both grids were created by identical calls to hotspot_grid()). This means we can combine the two objects using the bind_cols() function from the dplyr package (part of the tidyverse).\n\nburglary_risk_bldg &lt;- bind_cols(burglary_risk, building_counts)\n\nNew names:\n• `n` -&gt; `n...1`\n• `ward_code` -&gt; `ward_code...3`\n• `ward_name` -&gt; `ward_name...4`\n• `geometry` -&gt; `geometry...5`\n• `n` -&gt; `n...6`\n• `ward_code` -&gt; `ward_code...7`\n• `ward_name` -&gt; `ward_name...8`\n• `geometry` -&gt; `geometry...9`\n\nhead(burglary_risk_bldg)\n\n# A tibble: 6 × 9\n  n...1    kde ward_code...3 ward_name...4                    geometry...5 n...6\n  &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;         &lt;chr&gt;                           &lt;POLYGON [m]&gt; &lt;dbl&gt;\n1     0 0.0175 E05012277     Castle        ((456061.5 338809, 456061.5 33…     0\n2     0 0.0172 E05012277     Castle        ((456061.5 338809, 456161.5 33…     0\n3     0 0.0222 E05012277     Castle        ((456161.5 338809, 456261.5 33…     1\n4     0 0.0271 E05012277     Castle        ((456261.5 338809, 456354.2 33…     1\n5     0 0.0204 E05012277     Castle        ((455861.5 338909, 455861.5 33…     2\n6     0 0.0264 E05012277     Castle        ((455861.5 338909, 455961.5 33…     4\n# ℹ 3 more variables: ward_code...7 &lt;chr&gt;, ward_name...8 &lt;chr&gt;,\n#   geometry...9 &lt;POLYGON [m]&gt;\n\n\nLooking at this object, we can see that bind_cols() has changed the column names. This is because some of the column names were duplicated across the two datasets. Some of these new column names, such as ward_code...3 would be difficult to work with, so lets go back and remove duplicate columns before we combine the two datasets together.\n\nburglary_risk_bldg &lt;- burglary_risk |&gt; \n  # Keep only the `n` and `kde` columns, renaming `n` to `burglary_count`\n  # because there is also a column called `n` in the `building_counts` object\n  select(burglary_count = n, kde) |&gt; \n  # Remove the geometry column (since an identical one exists in \n  # `building_counts`). Note that we do this separately because we can't use\n  # `select()` to remove the `geometry` column.\n  st_drop_geometry() |&gt; \n  bind_cols(building_counts) |&gt; \n  # After calling `bind_cols()` it is safe to use `filter()` to wrangle the\n  # data\n  filter(is.finite(kde))\n\nWe can now order the dataset with the cells with highest burglary risk at the top and calculate the cumulative total (also called a running total) of buildings using the cumsum() function. We can use this running total to find the 100 buildings in the cells with highest burglary risk.\nWe could now plot the cells_for_prevention object on a map to show the grid cells containing the buildings that would receive the crime-prevention visits. We could also use st_join() to join those cells to the dataset of buildings in the nottingham_building_centroids object, which would give us a list of buildings to visit.\n\n11.3.1 Distinguishing hotspots from random variation\nThe density maps below show density estimates based on 1,000 points placed completely at random on 16 different maps. There are no real patterns in the data except for statistical noise. Nevertheless, the KDE process makes it appear that there are patterns in the data (if you reload this tutorial, these maps will change appearance completely, since the random numbers used for the x and y co-ordinates of the points will be regenerated).\n\ntibble(\n  x = runif(n = 1000 * 16, min = 0, max = 100), \n  y = runif(n = 1000 * 16, min = 0, max = 100),\n  group = rep(1:16, each = 1000)\n)  |&gt;  \n  ggplot() + \n  geom_density_2d_filled(aes(x, y), bins = 9) + \n  scale_fill_brewer(\n    labels = c(\"lower\", rep(\"\", 7), \"higher\"),\n    palette = \"Oranges\", \n    direction = 1,\n    guide = guide_legend(reverse = TRUE)\n  ) +\n  facet_wrap(vars(group), ncol = 4) +\n  coord_fixed() +\n  labs(fill = \"density\") +\n  theme_void() +\n  theme(strip.text = element_blank())\n\n\n\n\n\n\n\n\nThis is a problem because we might end up responding to an apparent problem that is nothing but an artefact of the random variation that we expect to see in many processes, including crime.\nIf the police and other agencies looked at these patterns and did nothing in response to them, it is likely that over time some of the areas with high density would become areas of low density, and vice versa. However, the harm caused by crime means it is very hard for agencies to justify sitting back and do nothing to respond to it – many people would consider it immoral to do so. So police and other agencies are very likely to try to respond to crime patterns, even if those patterns might have occurred by chance. This is very frustrating, because if we were to go back and look at the same data in a few months time it is very likely that the apparent hotspots would have shifted to somewhere different, making all the effort spent in responding to crime seem worthless (which, if the apparent patterns were actually artefacts of the KDE process, it may have been).\nYou might be thinking it’s better safe than sorry, and that police should respond to the apparent patterns just in case they represent real concentrations in crime. But police resources are always scarce, so responding to one problem in one place means not responding to another problem in another place. This is known as the opportunity cost of acting: if police focus their limited resources in one area, that comes at the cost of not being able to deploy those resources in other areas that might need it more.\nWe can try to avoid this problem of wasting resources responding to random variation in crime by determining whether the number of crimes in an area is more than the greatest number we would reasonably expect if there were no actual patterns in the data (if you have studied statistics before, you might recognise this as a description of a null hypothesis, but you don’t need to have studied statistics to apply the techniques in this course).\nTo determine if the number of crimes in each area is greater than we would expect by chance, we can use the Getis-Ord Gi* statistic (also called the local G statistic, spoken out-loud as the G-I star statistic). If the Gi* statistic for an area is greater than a certain value, we can say that the number of crimes in that area is higher than we would expect if there were no patterns in the data. We will call areas with more crimes than we would expect by chance as hotspots.\nTo see how this is done, watch this video that walks through the code needed to make a hotspot map using the Gi* statistic.\n\nWe can calculate the Gi* statistic using the hotspot_gistar() function from the sfhotspot package. This works in a similar way to the hotspot_kde() function, in that it takes an SF object of the locations of crimes and returns an SF object with a grid of cells, along with the Gi* value for each grid cell. Like hotspot_kde(), hotspot_gistar() will choose default values for several ways in which we could fine-tune the calculation of the Gi* statistic, but we could over-ride these defaults if we wanted to.\nIn this example, we will find the hotspots of robbery in Nottingham in 2020, based on a grid of 100-metre cells. We will store this in an object called robbery, transform it to use the British National Grid co-ordinate system (so we can specify the cell size in metres) and then use the resulting object to calculate the Gi* values.\n\nrobbery &lt;- read_csv(\"https://mpjashby.github.io/crimemappingdata/nottingham_robbery.csv.gz\") |&gt; \n  st_as_sf(coords = c(\"longitude\", \"latitude\"), crs = 4326) |&gt; \n  st_transform(27700)\n\nRows: 555 Columns: 5\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (2): location, lsoa_code\ndbl  (2): longitude, latitude\ndate (1): month\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nrobbery_gistar &lt;- hotspot_gistar(robbery, cell_size = 100, quiet = TRUE)\n\n# Use the `sample_n()` function from the `dplyr` package to return a random\n# sample of 10 rows from the result\nsample_n(robbery_gistar, 10)\n\nSimple feature collection with 10 features and 4 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 451367.5 ymin: 334874.1 xmax: 456867.5 ymax: 345774.1\nProjected CRS: OSGB36 / British National Grid\n# A tibble: 10 × 5\n       n   kde gistar pvalue                                            geometry\n * &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;                                       &lt;POLYGON [m]&gt;\n 1     0 32.9  -0.498  0.618 ((454667.5 344874.1, 454667.5 344974.1, 454767.5 3…\n 2     0 23.3   0.331  0.741 ((455267.5 339074.1, 455267.5 339174.1, 455367.5 3…\n 3     0  7.62 -0.498  0.618 ((455967.5 337774.1, 455967.5 337874.1, 456067.5 3…\n 4     0 14.7  -0.498  0.618 ((453067.5 344174.1, 453067.5 344274.1, 453167.5 3…\n 5     0  4.44 -0.407  0.684 ((452367.5 344774.1, 452367.5 344874.1, 452467.5 3…\n 6     0  4.71 -0.498  0.618 ((454567.5 335874.1, 454567.5 335974.1, 454667.5 3…\n 7     0 19.4  -0.498  0.618 ((454867.5 334874.1, 454867.5 334974.1, 454967.5 3…\n 8     0  5.66 -0.498  0.618 ((451367.5 339674.1, 451367.5 339774.1, 451467.5 3…\n 9     1 12.0   0.331  0.741 ((456767.5 345674.1, 456767.5 345774.1, 456867.5 3…\n10     0 15.8   0.331  0.741 ((452667.5 343074.1, 452667.5 343174.1, 452767.5 3…\n\n\nThe robbery_gistar object contains one row for each cell in a grid of cells covering the area of the robbery data. Each row has four columns:\n\nn shows the number of robberies that occurred in that grid cell,\nkde shows the density of robberies in that cell,\ngistar shows the Gi* value for that cell, and\npvalue shows the \\(p\\)-value for that cell.\n\nThe Gi* statistic is an example of a more general group of statistics called \\(Z\\) scores. Statisticians and data analysts compare the \\(Z\\) scores produced by statistical procedures such as hotspot_gistar() to reference values to decide if a \\(Z\\) score is large enough to be treated as statistically significant, i.e.  if it is large enough to conclude that it is larger than we would expect if there were no actual patterns in the data. Deciding on the right reference value to compare a \\(Z\\) score to can be difficult because of what’s known as the multiple comparison problem (which we don’t need to go into detail about). Fortunately, the values in the pvalue column have already been automatically adjusted to take account of the multiple comparison problem, so we can interpret the \\(p\\)-values instead of interpreting the Gi* statistic directly.\n\nSince Gi* is a relative measure, if you have data for a large area (e.g. a country) but only want to show data for a smaller area (e.g. a city), the Gi* values will be influenced by the large areas with no crime and all of the city is likely to be identified as a hotspot. To prevent this, it is important to clip the dataset before calculating the Gi* values, as well as then clipping afterwards where necessary.\n\nBy convention, \\(p\\)-values are considered to be significant if they are less than 0.05. So if \\(p&lt;0.05\\), we can say that the number of robberies occurring in a given grid cell is significantly different from zero. Values of Gi* greater than zero indicate cells with more robberies than expected and values of Gi* less than zero indicate cells with fewer robberies than expected. We can combine these two values to find cells with significantly more robberies than expected by chance, which are those cells for which \\(Z&gt;0\\) and \\(p&lt;0.05\\). To put that into R code, we would write gistar &gt; 0 and p &lt; 0.05.\nWe could use this information in various ways. For example, if we wanted to give local police officers a printed map of which areas to patrol, we could simply show the significant hotspot cells over a base map.\n\n# Plot a map\nggplot() +\n  annotation_map_tile(type = \"cartolight\", zoomin = 0, progress = \"none\") +\n  geom_sf(\n    data = filter(robbery_gistar, gistar &gt; 0, pvalue &lt; 0.05), \n    fill = \"red\", \n    alpha = 0.75,\n    colour = NA\n  ) +\n  fixed_plot_aspect() +\n  theme_void()\n\n\n\n\n\n\n\n\nSince hotspot_gistar() also estimates density for each grid cell, we could more usefully show the density of robberies in each cell, but only for cells that the Gi* values showed were significant hotspots.\n\nggplot() +\n  annotation_map_tile(type = \"cartolight\", zoomin = 0, progress = \"none\") +\n  geom_sf(\n    aes(fill = kde),\n    data = filter(robbery_gistar, gistar &gt; 0, pvalue &lt; 0.05), \n    alpha = 0.75,\n    colour = NA\n  ) +\n  scale_fill_distiller(direction = 1) + \n  fixed_plot_aspect() +\n  theme_void()\n\n\n\n\n\n\n\n\nThis map could be very useful for police officers deciding where to conduct anti-robbery patrols, because it not only shows the areas with the highest density of robberies but only shows those areas if there are more robberies than we would expect by chance. This makes it more likely that officers won’t waste time chasing apparent patterns that are actually the result of random variation.\n\n\n11.3.2 Check your understanding\n\nquiz(\n  caption = \"\",\n\n  question(\"`robbery_gi` is an object storing a result produced by the `hotspot_gistar()` function. Which of these pieces of code could be used to extract _only_ those rows in the data with significant p-values?\",\n    answer(\"`filter(robbery_gi, pvalue &lt; 0.05)`\", correct = TRUE),\n    answer(\n      \"`filter(robbery_gi, pvalue &gt; 0.05)`\",\n      message = \"That code would extract only _non-significant_ p-values. Try again!\"\n    ),\n    answer(\n      \"`filter(robbery_gi, pvalue &lt;= 0.05)`\",\n      message = \"Almost right, but not quite. Try re-reading the section of the tutorial above.\"\n    ),\n    answer(\n      \"`filter(robbery_gi, pvalue == 0.05)`\",\n      message = \"That's not right. Try re-reading the section of the tutorial above.\"\n    ),\n    allow_retry = TRUE,\n    random_answer_order = TRUE\n  ),\n  \n  question(\"Which *one* of these statements is true about an SF object called `robberies_in_nottingham`?\",\n    answer(\"We cannot remove the `geometry` column of an SF object with `select()`, so we must use `st_drop_geometry()` instead.\", correct = TRUE),\n    answer(\"We can remove the `geometry` column with the code `select(robberies_in_nottingham, -geometry)`.\"),\n    answer(\"We can remove the `geometry` column with the code `filter(robberies_in_nottingham, -geometry)`.\"),\n    answer(\"There is no way to remove the `geometry` column from an SF object.\"),\n    allow_retry = TRUE,\n    random_answer_order = TRUE\n  )\n)",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Mapping hotspots</span>"
    ]
  },
  {
    "objectID": "11_mapping_hotspots/index.html#putting-it-all-together",
    "href": "11_mapping_hotspots/index.html#putting-it-all-together",
    "title": "11  Mapping hotspots",
    "section": "11.4 Putting it all together",
    "text": "11.4 Putting it all together\n\nIn this tutorial we have learned about hotspots, how to create dual KDE maps and how to find significant hotspots using the Gi* statistic. We can put this all together to create a complete script for producing a map of robbery hotspots in Nottingham\n\nThe following code is all that is needed to produce this map. Read through the comments accompanying the code to see how what we have learned in this tutorial fits together, then run the code to produce the map.\n\nIt is possible that this code will not run in this tutorial window because of limits on how long code can run in an R tutorial. If that happens, paste the code below into a blank R script in RStudio and run it from there to see the map.\n\n\n# Prepare ----------------------------------------------------------------------\n\n# Load packages\nlibrary(ggspatial)\nlibrary(sf)\nlibrary(sfhotspot)\nlibrary(tidyverse)\n\n# Load data and transform to British National Grid, which is easier to work with\n# for functions that use spatial units such as metres\nrobbery &lt;- read_csv(\"https://mpjashby.github.io/crimemappingdata/nottingham_robbery.csv.gz\") |&gt; \n  st_as_sf(coords = c(\"longitude\", \"latitude\"), crs = \"EPSG:4326\") |&gt; \n  st_transform(\"EPSG:27700\")\nnottingham_wards &lt;- read_sf(\"https://mpjashby.github.io/crimemappingdata/nottingham_wards.gpkg\") |&gt; \n  st_transform(\"EPSG:27700\")\n\n\n# Find significant grid cells --------------------------------------------------\n\n# Calculate Gi* statistic, filter for only significant hotspot cells and clip to\n# the city boundary\nrobbery_gi &lt;- robbery |&gt; \n  hotspot_gistar(cell_size = 100, bandwidth_adjust = 0.25, quiet = TRUE) |&gt; \n  filter(gistar &gt; 0, pvalue &lt; 0.05) |&gt; \n  st_intersection(nottingham_wards)\n\nWarning: attribute variables are assumed to be spatially constant throughout\nall geometries\n\n# Plot map ---------------------------------------------------------------------\n\nggplot() + \n  annotation_map_tile(type = \"cartolight\", zoomin = 0, progress = \"none\") +\n  # Add density for significant cells\n  geom_sf(\n    aes(fill = kde), \n    data = robbery_gi, \n    alpha = 0.8,\n    colour = NA\n  ) +\n  # Add ward boundaries\n  geom_sf(data = nottingham_wards, colour = \"grey70\", fill = NA) +\n  scale_fill_distiller(\n    breaks = range(pull(robbery_gi, kde)),\n    labels = c(\"lower\", \"higher\"),\n    direction = 1\n  ) +\n  fixed_plot_aspect() +\n  labs(\n      title = \"Nottingham robbery hotspots\",\n      subtitle = str_glue(\n        \"density of robbery in places with more violence than expected by \",\n        \"chance\"\n      ),\n      # Don't forget to add the licence statement -- it's a legal requirement!\n      caption = str_glue(\n        \"Contains public sector information licensed under the Open \",\n        \"Government Licence v3.0. Map data from OpenStreetMap.\"\n      ),\n      fill = str_wrap(\"density of robbery at significant hotspots, 2022\", 15)\n  ) +\n  theme_void() +\n  theme(\n    plot.caption = element_text(colour = \"grey40\", hjust = 0),\n    plot.subtitle = element_text(margin = margin(t = 6, b = 6)),\n    plot.title = element_text(colour = \"grey50\", face = \"bold\", size = 16)\n  )",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Mapping hotspots</span>"
    ]
  },
  {
    "objectID": "12_messy_data/index.html",
    "href": "12_messy_data/index.html",
    "title": "12  Handling messy data",
    "section": "",
    "text": "12.1 Introduction\nLearn how to tidy messy data so that it is easier to use for data analysis.\nData is the foundation of everything we do in crime mapping. To make a crime map needs data on the locations and types of crimes; data on roads, buildings and natural features to make up a base map, and data on other features (such as particular types of facility) that might be relevant to why crime happens in particular place.\nUntil now, all the data we have used has been tidy data. Data is tidy if it comes in a particular format where every variable (e.g. the date on which a crime occurred) is stored in a separate column and data for every observation (e.g. all the data about a particular crime, a particular offender etc.) is stored in a separate row. For typical crime data, that means each crime is represented by one row in the data and each thing that we know about that crime is stored in a separate column.\nUnfortunately, not all the data we might like to use in crime mapping is available in a tidy format. The people, organisations and systems and produce data store it in many formats, which are often not tidy and not easy to analyse. Messy data is more difficult to work with because every messy dataset has a unique structure, which we have to remember every time we want to work with it.\nTidy data, on the other hand, is easier to work with because its format is familiar and consistent. Many R functions are also designed to work with tidy data, so tidying our datasets often makes analysis quicker, too.\nThe first step to analysing messy data is therefore to wrangle it into a tidy format. We have already seen this principle at work when we use the clean_names() function from the janitor package to convert column names into a consistent format so that we don’t have to keep remembering which unique format the column names are in.\nIn this tutorial we will learn how to tidy messy data to make it easier to work with.\nStats Illustrations by Allison Horst licensed under the Creative Commons Attribution licence.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Handling messy data</span>"
    ]
  },
  {
    "objectID": "12_messy_data/index.html#tidying-the-structure-of-data",
    "href": "12_messy_data/index.html#tidying-the-structure-of-data",
    "title": "12  Handling messy data",
    "section": "12.2 Tidying the structure of data",
    "text": "12.2 Tidying the structure of data\n\n\n\nEvery messy dataset is messy in its own unique way, but often messiness comes from the structure of data not being tidy. Tabular data can come in two general formats: long and wide. For example, imagine a dataset showing counts of several different types of crime in several different areas. We could store this data in wide format, where there is a column for the name of the area and then a column for each type of crime.\n\n\n\n\n\narea\nassault\nrobbery\nburglary\n\n\n\n\nNorthville\n4\n2\n10\n\n\nMiddletown\n6\n5\n20\n\n\nSoutham\n10\n0\n10\n\n\n\n\n\n\n\nWide-format data are often useful for presentation – you might often see a table like this in a report or article. But wide data are less useful for analysis because one of the variables – ‘type of crime’ – is being stored not as a variable but in the various column names. One sign that your data has this problem is if several of the columns form a group of columns, separate from the others. In this case, there is a group of columns that show crime counts that is different from the other column that shows the area name.\nFortunately, we can easily convert this data (which is stored in the object crime_counts) into long format using the pivot_longer() function from the tidyr package. To use pivot_longer(), we have to specify in the cols argument which columns we want to ‘gather’ together into one column to store the category names and one column to store the values. We can specify the columns we want to gather as a vector, i.e. c(assault, burglary, robbery).\n\nlibrary(tidyverse)\n\npivot_longer(crime_counts, cols = c(assault, burglary, robbery))\n\n# A tibble: 9 × 3\n  area       name     value\n  &lt;chr&gt;      &lt;chr&gt;    &lt;dbl&gt;\n1 Northville assault      4\n2 Northville burglary    10\n3 Northville robbery      2\n4 Middletown assault      6\n5 Middletown burglary    20\n6 Middletown robbery      5\n7 Southam    assault     10\n8 Southam    burglary    10\n9 Southam    robbery      0\n\n\nBy default, pivot_longer() calls the new column of category names name and the new column of values value. We can specify more-descriptive names using the names_to and values_to arguments.\n\npivot_longer(\n  crime_counts, \n  cols = c(assault, burglary, robbery), \n  names_to = \"type\", \n  values_to = \"count\"\n)\n\n# A tibble: 9 × 3\n  area       type     count\n  &lt;chr&gt;      &lt;chr&gt;    &lt;dbl&gt;\n1 Northville assault      4\n2 Northville burglary    10\n3 Northville robbery      2\n4 Middletown assault      6\n5 Middletown burglary    20\n6 Middletown robbery      5\n7 Southam    assault     10\n8 Southam    burglary    10\n9 Southam    robbery      0\n\n\nIt is better to have the names of different categories stored as a single variable rather than as the names of several variables because they are easier to work with that way. For example, if the data are in long format you could sort the categories alphabetically using arrange(crime_counts, type), or you could transform the names to title case using mutate(crime_counts, type = str_to_title(type)). Both these operations would be harder to do if the data were in wide format.\nA common reason for data to be stored in wide format is where repeated observations are made of some value over time. For example, we might have monthly counts of crimes for different areas.\n\n\n\n\n\narea\njan_2020\nfeb_2020\nmar_2020\napr_2020\nmay_2020\n\n\n\n\nNorthville\n13\n10\n12\n9\n10\n\n\nMiddletown\n21\n19\n22\n19\n20\n\n\nSoutham\n15\n13\n16\n15\n14\n\n\n\n\n\n\n\nStoring data in this way is particularly awkward because variable names can only contain text, so R does not know that the column names represent dates. This means, for example, that we could not filter the dataset to only include data from after a certain date.\nWhen we want to gather a large number of columns together, it can get tedious to type all the column names for the cols argument to pivot_longer(). Instead, since we want to gather all the columns except one, we can just specify that we should not gather the area column, which implicitly tells pivot_longer() to gather all the other columns. We tell pivot_longer() not to gather the area column by specifying cols = -area (note the minus sign in front of the column name).\n\nRun the code needed to gather a dataset called monthly_counts so that the names of the monthly counts are stored in a variable called month and the monthly counts themselves are stored in a variable called count.\n\n\npivot_longer(\n  monthly_counts, \n  cols = -area, \n  names_to = \"month\", \n  values_to = \"count\"\n)\n\n# A tibble: 15 × 3\n   area       month    count\n   &lt;chr&gt;      &lt;chr&gt;    &lt;dbl&gt;\n 1 Northville jan_2020    13\n 2 Northville feb_2020    10\n 3 Northville mar_2020    12\n 4 Northville apr_2020     9\n 5 Northville may_2020    10\n 6 Middletown jan_2020    21\n 7 Middletown feb_2020    19\n 8 Middletown mar_2020    22\n 9 Middletown apr_2020    19\n10 Middletown may_2020    20\n11 Southam    jan_2020    15\n12 Southam    feb_2020    13\n13 Southam    mar_2020    16\n14 Southam    apr_2020    15\n15 Southam    may_2020    14\n\n\nThere is still a problem with this data, which is that the month variable contains not date values but instead the month and year stored as text. We will learn how to deal with this issue in a future tutorial.\n\n12.2.1 Skipping unwanted rows in imported data\nData released by government organisations are often designed to be viewed by humans rather than processed by statistical software. We can see this in this screen shot of a dataset produced by the UK Office for National Statistics on cybercrime in the UK based on responses to the Crime Survey for England and Wales.\n\n\n\nLooking at the row numbers on the left-hand side, we can see that the that the first row is taken up not with the column names (as in a tidy dataset) but with the title of the dataset. The next row is blank, and the third row then contains some metadata to say that the data relates to England and Wales and to adults aged 16 and over. Only on row four do we see the column names. There are also blank rows within the data that are used to separate out different categories of data.\nIf we try to import this data using, for example, the read_excel() function from the readxl package, we will find various problems.\nlibrary(readxl)\n\n# Since `.xlsx` files are binary files, we need to add `mode = \"wb\"` on Windows.\n# On other platforms it makes no difference, but adding `mode = \"wb\" does not\n# cause any problems.\ndownload.file(\n  url = \"https://www.ons.gov.uk/file?uri=/peoplepopulationandcommunity/crimeandjustice/datasets/crimeinenglandandwalesexperimentaltables/yearendingdecember2018/additionalfraudandcybercrimetablesyearendingdecember2018correction.xlsx\",\n  destfile = str_glue(\"{tempdir()}/cybercrime.xlsx\"),\n  mode = \"wb\"\n)\n\nread_excel(str_glue(\"{tempdir()}/cybercrime.xlsx\"), sheet = \"Table E1\")\n\n\nNew names:\n• `` -&gt; `...2`\n• `` -&gt; `...3`\n• `` -&gt; `...4`\n• `` -&gt; `...5`\n\n\n# A tibble: 46 × 5\n   Table E1:  Fraud and computer misuse by loss (of mo…¹ ...2  ...3  ...4  ...5 \n   &lt;chr&gt;                                                 &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;\n 1 &lt;NA&gt;                                                  &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt; \n 2 England and Wales                                     &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;  Adul…\n 3 Offence group3                                        Numb… Rate… Numb… Perc…\n 4 &lt;NA&gt;                                                  &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt; \n 5 FRAUD5                                                3648  78.0… 3078  6.58…\n 6 &lt;NA&gt;                                                  &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt; \n 7 With loss, no or only partial reimbursement           660   14.1… 613   1.31…\n 8 With loss, fully reimbursed                           2066  44.2… 1765  3.77…\n 9 Without loss                                          922   19.7… 804   1.72…\n10 &lt;NA&gt;                                                  &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt; \n# ℹ 36 more rows\n# ℹ abbreviated name:\n#   ¹​`Table E1:  Fraud and computer misuse by loss (of money or property) - number and rate of incidents and number and percentage of victims, year ending December 2018 CSEW1,2`\n\n\nIt’s clear that this dataset has not loaded in the way that we want, because the first row doesn’t contain the column names. Fortunately, we can deal with this problem using the skip argument to the read_excel() function. This allows us to specify a number of rows to ignore at the start of the dataset. In this case, we want to ignore the first three rows of the data (the table title, a blank line and the the metadata line), so we can specify skip = 3. The same skip argument also exists in the read_csv() function for reading CSV data and the read_tsv() function for reading tab-separated data.\nread_excel(\n  str_glue(\"{tempdir()}/cybercrime.xlsx\"), \n  sheet = \"Table E1\", \n  skip = 3\n)\n\n\n# A tibble: 43 × 5\n   `Offence group3`                Number of incidents …¹ Rate per 1,000 adult…²\n   &lt;chr&gt;                                            &lt;dbl&gt;                  &lt;dbl&gt;\n 1 &lt;NA&gt;                                                NA                  NA   \n 2 FRAUD5                                            3648                  78.0 \n 3 &lt;NA&gt;                                                NA                  NA   \n 4 With loss, no or only partial …                    660                  14.1 \n 5 With loss, fully reimbursed                       2066                  44.2 \n 6 Without loss                                       922                  19.7 \n 7 &lt;NA&gt;                                                NA                  NA   \n 8 Bank and credit account fraud                     2433                  52.0 \n 9 With loss, no or only partial …                    235                   5.03\n10 With loss, fully reimbursed                       1727                  36.9 \n# ℹ 33 more rows\n# ℹ abbreviated names: ¹​`Number of incidents (thousands)`,\n#   ²​`Rate per 1,000 adults`\n# ℹ 2 more variables: `Number of victims (thousands)4` &lt;dbl&gt;,\n#   `Percentage victims once or more4` &lt;dbl&gt;\n\n\nThis has dealt with the problem caused by the extra rows at the top of the data. But if we look at the bottom of the dataset, we will see that there are also several rows of footnotes. These footnotes are important for us to have read so that we understand the data, but they aren’t part of the data itself.\n\n\n\nWe can remove these extra rows at the end of our data using the slice() function from the dplyr package. slice() allows us to choose certain rows from our data by row number. Looking at the screen shot above, our data finishes on row 34 (row 36 looks like part of our data but the value there actually shows the number of people involved in the survey, not a number of incidents). But:\n\nwe have already removed the first three rows using the skip argument to read_excel(), and\nrow four of the original spreadsheet has become our column names,\n\nso row 34 on the spreadsheet is actually row 30 in our loaded dataset. Knowing this, we can remove all the rows below row 30 using slice():\nstr_glue(\"{tempdir()}/cybercrime.xlsx\") |&gt; \n  read_excel(sheet = \"Table E1\", skip = 3) |&gt; \n  slice(1:30)\n\n\n# A tibble: 30 × 5\n   `Offence group3`                Number of incidents …¹ Rate per 1,000 adult…²\n   &lt;chr&gt;                                            &lt;dbl&gt;                  &lt;dbl&gt;\n 1 &lt;NA&gt;                                                NA                  NA   \n 2 FRAUD5                                            3648                  78.0 \n 3 &lt;NA&gt;                                                NA                  NA   \n 4 With loss, no or only partial …                    660                  14.1 \n 5 With loss, fully reimbursed                       2066                  44.2 \n 6 Without loss                                       922                  19.7 \n 7 &lt;NA&gt;                                                NA                  NA   \n 8 Bank and credit account fraud                     2433                  52.0 \n 9 With loss, no or only partial …                    235                   5.03\n10 With loss, fully reimbursed                       1727                  36.9 \n# ℹ 20 more rows\n# ℹ abbreviated names: ¹​`Number of incidents (thousands)`,\n#   ²​`Rate per 1,000 adults`\n# ℹ 2 more variables: `Number of victims (thousands)4` &lt;dbl&gt;,\n#   `Percentage victims once or more4` &lt;dbl&gt;\n\n\nThe final problem with the structure of this dataset is the blank rows that are used to separate different crime categories in the original table. We can deal with this using the remove_empty() function from the janitor package, which removes all rows and/or columns that contain only NA values. We will also clean the column names at the same time and then rename the columns to be shorter, which will make it easier to refer to them in our code.\nlibrary(janitor)\n\nstr_glue(\"{tempdir()}/cybercrime.xlsx\") |&gt; \n  read_excel(sheet = \"Table E1\", skip = 3) |&gt; \n  slice(1:30) |&gt; \n  remove_empty(which = \"rows\") |&gt; \n  clean_names() |&gt; \n  rename(\n    offence = offence_group3, \n    crimes = number_of_incidents_thousands, \n    incidence = rate_per_1_000_adults, \n    victims = number_of_victims_thousands_4, \n    prevalence = percentage_victims_once_or_more4\n  )\n\n\n# A tibble: 22 × 5\n   offence                                   crimes incidence victims prevalence\n   &lt;chr&gt;                                      &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;      &lt;dbl&gt;\n 1 FRAUD5                                      3648     78.0     3078      6.58 \n 2 With loss, no or only partial reimbursem…    660     14.1      613      1.31 \n 3 With loss, fully reimbursed                 2066     44.2     1765      3.78 \n 4 Without loss                                 922     19.7      804      1.72 \n 5 Bank and credit account fraud               2433     52.0     2056      4.40 \n 6 With loss, no or only partial reimbursem…    235      5.03     211      0.451\n 7 With loss, fully reimbursed                 1727     36.9     1458      3.12 \n 8 Without loss                                 470     10.1      419      0.896\n 9 Consumer and retail fraud6                  1031     22.1      954      2.04 \n10 With loss, no or only partial reimbursem…    380      8.13     361      0.771\n# ℹ 12 more rows\n\n\nThis dataset now has a tidy structure: each row represents an observation (in this case, data about a particular type of crime), each column represents a piece of information about the observation and each cell represents a single value. We still need to clean up the footnote numbers that appear in some of the values, but we will deal with that later in this tutorial.\n\n\n12.2.2 Separating multiple variables stored in a single column\nSometimes datasets include multiple variables in a single column. For example, data about crime victims stored in an object called victims might include a single column representing both age and sex.\n\n\n\n\n\nfirst_name\nlast_name\nage_sex\n\n\n\n\nLyda\nGartrell\n40/F\n\n\nKareem\nDavid\n26\n\n\nLisa\nDean\n21/F\n\n\nMelina\nShehan\n25/F\n\n\nAlfredo\nMatamoros\n18/M\n\n\n\n\n\n\n\nIt would be easier to wrangle this data (e.g. to filter by age) if the data in the age_sex column was stored in two separate columns. Making this change would also make sure our data meets the definition of being tidy: every variable should be stored in a separate column.\nWe can split the age_sex column into two using the separate() function from the tidyr package. We specify the existing column we want to split using the col argument, the names of the new columns we want to create using the into argument and the character(s) that represent the boundary between the two pieces of data in each column (in this case, /).\n\nseparate(victims, col = age_sex, into = c(\"age\", \"sex\"), sep = \"/\")\n\nWarning: Expected 2 pieces. Missing pieces filled with `NA` in 1 rows [2].\n\n\n# A tibble: 5 × 4\n  first_name last_name age   sex  \n  &lt;chr&gt;      &lt;chr&gt;     &lt;chr&gt; &lt;chr&gt;\n1 Lyda       Gartrell  40    F    \n2 Kareem     David     26    &lt;NA&gt; \n3 Lisa       Dean      21    F    \n4 Melina     Shehan    25    F    \n5 Alfredo    Matamoros 18    M    \n\n\nYou might have noticed that this function produced a warning message saying Expected 2 pieces. Missing pieces filled with NA in 1 rows [2]. This is because the second row of data in the victims object only contains one of the two pieces of data. When this happens, separate() fills in the values that are present from the left, filling any remaining columns to the right with NA. If this is what we want (as it is here), we can silence this warning by specifying fill = \"right\". If instead we wanted separate() to fill in columns with NA values from the left, we could specify fill = \"left\".\nAnother issue with the separated data is that because the column age_sex was a character column, both age and sex are character columns, too. Since age is actually numeric, we can get separate() to convert this new column to the correct type automatically by specifying convert = TRUE.\n\nseparate(\n  victims, \n  col = age_sex, \n  into = c(\"age\", \"sex\"), \n  sep = \"/\",\n  convert = TRUE,\n  fill = \"right\"\n)\n\n# A tibble: 5 × 4\n  first_name last_name   age sex  \n  &lt;chr&gt;      &lt;chr&gt;     &lt;int&gt; &lt;chr&gt;\n1 Lyda       Gartrell     40 F    \n2 Kareem     David        26 &lt;NA&gt; \n3 Lisa       Dean         21 F    \n4 Melina     Shehan       25 F    \n5 Alfredo    Matamoros    18 M    \n\n\nWe now know how to convert a data into a tidy format using pivot_longer() to convert data to long format, remove non-data rows using slice(), remove_empty() and the skip argument to many functions like read_csv(), and split columns with separate(). In the next section, we’ll learn how to tidy the content of individual cells.\n\nStats Illustrations by Allison Horst licensed under the Creative Commons Attribution licence.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Handling messy data</span>"
    ]
  },
  {
    "objectID": "12_messy_data/index.html#tidying-the-content-of-cells",
    "href": "12_messy_data/index.html#tidying-the-content-of-cells",
    "title": "12  Handling messy data",
    "section": "12.3 Tidying the content of cells",
    "text": "12.3 Tidying the content of cells\nAs well as data with a messy structure, you might be provided with data with messy content inside some of the cells. In this section we will clean messy content, mostly using functions from the stringr package that is loaded automatically when we load tidyverse. All the functions in the stringr package start with str_ so that they are easy to remember.\nWe can change the case of text using one of four str_to_ functions:\n\n\n\n\n\ninput\nfunction\noutput\n\n\n\n\nA stRinG oF TeXT\n`st_to_lower()`\na string of text\n\n\nA stRinG oF TeXT\n`st_to_upper()`\nA STRING OF TEXT\n\n\nA stRinG oF TeXT\n`st_to_sentence()`\nA string of text\n\n\nA stRinG oF TeXT\n`st_to_title()`\nA String Of Text\n\n\n\n\n\n\n\nWe can also remove unwanted text from within values. For example, if there is unwanted white-space (spaces, tabs, etc.) at the beginning or end of a string of characters, we can remove it with str_trim(). str_squish() does the same thing, but also reduces any repeated white-space characters in a string of text down to a single space. For example, str_squish(\"  A string   of text\") produces the result A string of text.\n\n\n\nData from the UK police open data website includes the words ‘on or near’ at the start of every value in the location column of the data. This is to remind users that the locations of crimes are deliberately obscured by being ‘snapped’ to the centre of the street on which they occur to protect victims’ privacy.\n\n\n\n\n\nmonth\nlongitude\nlatitude\nlocation\nlsoa_name\n\n\n\n\n2020-01\n-1.120\n53.3\nOn or near Supermarket\nBassetlaw 013C\n\n\n2020-02\n-0.993\n53.2\nOn or near Turner Lane\nNewark and Sherwood 001A\n\n\n2020-05\n-1.250\n53.1\nOn or near Brookdale Road\nAshfield 004C\n\n\n2020-08\n-1.160\n53.0\nOn or near Raithby Close\nNottingham 006B\n\n\n2020-10\n-1.180\n53.0\nOn or near Bowden Avenue\nAshfield 013A\n\n\n\n\n\n\n\nWe don’t need this constant value for analysis, and for large datasets it can unnecessarily increase the size of the data when we save it to a file. For this reason we might want to remove this constant value using the str_remove() function.\n\nmutate(uk_data, location = str_remove(location, \"On or near \"))\n\n# A tibble: 5 × 5\n  month   longitude latitude location       lsoa_name               \n  &lt;chr&gt;       &lt;dbl&gt;    &lt;dbl&gt; &lt;chr&gt;          &lt;chr&gt;                   \n1 2020-01    -1.12      53.3 Supermarket    Bassetlaw 013C          \n2 2020-02    -0.993     53.2 Turner Lane    Newark and Sherwood 001A\n3 2020-05    -1.25      53.1 Brookdale Road Ashfield 004C           \n4 2020-08    -1.16      53   Raithby Close  Nottingham 006B         \n5 2020-10    -1.18      53   Bowden Avenue  Ashfield 013A           \n\n\nThe lsoa_name code of this dataset includes the name of the small statistical area in which each crime occurred. Each name is made up of the name of the local government district covering the area followed by a unique code. If we wanted to extract just the district name (for example so we could count the number of crimes in each district) we can do that by removing the code that follows the district name. To do this we need to use a regular expression, which is a way of describing a pattern in a string of characters. Regular expressions can be used to find, extract or remove characters that match a specified pattern.\nRegular expressions can be complicated, so we will only scratch the surface of what’s possible here. You can find out much more about about them in the article on regular expressions included in the stringr package. The coded description of the pattern of characters we need to match the code at the end of the lsoa_name column is \\\\s\\\\w{4}$. This is made up three parts:\n\n\\\\s means match exactly one white-space character (e.g. a space or a tab),\n\\\\w{4} means match exactly four word characters (i.e. any letter, any number or some punctuation marks), and\n$ means match the end of the string.\n\nSo \\\\s\\\\w{4}$ means match exactly one white-space character followed by exactly four word characters at the end of the string. We can use this pattern as the second argument to the str_remove() function to remove the characters matched by the pattern.\n\nmutate(uk_data, district = str_remove(lsoa_name, \"\\\\s\\\\w{4}$\"))\n\n# A tibble: 5 × 6\n  month   longitude latitude location                  lsoa_name        district\n  &lt;chr&gt;       &lt;dbl&gt;    &lt;dbl&gt; &lt;chr&gt;                     &lt;chr&gt;            &lt;chr&gt;   \n1 2020-01    -1.12      53.3 On or near Supermarket    Bassetlaw 013C   Bassetl…\n2 2020-02    -0.993     53.2 On or near Turner Lane    Newark and Sher… Newark …\n3 2020-05    -1.25      53.1 On or near Brookdale Road Ashfield 004C    Ashfield\n4 2020-08    -1.16      53   On or near Raithby Close  Nottingham 006B  Notting…\n5 2020-10    -1.18      53   On or near Bowden Avenue  Ashfield 013A    Ashfield\n\n\nWe could also use regular expressions together with str_extract() to keep only the characters matched by the pattern, str_replace() to replace the first group of characters matched by the pattern and str_replace_all() to replace all the groups of characters matched by the pattern. For more tips on using regular expressions together with functions from the stringr package, see the stringr package cheat sheet or visit regexr.com.\n\n12.3.1 Converting between types of variable\nSometimes columns in your data will be stored as the wrong type of variable. read_csv() and other functions from the readr package try to guess what type of variable is contained in each column based on what is contained in the first few rows, but this does not always work. For example, a variable containing numbers might be stored as characters, meaning functions like mean() will not work. In cases like this, we can use the as.numeric() function to convert the character variable into a numeric variable.\n\n# Some numbers stored as text (note the quote marks around each number)\nnumbers_as_text &lt;- c(\"14\", \"6\", \"17\")\n\n# Trying to find the mean of these numbers stored as characters will produce \n# `NA` and a warning\nmean(numbers_as_text)\n\nWarning in mean.default(numbers_as_text): argument is not numeric or logical:\nreturning NA\n\n\n[1] NA\n\n# If we use `as.numeric()` then `mean()` now works\nmean(as.numeric(numbers_as_text))\n\n[1] 12.33333\n\n\n\nWe can use equivalent functions to convert columns to different types, e.g. as.character() to convert a variable to characters and as.logical() to convert a variable to only TRUE and FALSE values. Be careful, though: if you try to convert a variable to a data type that makes no sense, you are likely to find all of your values replaced with NA.\nSometimes numbers will be stored alongside other values, such as when currency values are stored together with a currency symbol. We can deal with values like these by using the parse_number() function from the readr package, which strips all the non-numeric characters from a value and then converts the numeric characters to a number. For example, parse_number(\"Room 14A\") produces the numeric value 14.\n\n\n12.3.2 Recoding categorical variables\nCrime data often includes categorical variables, such as crime types or location categories. It can be useful to change these categories, for example so that we can join two datasets or abbreviate category names for use in the axis labels of a chart.\nWe can use the if_else() function from the dplyr package to change particular values, but this only allows us to change a single value and can produce slightly untidy code. Instead we can use the recode() function from the dplyr package to change one or more values at the same time. For example, if we wanted to change the value Supermarket to Shop in the UK police data.\n\n# Once again we will remove the unnecessary 'On or near ' using `str_remove()`\nmutate(\n  uk_data, \n  location = recode(\n    str_remove(location, \"On or near \"),\n    \"Supermarket\" = \"Shop\"\n  )\n)\n\n# A tibble: 5 × 5\n  month   longitude latitude location       lsoa_name               \n  &lt;chr&gt;       &lt;dbl&gt;    &lt;dbl&gt; &lt;chr&gt;          &lt;chr&gt;                   \n1 2020-01    -1.12      53.3 Shop           Bassetlaw 013C          \n2 2020-02    -0.993     53.2 Turner Lane    Newark and Sherwood 001A\n3 2020-05    -1.25      53.1 Brookdale Road Ashfield 004C           \n4 2020-08    -1.16      53   Raithby Close  Nottingham 006B         \n5 2020-10    -1.18      53   Bowden Avenue  Ashfield 013A           \n\n\n\nRun the code needed to change the value Nottingham to City of Nottingham in the district column in the uk_data object (you will need to create the district column from the lsoa_name first).\n\nuk_data |&gt; \n  mutate(\n    district = str_remove(lsoa_name, \"\\\\s\\\\w{4}$\"),\n    district = recode(district, \"Nottingham\" = \"City of Nottingham\")\n  ) |&gt; \n  # Select only some columns to make the result easier to see below\n  select(lsoa_name, district)\n\n# A tibble: 5 × 2\n  lsoa_name                district           \n  &lt;chr&gt;                    &lt;chr&gt;              \n1 Bassetlaw 013C           Bassetlaw          \n2 Newark and Sherwood 001A Newark and Sherwood\n3 Ashfield 004C            Ashfield           \n4 Nottingham 006B          City of Nottingham \n5 Ashfield 013A            Ashfield           \n\n\n\n\n\n12.3.3 Missing values\nWe have already encountered the NA value, which R uses to represent a value that is missing from a dataset. While R uses NA to represent missing values, some data providers use other codes. For example, a data provider might use a dash (-) or two periods (..) to represent missing values. Fortunately, we can convert any value to NA so that we know that it represents a missing value.\nImagine that you have been provided with a dataset of burglaries that includes an estimate of the value of the goods that were stolen in British pounds. You have been told that when the value of the stolen goods wasn’t known, this is recorded as the value -1 in the data. If we use mean() to estimate the average value of property stolen, the value -1 will give us an incorrect result. If we instead convert that value to NA first, mean() will know to ignore that value as long as we specify the argument na.rm = TRUE.\n\n# Print some rows of burglary values\nhead(burglary_values)\n\n# A tibble: 5 × 2\n  date                value\n  &lt;dttm&gt;              &lt;dbl&gt;\n1 2020-01-21 21:52:38  1320\n2 2020-01-02 16:27:56   653\n3 2020-01-02 08:24:04  2068\n4 2020-01-23 12:16:27    -1\n5 2020-01-28 00:35:31   580\n\n# Try to calculate the mean value -- no error, but the answer is wrong\n# `pull()` is used to extract the `value` column from `burglary_values`\nmean(pull(burglary_values, \"value\"))\n\n[1] 924\n\n# Convert `-1` to `NA` first, now you get the correct mean value\nburglary_values &lt;- mutate(\n  burglary_values,\n  value = if_else(value == -1, NA_real_, value)\n)\n\n# Calculate the mean value again, now excluding the missing value\nmean(pull(burglary_values, \"value\"), na.rm = TRUE)\n\n[1] 1155.25\n\n\nExcluding the value -1 makes a substantial difference to the mean, increasing it by over £100.\n\n\n12.3.4 Null Island\nThe problem of missing values being stored as numbers manifests itself in a particularly frustrating way when it comes to spatial data. People and organisations that produce spatial datasets sometimes recording missing co-ordinates as being zero, instead of being missing. This is a problem because the longitude/latitude co-ordinates “0, 0” correspond to a real location on the surface of the earth, in the Gulf of Guinea off the coast of Ghana. This location is incorrectly recorded in spatial datasets so often that it’s become known as Null Island. Watch this video to find out more about Null Island and the problems it causes.\n\nSo if you see any co-ordinates on your map located in the Atlantic off the south coast of West Africa, you know that there are almost certainly rows in your data that have co-ordinates located at Null Island.\n\n\n\nWhen we make crime maps we are generally dealing with small areas such as cities and counties. So if you plot a dataset on a map and instead of seeing a map of the area you are interested in, you see a large area of the world with the place you are interested in in one corner and Null Island in the opposite corner, that almost certainly means some co-ordinates are located at Null Island.\nAs an example, imagine we were trying to create a map of the home addresses of several (fictional) suspects for bank fraud in and around Cairo, Egypt. The data contain 10 rows stored in an object called cairo_suspects that looks like this:\n\n\n\n\n\n\n\n\n\n\nname\naddress\ngeometry\n\n\n\n\nAhmed Hussein Ayman\n94, El Sheikh Abd El Jalil Issa Street, Al Banafseg 8, Banafseg Districts, New Cairo City, Cairo\nPOINT (31.46236 30.04954)\n\n\nMohamed Ali Mohamed\n22, Noran Street, Al Salam First, Al Qalyubiya\nPOINT (31.4032 30.16475)\n\n\nMahmoud Mostafa Ali\n43, Tarek Gamal Street, Cairo\nPOINT (31.29651 30.11844)\n\n\nOmar El-Badawi\n7, Abou Bakr Al Sediq Street, Al-Obour, Al Qalyubiya\nPOINT (31.37844 30.17814)\n\n\nTarek Hazim Abdel-Rahman\n16, Al Madina Al Mnoura Street, Ma‘ di, Cairo\nPOINT (31.26321 29.97942)\n\n\nYoussef Sayyid\n18, Fathy Abou Wedn Street, Shubra al Khayma, Al Qalyubiya\nPOINT (31.34177 30.15624)\n\n\nHussein El-Masri\n61, Al Ashgar Street, South West, El Shorouk City, May Fair, Cairo\nPOINT (31.61096 30.12838)\n\n\nMariam Abdel Mubarak\n15R, Mohamed Farid Street, EL Sheikh Mubarak, Cairo\nPOINT (31.24903 29.99366)\n\n\nFarah Youssef Anwar\n35, Al Sadat Road, Neighborhood 9, El Shorouk City, Sunrise, Cairo\nPOINT (31.6281 30.1478)\n\n\nNour El-Seifi\n23, Moustafa Kamel Street, Area 2, Badr, Cairo\nPOINT (0 0)\n\n\n\n\n\nIf we plotted these locations on a map, we might expect to see something like this:\n\n\n\nBut look again at the final row of data in the table above – the co-ordinates for the final row are both zero. There are several reasons why this might be. Perhaps the address was recorded incorrectly in a police database and that meant that when the addresses were run through geocoding software (which we will learn more about in the next section), no co-ordinates for that row could be found. What this means is that when we plot the data on a map, that map will actually look like this:\n\n\n\nWhat we can see here is most of the points on this map are correctly located in and around Cairo, but a single point is incorrectly located at Null Island.\nWe can deal with the problem of our data including points located at Null Island using the st_intersection() function that we have previously used to clip datasets to the boundaries of other datasets. In this case, we can use st_intersection() to remove any rows from the data that are not within the area that the data is supposed to cover.\nFor example, we know that all the addresses of bank fraud suspects are supposed to be in Egypt, so we can safely clip the suspect data to the boundary of Egypt before mapping the data. There are lots of sources of data on the outlines of countries, but perhaps the most convenient to use in R is the data provided by the rnaturalearth package. We can use the ne_countries() function to retrieve the outline of Egypt as an SF object, which we can then use to clip the suspect dataset.\n\n# By default, `ne_countries()` does not return an SF object, so we have to\n# specify that is what we want using the `returnclass = \"sf\"` argument\negypt_outline &lt;- rnaturalearth::ne_countries(\n  country = \"Egypt\", \n  returnclass = \"sf\"\n)\n\ncairo_suspects_valid &lt;- st_intersection(cairo_suspects, egypt_outline)\n\nWarning: attribute variables are assumed to be spatially constant throughout\nall geometries\n\n\nNow if we were to plot a map using the cairo_suspects_valid object, we would see the map covered only the Cairo area, as we originally wanted.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Handling messy data</span>"
    ]
  },
  {
    "objectID": "12_messy_data/index.html#geocoding-locations",
    "href": "12_messy_data/index.html#geocoding-locations",
    "title": "12  Handling messy data",
    "section": "12.4 Geocoding locations",
    "text": "12.4 Geocoding locations\nThroughout this course we have used geographic data that includes locations stored as pairs of co-ordinates, e.g. latitude and longitude or easting and northing. Sometimes geographic data will not contain co-ordinates but instead store the locations of places or events as free-text addresses.\n\nAddress fields can be very messy indeed. This is because addresses can often be stored in different formats, include different abbreviations or use different spellings (including typos). For example, the official postal address ‘Kelley’s Grill & Bar, 15540 State Avenue, Basehor, Kansas 66007, United States’ could be stored in a local police report as:\n\nKelly’s Bar, 15540 US Highway 40, Basehor\nKelley’s Grille, 15540 State\nKelley’s, 15540 State Av\nKelley’s Bar and Grill, 15540 State Ave\nKelley’s Bar, State Av and 155th St\nKelly’s Grill, State Ave btwn 155 and 158\n\nAll of these address descriptions would probably be good enough for local police officers to know which building the author intended to reference. But since all these different addresses relate to the same physical location, they would make it very hard to (for example) work out how many incidents had occurred at Kelley’s Grill & Bar using count() or a similar function.\nTo make use of data containing addresses, it is typically necessary to geocode the locations, i.e. to convert the addresses into co-ordinates. The many ways to describe an address mean that geocoding is often quite hard.\n\nWe can geocode addresses in R using the tidygeocoder package, which provides an interface to several online geocoding services.\nTo run a geocoding service an organisation has to maintain a database of many millions of addresses (which must be constantly updated) and handle addresses in many different formats, so organisations typically charge for these services or limit how many addresses you can geocode for free. Most services also require you to register, even if you are only making few-enough queries that you will not be charged. tidygeocoder supports several geocoding services:\n\n\n\n\n\n\n\n\nservice\ncoverage\nfree limits\n\n\n\n\nNominatim\nWorldwide\n1 address per second\n\n\nLocation IQ\nWorldwide\n5,000 addresses per day\n\n\nGeoapify\nWorldwide\n3,000 addresses per day\n\n\nOpenCage\nWorldwide\n2,500 addresses per day\n\n\nTomTom\nWorldwide\n2,500 addresses per day\n\n\nGoogle\nWorldwide\n40,000 addresses per month\n\n\nGeocodio\nUnited States and Canada\n2,500 addresses per day\n\n\nUS Census\nUnited States\nnone\n\n\n\n\n\n\n\nThe tidygeocoder package also supports some other services that do not offer any free option – you can find out more about these options on the package website.\nTo illustrate the geocoding process, we will find co-ordinates for the addresses in the object addresses, which holds data for 10 sexual assaults in Chicago.\n\n\n\n\n\noffense_date\nlocation_type\naddress\n\n\n\n\n2019-01-01 00:00:00\nresidence\n2400 W Carmen Ave\n\n\n2019-01-01 00:00:00\nresidence\n2700 S TRIPP AVE\n\n\n2019-01-01 11:44:00\nresidence\n3700 S PAULINA ST\n\n\n2019-01-01 11:44:00\nresidence\n3700 S Paulina St\n\n\n2019-01-01 16:37:00\ngovernment\n1100 S HAMILTON AVE\n\n\n2019-01-02 17:09:00\ngas station\nNA\n\n\n2019-01-02 17:09:00\ngas station\n8200 S HALSTED ST\n\n\n2019-01-05 00:01:00\nresidence\n1300 N HUDSON AVE\n\n\n2019-01-05 14:00:00\nother\n6200 N Claremont Ave\n\n\n2019-01-07 06:50:00\nresidence\n9500 S BELL AVE\n\n\n\n\n\n\n\nSince most geocoding services limit the number of addresses you can look up at a time, the first step in geocoding is removing duplicate addresses and rows with missing address values. This avoids us geocoding identical addresses several times, which would otherwise unnecessarily increase our chance of hitting the limit on geocoding queries each day.\nWe will also add the city and state to the end of each address, since at the moment (as with much data produced by local organisations) it includes only the building number and street.\n\naddresses_for_geocoding &lt;- addresses |&gt; \n  # Drop rows that have NA values in the `address` column\n  drop_na(address) |&gt; \n  # Add city and state then convert to upper case so that `count()` will not \n  # treat identical addresses as different because of different cases, e.g. \n  # 'ST' vs 'St' as abbreviations for 'Street'\n  mutate(address = str_to_upper(str_glue(\"{address}, CHICAGO, IL\"))) |&gt; \n  # Select only the address column, since we won't send the other columns to the\n  # geocoding function\n  select(address) |&gt; \n  # Find all the unique rows in the data\n  distinct(address)\n\nhead(addresses_for_geocoding)\n\n# A tibble: 6 × 1\n  address                         \n  &lt;chr&gt;                           \n1 2400 W CARMEN AVE, CHICAGO, IL  \n2 2700 S TRIPP AVE, CHICAGO, IL   \n3 3700 S PAULINA ST, CHICAGO, IL  \n4 1100 S HAMILTON AVE, CHICAGO, IL\n5 8200 S HALSTED ST, CHICAGO, IL  \n6 1300 N HUDSON AVE, CHICAGO, IL  \n\n\nSince two addresses in the data were duplicates and one address was missing, we now have eight unique addresses, stored in a tibble with a single column. We can use this as the input to the geocode() function from the tidygeocoder package. The address argument specifies which column in the data contains the addresses and the method column specifies which geocoding service to use. In this case we use the Nominatim service (because it does not require registration and works worldwide). Nominatim is based on OpenStreetMap data, so can be chosen by specifying method = \"osm\".\n\nlibrary(tidygeocoder)\n\naddresses_geocoded &lt;- geocode(\n  addresses_for_geocoding, \n  address = \"address\", \n  method = \"osm\"\n)\n\nPassing 8 addresses to the Nominatim single address geocoder\n\n\nQuery completed in: 8 seconds\n\naddresses_geocoded\n\n# A tibble: 8 × 3\n  address                             lat  long\n  &lt;chr&gt;                             &lt;dbl&gt; &lt;dbl&gt;\n1 2400 W CARMEN AVE, CHICAGO, IL     42.0 -87.7\n2 2700 S TRIPP AVE, CHICAGO, IL      41.8 -87.7\n3 3700 S PAULINA ST, CHICAGO, IL     41.8 -87.7\n4 1100 S HAMILTON AVE, CHICAGO, IL   41.9 -87.7\n5 8200 S HALSTED ST, CHICAGO, IL     41.7 -87.6\n6 1300 N HUDSON AVE, CHICAGO, IL     41.9 -87.6\n7 6200 N CLAREMONT AVE, CHICAGO, IL  42.0 -87.7\n8 9500 S BELL AVE, CHICAGO, IL       41.7 -87.7\n\n\nNow that we have the latitude and longitude for each address, we can join that back to the original data using the address column to match the two datasets together. To do this we will create a temporary column in the original addresses object that matches the formatting changes we made to the original address.\n\naddresses |&gt; \n  # Create a temporary address column to use in matching the geocoded addresses\n  mutate(temp_address = str_to_upper(str_glue(\"{address}, CHICAGO, IL\"))) |&gt; \n  # `left_join()` keeps all the rows in the left-hand dataset (the original\n  # `addresses` object) and matching rows in the right-hand dataset (the \n  # geocoding results)\n  left_join(addresses_geocoded, by = c(\"temp_address\" = \"address\")) |&gt; \n  # Remove the temporary address column\n  select(-temp_address)\n\n# A tibble: 10 × 5\n   offense_date        location_type address                lat  long\n   &lt;dttm&gt;              &lt;chr&gt;         &lt;chr&gt;                &lt;dbl&gt; &lt;dbl&gt;\n 1 2019-01-01 00:00:00 residence     2400 W Carmen Ave     42.0 -87.7\n 2 2019-01-01 00:00:00 residence     2700 S TRIPP AVE      41.8 -87.7\n 3 2019-01-01 11:44:00 residence     3700 S PAULINA ST     41.8 -87.7\n 4 2019-01-01 11:44:00 residence     3700 S Paulina St     41.8 -87.7\n 5 2019-01-01 16:37:00 government    1100 S HAMILTON AVE   41.9 -87.7\n 6 2019-01-02 17:09:00 gas station   &lt;NA&gt;                  NA    NA  \n 7 2019-01-02 17:09:00 gas station   8200 S HALSTED ST     41.7 -87.6\n 8 2019-01-05 00:01:00 residence     1300 N HUDSON AVE     41.9 -87.6\n 9 2019-01-05 14:00:00 other         6200 N Claremont Ave  42.0 -87.7\n10 2019-01-07 06:50:00 residence     9500 S BELL AVE       41.7 -87.7\n\n\nWe can now use this data as we would any other spatial data. We would often start by converting our new object to an SF object using st_as_sf(). In that case, remember that all the geocoding services return co-ordinates as latitude and longitude using the WGS84 co-ordinate reference system, so you should use the EPSG code 4326.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Handling messy data</span>"
    ]
  },
  {
    "objectID": "12_messy_data/index.html#conflicts-between-function-names",
    "href": "12_messy_data/index.html#conflicts-between-function-names",
    "title": "12  Handling messy data",
    "section": "12.5 Conflicts between function names",
    "text": "12.5 Conflicts between function names\nR packages are written by experts in different types of data analysis. This means we can use R to conduct all sorts of analysis, including cutting-edge techniques. But because R packages are not all written by people working for a single organisation, it means there can sometimes be functions from different packages that have the same name – often because they do similar (but not necessarily identical) things.\nThis can be a problem because R might be running a function from one package when you think you have written code that uses a function from another package. If the two functions have different arguments or produce different types of result, your code is unlikely to run properly. For example, if two functions have the same name but one produces an SF object while the other produces a tibble, that might cause problems later in your code if you are using other functions that expect to receive a particular type of input.\nOne example of functions from two different packages with the same name is the geocode() function. We have already learned that the tidygeocoder package contains a function called geocode(), but the ggmap package also has a function called geocode().\nIf both the tidygeocoder and ggmap packages are loaded, R will use the geocode() function from the package that has been loaded most recently. So if your script includes the code:\nlibrary(ggmap)\nlibrary(tidygeocoder)\n\n# [Code to load data, etc.]\n\n# Geocode some data\ngeocoded_data &lt;- geocode(ungeocoded_data)\nThen R will use the geocode() function from tidygeocoder, since that was loaded after ggmap. But if you reverse the order in which these packages are loaded:\nlibrary(tidygeocoder)\nlibrary(ggmap)\n\n# [Code to load data, etc.]\n\n# Geocode some data\ngeocoded_data &lt;- geocode(ungeocoded_data)\nThen R will use the geocode() function from ggmap because that package was loaded after tidygeocoder.\nKnowing which of the two functions R uses when you type geocode() is important because they accept different inputs. The geocode() function from ggmap expects the first argument to contain a vector of addresses, while the geocode() function from tidygeocoder expects a data frame or tibble. If you provide the wrong type of input, either function will produce an error.\nWhen you load a package that includes a function with the same name as a function in a package that is already loaded, R will print a message in the console. For example, if you run library(ggmap) and then library(tidygeocoder), you will see a message saying the function from the already loaded package has been masked and so will be replaced by the function from the newly loaded package:\nAttaching package: ‘tidygeocoder’\n\nThe following object is masked from ‘package:ggmap’:\n\n    geocode\nWhen you see a message like this, it means you need to check which version of that function you want to use. If you want to use the function from the most-recently loaded package then you don’t need to do anything. If you want to use the function that has been masked, you need to take one of the steps explained below.\nOur R scripts very often load the tidyverse package, which loads eight packages (dplyr, forcats, ggplot2, purrr, readr, tibble, tidyr and stringr) that we often need to use. Because tidyverse itself loads several packages, it produces a separate message to let you know which existing functions have been masked when you load tidyverse. If library(tidyverse) is the first code you run after starting R, that message will look something like this:\n── Conflicts ──────────────────────────────────────── tidyverse_conflicts() ──\nx dplyr::filter() masks stats::filter()\nx dplyr::lag()    masks stats::lag()\nSince we more commonly want the tidyverse functions than the functions they mask (for example, we frequently use the filter() function from dplyr but rarely use the function with the same name in the stats package), it is common to load tidyverse after loading all the other packages needed for our code.\nThere are several ways you can resolve conflicts between functions from different packages:\n\nMake sure you load packages in the order that gives you access to the functions you need. This can be simple if there are few conflicts, but can fall apart if you later change your code to load different packages. This is the option that almost everyone uses most of the time (and most people use all of the time).\nSpecify which function you want to use using the :: notation that we have already used in previous tutorials, e.g. by using the code ggmap::geocode() or tidygeocoder::geocode().\nLoad the conflicted package, which produces an error whenever you try to use a function that is included in more than one loaded package. This makes it easy to know when there is a potential conflict, which you can then resolve using the conflict_prefer() function.\n\nIn this final option, the authors of the conflicted package recommend that you specify which conflict-prone functions to use by calling conflict_prefer() just after you’ve loaded all the packages you need for your script. For example:\n# This code produces several conflicts:\n# *  the `stats` package, which is loaded automatically in the background when \n#    you start R, contains a function called `filter()`\n# *  `MASS` contains a function called `select()`\n# *  `dplyr` contains functions called `filter()` and `select()`\nlibrary(dplyr)\nlibrary(MASS)\nlibrary(conflicted)\n\n# This code specifies that in the case of both conflicts, we want to use the\n# functions from `dplyr`\nconflict_prefer(\"select\", \"dplyr\")\nconflict_prefer(\"filter\", \"dplyr\")\n\n# In the rest of this script, any reference to `filter()` or `select()` will\n# use the functions with those names in the `dplyr` package, not those from\n# `stats` or `MASS`\nUnderstanding function-name conflicts is important because they can be the cause of errors that are (for reasons we needn’t explain in detail) extremely hard to track down the cause of. So make sure that when you load packages, you take note of any messages in the console that warn you that functions have been masked. As with other messages in R, they will not always need you to take any action, but you always need to read the message and make a decision about whether to take any action.\n\n12.5.0.1 Check your understanding\n\nquestion(\n  \"If you load two packages in R that both contain a function with the same name, which function will R use by default?\",\n  answer(\"The function from the package that was loaded *last*\", correct = TRUE),\n  answer(\"The function from the package that was loaded *first*\"),\n  answer(\"Neither function will run because R will produce an error.\"),\n  answer(\"The function that is from the `tidyverse` suite of packages.\"),\n  correct = random_praise(),\n  incorrect = random_encouragement(),\n  allow_retry = TRUE,\n  random_answer_order = TRUE\n)",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Handling messy data</span>"
    ]
  },
  {
    "objectID": "12_messy_data/index.html#in-summary",
    "href": "12_messy_data/index.html#in-summary",
    "title": "12  Handling messy data",
    "section": "12.6 In summary",
    "text": "12.6 In summary\n\nIn this tutorial we have learned to tidy messy data to make it easier to work with. In real-world data analysis (not just crime mapping), you will often have to deal with data that is messy in different ways. Fortunately, you can use R to fix all of these problems in a way that minimises the risk that you might introduce mistakes when you clean the data.\n\n\nYou can find out more about data cleaning and tidying with these resources:\n\nA tutorial on using the pivot_longer() and pivot_wider() functions to convert data between long and wide format.\nA more-detailed Introduction to tidygeocoder.\nAn introduction to some other packages than can help you clean and tidy data.\nA more-detailed description of how to use the conflicted package.\n\n\n\n\n\n\nArtwork by @allison_horst",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Handling messy data</span>"
    ]
  },
  {
    "objectID": "13_crime_series/index.html",
    "href": "13_crime_series/index.html",
    "title": "13  Mapping crime series",
    "section": "",
    "text": "13.1 Introduction\nLearn how to map crime series, including mapping movement over time and the journey to crime.\nIt’s common in crime mapping to treat each crime as a separate event. But a large proportion of crimes are committed by a relatively small proportion of all the people who commit crime. These persistent or repeat offenders commit crimes frequently, maybe several times a day in the case of some people. One type of repeat offending is the crime series, in which an offender or group of offenders commits the same or related crimes in different places over time.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Mapping crime series</span>"
    ]
  },
  {
    "objectID": "13_crime_series/index.html#introduction",
    "href": "13_crime_series/index.html#introduction",
    "title": "13  Mapping crime series",
    "section": "",
    "text": "13.1.1 Case linkage\nOne of the difficulties in studying serial offending is in identifying which offences are committed by the same offender. Case linkage is the process of identifying crime series by looking for similarities in the method used by an offender (e.g. entering a house by breaking a lock on a side door out of sight from the street) or by evidence left at the scene (such as fingerprints or DNA). Case linkage is typically an imperfect process – investigators might believe that two offences were committed by the same person, but in most cases they are unlikely to know for sure unless the offender has been caught (and sometimes not even then).\nWatch this video to learn a bit more about crime linkage.\n\nOnce crimes have been linked together as being part of the same series, it can be useful to map them to better understand how the series has progressed. In this tutorial we will learn how to map linked cases.\n\n\n13.1.2 The case used in this tutorial\nDue to the uncertainty of case linkage, there is little publicly available data on the locations of serial crimes. The available data tends to focus on serial murders. To minimise the likelihood of anyone taking this course having been directly affected by – or know anyone affected by – the cases we use in this tutorial, we will use a historical example.\nThe Hungerford massacre occurred in southern England in August 1987, when a marauding attacker (also known as a spree offender) killed 16 people and shot many others in the space of about 90 minutes. We will use a dataset called hungerford_shootings that contains the names of each victim and the approximate location at which they were shot (recorded as eastings and northings in the British National Grid). This data is taken from the official report into the massacre. The order column shows the order in which the victims were shot. These are the first few rows of the dataset (the dagger symbol † shows that the victim was killed):\n\n\n\n\n\n\n\n\n\n\n\nvictims\norder\neasting\nnorthing\n\n\n\n\nSusan GODFREY†\n1\n423168\n167694\n\n\nKakaub DEAN\n2\n429455\n167904\n\n\nRoland MASON†, Sheila MASON†, Marjorie JACKSON, Lisa MILDENHALL\n3\n433849\n168176\n\n\nKenneth CLEMENTS†\n4\n434044\n168133\n\n\nRoger BRERETON†, Linda CHAPMAN, Alison CHAPMAN\n5\n433897\n168165\n\n\n\n\n\nIn this tutorial we will learn how to create this composite map of the locations of the shootings in this crime series.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Mapping crime series</span>"
    ]
  },
  {
    "objectID": "13_crime_series/index.html#mapping-linked-cases",
    "href": "13_crime_series/index.html#mapping-linked-cases",
    "title": "13  Mapping crime series",
    "section": "13.2 Mapping linked cases",
    "text": "13.2 Mapping linked cases\nMapping crime series can be useful for several reasons. For example, a map of a crime series might help the jurors in a criminal trial to better understand the sequence of events that the suspect is accused of. This can be especially helpful if the sequence is complicated or especially long.\nAt the moment, the hungerford_shootings dataset contains the point location of each event and the order in which they occurred. To show a sequence of events on a map, we need to link each event with those that happened immediately before and after it.\n\n\n\n\n\n\n\n\n\n\n\nvictims\norder\neasting\nnorthing\n\n\n\n\nSusan GODFREY†\n1\n423168\n167694\n\n\nKakaub DEAN\n2\n429455\n167904\n\n\nRoland MASON†, Sheila MASON†, Marjorie JACKSON, Lisa MILDENHALL\n3\n433849\n168176\n\n\nKenneth CLEMENTS†\n4\n434044\n168133\n\n\nRoger BRERETON†, Linda CHAPMAN, Alison CHAPMAN\n5\n433897\n168165\n\n\n\n\n\nOne way to do this is to draw lines on the map connecting each incident in turn. To do this, we will create a dataset where each row contains two pairs of co-ordinates: one representing the location of a particular shooting and the other representing the location of the previous shooting. To do that, we will:\n\nTake the hungerford_shootings dataset and rename the existing columns holding the co-ordinates so that it is clear those co-ordinates are for the end of each line between points. To do this we will use the rename() function from the dplyr package.\nAdd two new columns to each row that show the co-ordinates of the row in the dataset immediately above the current row (i.e. the row that represents the location before the current location. We will do this with the lag() function, also from dplyr.\nUse these two pairs of co-ordinates to plot lines on a map.\n\n\nhungerford_lines &lt;- hungerford_shootings |&gt; \n  rename(x_end = easting, y_end = northing) |&gt; \n  mutate(x_start = lag(x_end), y_start = lag(y_end)) |&gt; \n  # Remove the first line, which contains missing values and which we don't need\n  remove_missing(vars = c(\"x_start\", \"y_start\"), na.rm = TRUE)\n\nhead(hungerford_lines)\n\n# A tibble: 6 × 6\n  victims                                    order  x_end  y_end x_start y_start\n  &lt;chr&gt;                                      &lt;int&gt;  &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n1 \"Kakaub DEAN\"                                  2 429455 167904  423168  167694\n2 \"Roland MASON†\\nSheila MASON†\\nMarjorie J…     3 433849 168176  429455  167904\n3 \"Kenneth CLEMENTS†\"                            4 434044 168133  433849  168176\n4 \"Roger BRERETON†\\nLinda CHAPMAN\\nAlison C…     5 433897 168165  434044  168133\n5 \"Abdur KHAN†\\nAlan LEPETIT\\nHazel HASLETT…     6 433821 168176  433897  168165\n6 \"Betty TOLLADAY\"                               7 433954 168032  433821  168176\n\n\n\n\nWhy does this code include remove_missing()?\n\n\nYou might notice that the values in the first row of the x_start and x_end columns in the hungerford_lines object are NA values. This is because these columns have been created using the lag() function, which gets the value from the same column in the row immediately above. The first row doesn’t have a row immediately above it, so lag() returns NA. Since the first row does not represent a link between shootings in any case, we can remove it with remove_missing().\n\n\n\nThis method for creating lines between points is one of several ways we could achieve the same result. For example, we could create lines connecting the points and store them as SF objects. However, the code to do that is more complicated and gives us slightly less control over the appearance of the lines, so we will not use that method. Using the method shown in the box above means the lines on our map are not SF objects, so we cannot use geom_sf() to add them to the map. Instead, we will create a map using the geom_segment() function from the ggplot2 package.\n\n# Convert the `hungerford_shootings` object to SF\nhungerford_shootings_sf &lt;- st_as_sf(\n  hungerford_shootings, \n  coords = c(\"easting\", \"northing\"), \n  crs = \"EPSG:27700\"\n)\n\n# Plot a basic map\nggplot() +\n  # Plot base map\n  annotation_map_tile(zoomin = 0, progress = \"none\") +\n  # Plot lines between points\n  geom_segment(\n    aes(x = x_start, y = y_start, xend = x_end, yend = y_end), \n    data = hungerford_lines\n  ) +\n  # Plot points\n  geom_sf(data = hungerford_shootings_sf) +\n  # Since one of our layers is not an SF object, tell `ggplot()` what \n  # co-ordinate system to use\n  coord_sf(crs = \"EPSG:27700\") +\n  theme_void()\n\nLoading required namespace: raster\n\n\n\n\n\n\n\n\n\nThis map is not particularly useful – we will return to that in a minute.\nIf you look at this code, you’ll see that we’ve included the function coord_sf() in our ggplot() stack. In all the previous maps that we have made, we have used geom_sf() to plot geographic data on our maps. geom_sf() understands how to translate co-ordinates specified using different co-ordinate systems to locations on the surface of each map. Other functions in the geom_*() family of functions do not know how to do this automatically, so we must specify the co-ordinate system by adding the coord_sf() function to the stack. We only need to use one argument to coord_sf(): the crs argument to specify the co-ordinate system that our data uses. In this case, we know that the co-ordinates in the hungerford_lines object are specified using the British National Grid, which has the EPSG code 27700.\nThe map we have made above shows the sequence of events, but there are at least four ways that we can make it better.\n\nIt would be useful to be able to see a larger area around the shooting locations, to see more of the town surrounding the area in which the crimes occurred.\nWe don’t know which order to move through the sequence of lines. We can deal with this by adding arrows to show the direction of travel.\nThe straight lines make it look like the offender travelled across fields between locations, which is probably not true. Since we do not have data on what routes the offender took, we can replace the straight lines with curves to indicate that the exact route is unknown.\nBecause the first two shootings occurred outside the town, the map has to cover a large area and this makes it harder to see the sequence of events in the town itself.\n\n\n13.2.1 Extending the map area\nBy default, ggplot() works out the limits of the area shown on the map based on the area covered by the data we add to each stack with functions from the geom_*() family of functions. Usually this works well, but in this case the distribution of the shootings means that the map shows only a small slice of the town of Hungerford and the surrounding area. It would be useful for readers to be able to see a larger area, to be able to better identify where the shootings occurred.\nTo expand the area shown on the map, we can use scale_y_continuous() function from the ggplot2 package. We have used the scale_fill_distiller() and scale_fill_gradient() functions in previous tutorials to control how columns in the data are represented as colours on the map. The scale_y_continuous() function is similar, in that it controls the vertical (Y) axis on the map. We don’t need to use most of the capabilities of scale_y_continuous() because ggplot() sets reasonable default values. The only argument we need to set is the expand argument, which controls how far the map should extend around the area covered by the data.\nThe easiest way to set the correct values for the expand function is to use the expansion() helper function. We can use this to specify how much extra area around the data we want to include on the map. For example, expansion(2) means that the Y axis of the map should cover the area covered by the data and twice that distance either side of the data.\n\n# Plot a basic map\nggplot() +\n  # Plot base map\n  annotation_map_tile(zoomin = 0, progress = \"none\") +\n  # Plot lines between points\n  geom_segment(\n    aes(x = x_start, y = y_start, xend = x_end, yend = y_end), \n    data = hungerford_lines\n  ) +\n  # Plot points\n  geom_sf(data = hungerford_shootings_sf) +\n  scale_y_continuous(expand = expansion(2)) +\n  coord_sf(crs = \"EPSG:27700\") +\n  theme_void()\n\n\n\n\n\n\n\n\n\nThe scale_y_continuous() function specifically controls the vertical (Y) axis of the map, so it only adds space above and below the data on the map. If we wanted to add space to the left and right of the data, we would need to use the scale_x_continuous() function. But in this case, since the map is already very wide relative to its height, we will not make the map any wider.\n\n\n\n13.2.2 Adding arrows and curves\nTo add an arrow to the end of each line segment we can use the arrow() helper function from the ggplot2 package to specify the arrow argument of the geom_segment() function in our ggplot() stack. Here, we make the arrow head smaller than the default by setting length = unit(3, \"mm\") and choose the style of the arrow head with type = \"closed\".\nTo prevent the arrow heads from obscuring the points, we will give the points a thin white border by specifying shape = 21 (a circle with a separate border) and colour = \"white\", as well as making the points slightly bigger with size = 2.\n\nggplot() +\n  # Plot base map\n  annotation_map_tile(zoomin = 0, progress = \"none\") +\n  # Plot lines between points\n  geom_segment(\n    aes(x = x_start, y = y_start, xend = x_end, yend = y_end),\n    data = hungerford_lines, \n    arrow = arrow(length = unit(3, \"mm\"), type = \"closed\"),\n    colour = \"orange3\"\n  ) +\n  # Plot points\n  geom_sf(\n    data = hungerford_shootings_sf,\n    shape = 21,\n    colour = \"white\",\n    fill = \"orange3\",\n    size = 3\n  ) +\n  scale_y_continuous(expand = expansion(2)) +\n  coord_sf(crs = \"EPSG:27700\") +\n  theme_void()\n\n\n\n\n\n\n\n\nThis makes it easier to see that the shootings started at the point on the left of the map, but makes the problem of understanding the sequence of events in the town itself even worse. We will deal with this in the next section.\nIn the map above, we used the geom_segment() function to add the lines to our map. If we change this to geom_curve() the lines will become curved rather than straight. By specifying curvature = -0.2 we get a slightly straighter line than the default, and a left-hand curve because the value of curvature is negative.\nThe other change we can make at this point is to add a layer of labels showing the order in which the shootings occurred. We don’t want the labels to overlap the points, so we will use geom_label_repel() from the ggrepel package to create labels that are automatically offset from the points they relate to. For now, we will just label the first two locations.\nAt this point we can also make some minor changes to the map – adding a title and scale bar, putting a neat line around the map – the purpose of which will become clear in the next section.\n\nlibrary(ggrepel)\n\nhungerford_map_overall &lt;- ggplot() +\n  # Plot base map\n  annotation_map_tile(zoomin = 0, progress = \"none\") +\n  # Plot lines between points\n  geom_curve(\n    aes(x = x_start, y = y_start, xend = x_end, yend = y_end),\n    data = hungerford_lines, \n    arrow = arrow(length = unit(3, \"mm\"), type = \"closed\"),\n    curvature = -0.2,\n    colour = \"orange3\"\n  ) +\n  # Plot points\n  geom_sf(\n    data = hungerford_shootings_sf, \n    shape = 21,\n    colour = \"white\",\n    fill = \"orange3\",\n    size = 3\n  ) +\n  geom_label_repel(\n    aes(x = easting, y = northing, label = order),\n    data = filter(hungerford_shootings, order %in% 1:2), \n    colour = \"white\",\n    fill = \"orange3\",\n    fontface = \"bold\",\n    label.size = NA\n  ) +\n  annotation_scale(style = \"ticks\", line_col = \"grey40\", text_col = \"grey40\") +\n  scale_y_continuous(expand = expansion(2)) +\n  labs(title = \" Shootings in Wiltshire\") +\n  coord_sf(crs = \"EPSG:27700\") +\n  theme_void() +\n  theme(\n    panel.border = element_rect(colour = \"grey20\", fill = NA),\n    plot.title = element_text(margin = margin(b = -18))\n  )\n\nhungerford_map_overall\n\n\n\n\n\n\n\n\nNow we have added arrow heads and curved lines, in the next section we will learn how to solve the problem of the events in the town itself being unclear on the map.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Mapping crime series</span>"
    ]
  },
  {
    "objectID": "13_crime_series/index.html#multiple-maps",
    "href": "13_crime_series/index.html#multiple-maps",
    "title": "13  Mapping crime series",
    "section": "13.3 Multiple maps",
    "text": "13.3 Multiple maps\n\n\n\nOur existing map is difficult to understand because we have a mix of some events very close together (including several on the same street) and some that occurred further away. This means the closer events overlap on the map, especially now that we have made the points larger to distinguish them from the lines linking each event.\n\nTo deal with this problem we can display two maps: one showing the whole area covered by the points and a second (sometimes called an inset map) showing only the events in the town of Hungerford itself. We will do this using the patchwork package to combine multiple maps made using ggplot().\nWe have already saved the first map as hungerford_map_overall. We can create a map showing only the shootings in Hungerford itself by removing the first two points from hungerford_points_sf (since they represent shootings outside the town itself). We can do this for all the existing layers using slice().\nWe will use the plot.margin argument to the theme() function to add a small amount of space at the top of this map, which will help separate it from the first map when we put them together. We will also get the map to cover a slightly larger area than it would by default by using the scale_x_continuous() and scale_y_continuous() functions.\n\nhungerford_map_town &lt;- ggplot() +\n  annotation_map_tile(zoomin = 0, progress = \"none\") +\n  # Plot lines between points\n  geom_curve(\n    aes(x = x_start, y = y_start, xend = x_end, yend = y_end),\n    data = slice(hungerford_lines, 3:n()), \n    arrow = arrow(length = unit(3, \"mm\"), type = \"closed\"),\n    curvature = -0.2,\n    colour = \"orange3\"\n  ) +\n  # Plot points\n  geom_sf(\n    data = slice(hungerford_shootings_sf, 3:n()), \n    shape = 21,\n    colour = \"white\",\n    fill = \"orange3\",\n    size = 3\n  ) +\n  geom_label_repel(\n    aes(x = easting, y = northing, label = order),\n    data = slice(hungerford_shootings, 3:n()), \n    colour = \"white\",\n    fill = \"orange3\",\n    fontface = \"bold\",\n    label.size = NA\n  ) +\n  annotation_scale(style = \"ticks\", line_col = \"grey40\", text_col = \"grey40\") +\n  scale_x_continuous(expand = expansion(0.3)) +\n  scale_y_continuous(expand = expansion(0.3)) +\n  coord_sf(crs = \"EPSG:27700\") +\n  labs(\n    title = \"Shootings in Hungerford town\"\n  ) +\n  theme_void() +\n  theme(\n    panel.border = element_rect(colour = \"grey20\", fill = NA),\n    plot.margin = margin(t = 12)\n  )\n\nhungerford_map_town\n\n\n\n\n\n\n\n\nWe can now combine the two maps using the patchwork package. To place two plots next to each other using patchwork you just add one map object to the other using the | operator. To place one plot on top of the other, you use the / operator (you can combine plots in more-complicated ways by combining these operators, together with parentheses).\n\nlibrary(patchwork)\n\n(hungerford_map_overall / hungerford_map_town)\n\n\n\n\n\n\n\n\nWhen we created the hungerford_map_overall object in the previous section, we gave the map title a negative border using the plot.title argument to the theme() function. This had the effect of moving the title downwards onto the map to save space, since the top-left corner of the map was empty. We also added a scale bar, since when you present two related maps of different scales next to one another, it is useful to show the scales of both maps to help readers relate them to one another.\nWe can add shared titles and captions to combined maps created with patchwork using the plot_annotation() function. This function has a theme argument, which we can use to change the appearance of the shared title and caption just as we use theme() to change the appearance of elements on single maps.\n\n(hungerford_map_overall / hungerford_map_town) + \n  plot_annotation(\n    title = \"Shootings during the Hungerford massacre\",\n    caption = \"data from the official report into the shootings\",\n    theme = theme(\n      plot.caption = element_text(colour = \"grey40\", hjust = 0),\n      plot.title = element_text(colour = \"grey50\", face = \"bold\", size = 14)\n    )\n  )\n\n\n\n\n\n\n\n\nThere are various other improvements that we could make to this map based on the mapping skills we have already learned during this course. For example, we could add the locations of buildings or particular facilities using data from OpenStreetMap. What information we choose to present on the map will depend on what information we think it is important to communicate to our audience.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Mapping crime series</span>"
    ]
  },
  {
    "objectID": "13_crime_series/index.html#putting-it-all-together",
    "href": "13_crime_series/index.html#putting-it-all-together",
    "title": "13  Mapping crime series",
    "section": "13.4 Putting it all together",
    "text": "13.4 Putting it all together\n\nIn this tutorial we have learned how to map crime series, focusing on displaying a sequence of events on a map. We have also learned how to combine maps using the patchwork package.\n\nWhen we are mapping crime data, it is important to remember that the rows in the data we are using often represent traumatic events that have happened to people. In this tutorial, every row in the data represents one or more people who were killed or injured during the massacre. The people killed during the tragedy were Marcus Barnard, Roger Brereton, Francis Butler, Kenneth Clements, Victor Gibbs, Myrtle Gibbs, Susan Godfrey, Sandra Hill, Abdur Khan, Roland Mason, Sheila Mason, Ian Playle, Dorothy Ryan, Eric Vardy, Douglas Wainwright and George White.\n\n\n\nYou can put the code from this tutorial together to see everything that is needed to make a composite map of the Hungerford massacre from the original data file called hungerford_shootings.\n\nlibrary(ggrepel)\nlibrary(ggspatial)\nlibrary(patchwork)\nlibrary(sf)\nlibrary(tidyverse)\n\n\n\n# Prepare data -----------------------------------------------------------------\n\n# Create lines showing progression between points\nhungerford_lines &lt;- hungerford_shootings |&gt; \n  rename(x_end = easting, y_end = northing) |&gt; \n  mutate(x_start = lag(x_end), y_start = lag(y_end)) |&gt; \n  # Remove the first line, which contains missing values and which we don't need\n  remove_missing(vars = c(\"x_start\", \"y_start\"), na.rm = TRUE)\n\n# Convert the `hungerford_shootings` object to SF\nhungerford_shootings_sf &lt;- st_as_sf(\n  hungerford_shootings, \n  coords = c(\"easting\", \"northing\"), \n  crs = \"EPSG:27700\"\n)\n\n\n\n# Make component maps ----------------------------------------------------------\n\nhungerford_map_overall &lt;- ggplot() +\n  # Plot base map\n  annotation_map_tile(zoomin = 0, progress = \"none\") +\n  # Plot lines between points\n  geom_curve(\n    aes(x = x_start, y = y_start, xend = x_end, yend = y_end),\n    data = hungerford_lines, \n    arrow = arrow(length = unit(3, \"mm\"), type = \"closed\"),\n    curvature = -0.2,\n    colour = \"orange3\"\n  ) +\n  # Plot points\n  geom_sf(\n    data = hungerford_shootings_sf, \n    shape = 21,\n    colour = \"white\",\n    fill = \"orange3\",\n    size = 3\n  ) +\n  geom_label_repel(\n    aes(x = easting, y = northing, label = order),\n    data = filter(hungerford_shootings, order %in% 1:2), \n    colour = \"white\",\n    fill = \"orange3\",\n    fontface = \"bold\",\n    label.size = NA\n  ) +\n  annotation_scale(style = \"ticks\", line_col = \"grey40\", text_col = \"grey40\") +\n  scale_y_continuous(expand = expansion(2)) +\n  labs(title = \" Shootings in Wiltshire\") +\n  coord_sf(crs = \"EPSG:27700\") +\n  theme_void() +\n  theme(\n    panel.border = element_rect(colour = \"grey20\", fill = NA),\n    plot.title = element_text(margin = margin(b = -18))\n  )\n\nhungerford_map_town &lt;- ggplot() +\n  annotation_map_tile(zoomin = 0, progress = \"none\") +\n  # Plot lines between points\n  geom_curve(\n    aes(x = x_start, y = y_start, xend = x_end, yend = y_end),\n    data = slice(hungerford_lines, 3:n()), \n    arrow = arrow(length = unit(3, \"mm\"), type = \"closed\"),\n    curvature = -0.2,\n    colour = \"orange3\"\n  ) +\n  # Plot points\n  geom_sf(\n    data = slice(hungerford_shootings_sf, 3:n()), \n    shape = 21,\n    colour = \"white\",\n    fill = \"orange3\",\n    size = 3\n  ) +\n  geom_label_repel(\n    aes(x = easting, y = northing, label = order),\n    data = slice(hungerford_shootings, 3:n()), \n    colour = \"white\",\n    fill = \"orange3\",\n    fontface = \"bold\",\n    label.size = NA\n  ) +\n  annotation_scale(style = \"ticks\", line_col = \"grey40\", text_col = \"grey40\") +\n  scale_x_continuous(expand = expansion(0.3)) +\n  scale_y_continuous(expand = expansion(0.3)) +\n  coord_sf(crs = \"EPSG:27700\") +\n  labs(\n    title = \"Shootings in Hungerford town\"\n  ) +\n  theme_void() +\n  theme(\n    panel.border = element_rect(colour = \"grey20\", fill = NA),\n    plot.margin = margin(t = 12)\n  )\n\n\n\n# Combine maps and add titles --------------------------------------------------\n\n(hungerford_map_overall / hungerford_map_town) + \n  plot_annotation(\n    title = \"Shootings during the Hungerford massacre\",\n    caption = \"data from the official report into the shootings\",\n    theme = theme(\n      plot.caption = element_text(colour = \"grey40\", hjust = 0),\n      plot.title = element_text(colour = \"grey50\", face = \"bold\", size = 14)\n    )\n  )\n\n\n\n\n\n\n\n\n\nPhotograph of PC Roger Brereton’s funeral from Berkshire Live\n\n\nArtwork by @allison_horst",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Mapping crime series</span>"
    ]
  },
  {
    "objectID": "14_writing_reports/index.html",
    "href": "14_writing_reports/index.html",
    "title": "14  Writing reports in R",
    "section": "",
    "text": "14.1 Introduction\nLearn how to write reports directly in RStudio that integrate text, data, tables and charts using Quarto.\nWe already know how to produce useful maps in R so that people can use them to make decisions about understanding and responding to crime problems. But maps are only one part of most spatial analysis. In practice, maps are usually part of a larger report in which you as an analyst will explain what your maps show and perhaps make recommendations about what decisions people should make.\nYou are probably used to creating graphics in one piece of software (such as a map in R or a chart in Excel) and then importing or pasting those graphics into another program (such as Word) to including them in a written report. This way of working is often fine, but it has some shortcomings. For example, if you want to make a change to a graphic (maybe to correct a typo), you then have to import it into your writing program again. More importantly, it can become hard to keep track of which version of a graphic you need to import and you may end up including the wrong version of a file.\nThe same problem applies to numbers that you might calculate in statistical software such as R. You might calculate a statistic such as the mean number of burglaries in local council wards, then paste the result into Word to include it in your report. But if you then realise later on that there is a problem with your data and re-run your code, it would be very easy (especially in a long report) to forget that you needed to update the mean value presented in the report. The risk of making errors like this is particularly high if you are asked to update an existing report based on new data. Perhaps the worst aspect of this problem is that you will never know if the numbers in your final report are correct, unless you go back and check every one of them in whatever program you used to generate the numbers in the first place.\nWe can describe this risk of a report containing obsolete charts or incorrect statistics by saying that is is not reproducible – if we were asked to go back and demonstrate each stage in producing the report to prove that we had done everything correctly, it would be very hard to do so. This is important because the reports that analysts write about crime are so often used to make decisions about how to respond to crime. An error in copying and pasting a number from R or Excel into Word could lead to police officers being deployed to the wrong place, or the wrong local council being given funding to install crime-prevention measures.\nIn this tutorial we will learn to use a tool called Quarto to write reports directly in RStudio, integrating data, maps and statistics directly into reports so that they are always up-to-date. Quarto can be used to produce detailed and polished reports in a way that makes sure they are always up to date. For example, the report Stop and Search in London was created entirely in Quarto.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Writing reports in R</span>"
    ]
  },
  {
    "objectID": "14_writing_reports/index.html#markdown",
    "href": "14_writing_reports/index.html#markdown",
    "title": "14  Writing reports in R",
    "section": "14.2 Markdown",
    "text": "14.2 Markdown\nMarkdown is a way of formatting plain text so that a computer can convert it into different file formats. For example, RStudio can convert Markdown documents into Word, PDF, PowerPoint and many other formats.\n\n\n\nMarkdown uses plain-text characters to represent formatting such as titles, italic text and so on. For example, the Markdown text:\n# Introduction\n\nIn this tutorial we will learn to use a tool called *Quarto* to write reports directly in RStudio.\nproduces the output:\n\n\nIntroduction\n\n\nIn this tutorial we will learn to use a tool called Quarto to write reports directly in RStudio.\n\n\nIn this example, the character # followed by a space at the start of a line mean that line should be shown as a first-level heading and the asterisks (* – we could also have used the underscore character _) around the word Quarto indicate that it should be emphasised (typically with italic text).\n\nIn Markdown we use the hash character # followed by a space to indicate that a line should be formatted as a top-level heading. The space after the # is important – if you write #Introduction instead of # Introduction then R will not recognise that line as representing a heading and it will not be formatted properly.\n\nMarkdown is designed to be easy to read and easy to write. It is perfectly possible, for example, to read the unformatted Markdown text in the example above. Since Markdown files (which have the file extension .md) are plain text, it’s also possible to open them on virtually any computer. Markdown is very widely used on the web – using underscores to mark out italic text and asterisks to mark out bold text even works in messaging apps such as Telegram and WhatsApp.\nIn Markdown you describe the structure of a document, not its appearance. The appearance of the document (which fonts it uses, what margins the pages have and so on) are determined by the templates that RStudio uses to convert markdown files into documents of different types. This can save you a lot of time, because you don’t need to specify fonts and other formatting. Instead, you can concentrate on the structure of your argument rather than the details of formatting.\nYou can create your own templates for documents (for example if you want to create documents that match a particular corporate style) but in this course we will use the templates built into RStudio, which are designed to create documents that communicate data effectively.\n\n14.2.1 Markdown document structure\nTo create paragraphs in Markdown, you simply split text with a blank line. So this Markdown text:\nIn this tutorial we will learn to use a tool called _Quarto_ to write reports directly in RStudio.\nMarkdown is a way of formatting plain text so that a computer can convert it into different file formats.\nproduces the output:\n\nIn this tutorial we will learn to use a tool called Quarto to write reports directly in RStudio. Markdown is a way of formatting plain text so that a computer can convert it into different file formats.\n\nbecause there is no blank line between the first and second sentences. We can split this into two paragraphs just by adding a blank line:\nIn this tutorial we will learn to use a tool called _Quarto_ to write reports directly in RStudio.\n\nMarkdown is a way of formatting plain text so that a computer can convert it into different file formats.\nwhich produces the output:\n\nIn this tutorial we will learn to use a tool called Quarto to write reports directly in RStudio.\nMarkdown is a way of formatting plain text so that a computer can convert it into different file formats.\n\n\n14.2.1.1 Headings\nThere are six levels of headings available in Markdown documents, although it is very unlikely that you will need all six. Headings are specified by adding one or more # characters to the start of the line, followed by a space:\n# First-level heading\n\n## Second-level heading\n\n### Third-level heading\nproduces:\n\n\nFirst-level heading\n\n\nSecond-level heading\n\n\nThird-level heading\n\n\nHeadings should have at least one blank line above and below them, so that they stand out from the surrounding code. I usually leave three blank lines before a second-level heading and two blank lines before other headings, with exactly one blank line after every type of heading.\n\nRemember that Markdown describes the structure of a document, not its appearance, so you should not choose a heading level based on what size of text it produces in a particular output format. It is often best to save the first-level heading for your document title, so break the document up into sections by specifying second-level headings and then break those sections up into sub-sections with third-level headings etc.\n\n\n\n14.2.1.2 Lists\nMarkdown supports two types of list: ordered lists and unordered lists. You can make an ordered list by putting each list item on a new line and starting each line with a number followed by a full stop (.). You make an unordered list in the same way, but starting each line with an asterisk:\n1. A list of items\n2. for which the ordering\n3. of items is important\n\n* A list of items\n* for which the ordering\n* of items is *not* important\nproduces:\n\n\nA list of items\nfor which the ordering\nof items is important\n\n\nA list of items\nfor which the ordering\nof items is not important\n\n\n\n\n14.2.1.3 Quotes\nIf you want to insert a quote into your Markdown document, you can do that by putting a greater-than (&gt;) symbol followed by a space at the start of each line of the quote:\n&gt; In this tutorial we will learn to use a tool called _Quarto_ to write reports directly in RStudio.\nproduces\n\n\nIn this tutorial we will learn to use a tool called Quarto to write reports directly in RStudio.\n\n\n\n\n\n14.2.2 Inline elements\nAs well as using Markdown to describe the structure of a document, you can use it to mark up particular text within a paragraph. We’ve already seen how to do this using _to emphasise text_ (usually displayed in italics). We can also **strongly emphasise** text, which will usually appear in bold. Note that the _, * or ** characters must be touching a word on exactly one side:\n\nsome _emphasised_ text produces some emphasised text\nsome_emphasised_text does not produce emphasised text\nsome _ emphasised _ text does not produce emphasised text\n\nWe can add links to a document using the format [link text](URL). For example, the text:\n[learn about the tidyverse](https://www.tidyverse.org/)\nproduces the link:\n\nlearn about the tidyverse\n\nThere are several other Markdown codes for describing different elements within a document, including images, videos and segments of code. You can find out more about what’s possible with Markdown on the Markdown Basics page of the Quarto website.\n\n\n14.2.3 Processing Markdown in RStudio\nOnce you’ve written a Markdown document, you can convert it into a web page, a Word document or a PDF file from within RStudio. First, save your document with a .md file extension – this tells RStudio that the file is a Markdown document. You should now see a new button at the top of the RStudio script panel marked Preview.\n\n\n\nIf you click on the small downward-pointing triangle next to that button, you have the option to produce your report in HTML (web page) format, PDF or Microsoft Word. HTML format is useful for creating documents to be published on the web, but also for quickly previewing documents in RStudio while you are working on them. Click on the relevant option to create the document in your format of choice.\nYou do not need to have Microsoft Word installed on your computer to produce Word documents from Markdown files in RStudio, although you will need Word (or other word-processing software) to view the file.\nTo create PDF files from Markdown in RStudio, your computer has to have a version of software called TeX installed. If you’ve never heard of TeX, you can install it automatically using the tinytex R package. To install TeX, just run this R code once on your computer:\ninstall.packages(\"tinytex\")\ntinytex::install_tinytex()\nTeX should now be installed on your computer. After you have restarted RStudio you will be able to create PDFs from within RStudio.\n\nNote that you only have to install TeX once on each computer you use, so you should not include tinytex::install_tinytex() in any Quarto scripts that you write.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Writing reports in R</span>"
    ]
  },
  {
    "objectID": "14_writing_reports/index.html#quarto",
    "href": "14_writing_reports/index.html#quarto",
    "title": "14  Writing reports in R",
    "section": "14.3 Quarto",
    "text": "14.3 Quarto\nMarkdown allows you to create static documents in RStudio. We can add all types of R output – including maps, tables, charts and the results of calculations – to Markdown documents using Quarto, which is built into RStudio.\nQuarto is a system that converts Markdown documents that contain chunks of code written in R or Python, runs the code and then integrates the Markdown text and the code results into one or more output files. Quarto can produce web pages, Word documents, PDF files, presentations, websites, e-books and other formats.\nA Quarto file is just like a Markdown document except that it has the file extension .qmd rather than the extension .md. The .qmd extension tells RStudio that a file will contain a mixture of text formatted with Markdown and code that produces tables, charts and so on.\nSince Quarto can produce many types of document, RStudio has a dialogue box in which you can select from all the installed Quarto templates. Click on the ‘new file’ button in the top-left corner of the RStudio window and then on Quarto Document…\n\n\n\nThis will open a box containing all the Quarto templates that are installed on your computer. Some of these are installed by default and some may have been added by packages that you have installed.\n\n\n\nYou can edit Quarto documents in two ways in RStudio. When you create a new Quarto document, RStudio will open it in the Visual editor. This allows you to write the text content of your document as you would in word-processing software such as Word. For example, you can make text bold using buttons on the tool bar at the top of the document, or using standard keyboard short-cuts such as Ctrl+B on Windows or Command+B on Mac. There is also a Source editor, which allows you to edit the underlying Markdown formatted text directly.\nIn this tutorial we will use the Visual editor, but you can choose to use the Source editor if you prefer. To switch between the two editors, slick the Source or Visual buttons at the left-hand end of the formatting tool bar at the top of the Quarto document window.\nWhichever editor you use, Quarto documents start with a header that provides some basic information about the document, such as the title. A simple Quarto header looks like this:\n---\ntitle:  \"An Quarto document\"\nauthor: \"Joe Bloggs\"\ndate:   \"3 March 2021\"\n---\nThe header is written in yet another programming language called YAML. You don’t need to know the details of YAML to write headers for Quarto documents. Two things you do need to know, though:\n\nThe three dashes (---) are important, because they tell RStudio that the content inside the dashes is the document header. The dashes must appear at the start of a line on their own.\nIndentation matters in YAML. Every line takes the form key: value and in most cases the key must be at the very start of the line. Lines in YAML are not limited to 80 characters, so you should not break a single value (such as the document title) over multiple lines.\n\n\n14.3.1 R code in Quarto\nQuarto will process everything in an Quarto document after the header (marked with ---) as Markdown text. The only exception to this is when you include sections of code in an Quarto document.\nYou can include R code inside a line of Markdown text (known as inline code) and the result of that code will be included in the document output. To do this in the visual editor, you type a lower-case r followed by a space, then the code you want to run. You then select the text from the letter r to the end of the code, open the Format menu in the Quarto tool bar and select Code (alternatively you can select the text and use the keyboard short-cut Command+D on Mac or Ctrl+D on Windows).\nFor example, if you wanted to insert today’s date into a Quarto document, you can type:\n\n\n\nWhich would produce the output:\n\n\n\nPutting R code inline is fine for simple pieces of code, but longer pieces of code included inline would become difficult to read (and therefore difficult to debug). Fortunately, we can put as much code as we like in a code block. To add a block of R code to an Quarto document, we open the Insert menu on the Quarto tool bar, click Code Chunk and then R. You will then see a grey box appear at the point in your document where you had placed the cursor, with the text {r} at the start of the box.\nWe can use code chunks to run longer pieces of code that would be difficult to read if the code were inline. For example, if you wanted to load a data file of crime data, filter it for crimes occurring in a particular month and then count the number of rows, you could do the calculation in a code chunk and then include the result inline:\n\n\n\nNote that to make the Quarto document more readable, RStudio has given the R code chunk a grey background, in contrast to the white background of the rest of the document. If a code chunk does not have a grey background, this is a good sign that there is something wrong: maybe you haven’t saved the file with a .qmd file extension, or maybe you have not inserted the code block correctly.\nCode chunks can also be used to insert tables and charts (including maps) into a Quarto document. To add a nicely formatted table of data, use the kable() function from the knitr library. For example:\n# Print table of crime counts in each police division\ncrimes |&gt; \n  filter(year == 2021, month == \"February\") |&gt; \n  count(division) |&gt; \n  # We haven't loaded the `knitr` package, so we need to refer to the `kable()`\n  # function using the `::` operator\n  knitr::kable()\nWe can also use code chunks to produce graphics. In this case, you just include the code to produce your map or chart in a code chunk as you normally would.\nTo help keep track of code chunks, we can name them. To name a code chunk, put a single space after the letter r in the line that begins the chunk and then give the chunk name. Names can include letters, numbers and dashes (-). For example, if we wanted to call the chunk above count-all-crimes we would start the code chunk with the code {r count-all-crimes}.\nThe package that converts Quarto documents into other formats is called knitr – actually, it’s a bit more complicated than that, but one of the nice things about Quarto is you don’t need to worry about what’s happening behind the scenes.\n\n\n\nThe default Quarto template is set up so that the final document that is produced from your Quarto file will include both the code that you include in any code chunks and the output that the code produces. For example, this Quarto document:\n\n\n\nProduces this output, which is probably not what you want:\n\n\n\nWe can control the results of code chunks using chunk options, sometimes called knitr options. We put these on the first line of each code chunk, with each option on a separate line and each line starting with the characters #| (referred to as a hash-pipe). Note that the #| must be followed by a space for Quarto to recognise it as a chunk option.\nTo specify that the code in our R code chunks should not be printed in the final document, we can set the chunk option #| echo: false. To specify that both the R code and the output produced by that code in each code chunk (e.g. and messages or warnings) should not be shown, we can set the chunk option #| include: false.\nIf we change the code in our previous example to include this line, the output becomes much more like the report we want:\n\n\n\nIf a code chunk produces a chart or map then we do want to show the output (although not the code), so in that case we should not set #| include: false – we do not need to set #| include: true because it is the default.\nSince we probably don’t want any chunks in our code to include the code in our final document, we could end up setting the chunk option #| echo: false for every code chunk. In a long or complicated document, this would get tedious. Fortunately, we can set chunk options globally (i.e. for all chunks in a document) by setting them in the YAML document header. To do this, we add a key called execute to the header. Instead of giving the execute key a value such as true or false, we instead indent the next line of the header by exactly two spaces and then set the chunk option echo: false. If we wanted to set multiple global chunk options here, we would put each one on a new line, all of the lines indented by two spaces from the start of each document.\n---\ntitle:  \"An Quarto document\"\nauthor: \"Joe Bloggs\"\ndate:   \"3 March 2021\"\nexecute:\n  echo: false\n---\nSome of the R code we write produces messages or warnings that we will not want to include in our Quarto documents. For example, readers of a report based on crime data are unlikely to want to see the warning produced by the st_intersection() function, and will probably be quite surprised if this appears in their code. To hide these messages, we can split the code needed to produce a map into two code chunks: one to do the pre-processing that often creates messages or warnings, and a second chunk to create the map itself.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Writing reports in R</span>"
    ]
  },
  {
    "objectID": "14_writing_reports/index.html#rendering-quarto",
    "href": "14_writing_reports/index.html#rendering-quarto",
    "title": "14  Writing reports in R",
    "section": "14.4 Rendering Quarto",
    "text": "14.4 Rendering Quarto\nOnce you’ve written the text for your document and added the code needed to produce statistics, tables and figures, it’s time to convert your Quarto file to a document in the format you need. This process is called rendering.\nIn RStudio you can render a Quarto document by clicking the Render button (if you can’t see the Render button at the top of your Quarto document, that is a sign you haven’t saved the file as a .qmd file). By default, RStudio will produce an HTML file (a web page) and open it in the HTML viewer built into RStudio. You can produce other formats instead of a web page by clicking on the downward-pointing arrow next to the Render button and then choosing the format you want.\nYou can control the output format that RStudio will produce by changing the format: value in the YAML header of your Quarto document. The default is format: html, but you can also use format: pdf to produce a PDF file, format: docx to produce a Word document, format: epub to produce an e-book, format: pptx to produce a PowerPoint presentation. There are many other ouput formats available.\n\nIf you have a choice of which format to produce a report in, I would suggest you choose HTML because it can do things (like include interactive maps made with leaflet) that Word and PDF format cannot, and can easily be read on mobile devices. If, for whatever reason, you cannot choose HTML, I recommend choosing PDF format since it is readable on almost all computers, even those that do not have Word installed on them. Only produce reports in Word format if you are specifically required to submit something in that format.\n\nWhatever format you choose, the output file will be saved in the same folder on your computer that you saved your .qmd file in, and will have the same file name but with a different file extension (.html, .pdf, .docx, etc.). If you render the document again, RStudio will overwrite the previous output file.\nWhen you render a Quarto document, RStudio will run all the code in the file so that it can knit the results together with the document text. This is similar to running the code in a .R file using the Source button, except that when RStudio runs the code in a Quarto document, RStudio can only access functions from the packages that are loaded in the Quarto document itself, and only make use of objects created inside the Quarto document. This means that nothing you do in the console or in any other files you have open in RStudio (e.g. loading a package or creating an object) affects the results that will be produced when you render a Quarto document.\nSince rendering a Quarto document involves running all the code contained in it, it is possible that the code will cause errors at the time when the Quarto document is rendered. When this happens, the R error message will appear in the RStudio Render panel. When you run a .R file that produces an error, the error message typically tells you which line of code the error occurred on. But when an error occurs while rendering a Quarto file, the error message only specifies the first and last lines of the part of the document in which the error occurred. If the error occurred inside a code chunk, the error message will refer to the first and last lines of that chunk, but the error itself could have occurred anywhere inside that chunk. If the error occurs outside a code chunk, the error message will report that the error occurred somewhere between the last line of the previous code chunk and the first line of the next code chunk. It is up to you to find the exact location of the error.\n\n14.4.1 Making self-contained HTML documents\nWeb pages written in HTML typically make use of images, videos and other resources stored in separate files. In fact, the web page you’re reading now makes use of resources stored in 51 separate files. This works well on the web, since it allows resources to be shared between lots of HTML files, which reduces how much data has to be stored and transmitted across networks. But it works less well when you want to send an HTML report to someone else over email, since it can be hard to keep track of all the external files associated with a particular HTML file.\nTo deal with this problem, we can tell Quarto that when it creates an HTML report, all the necessary data and other resources should be embedded inside the HTML file itself. This makes the HTML file larger, but easier to manage. To make HTML reports produced by Quarto self-contained in this way, we can change the format section of the header of a Quarto document from simply format: html to instead specify exactly what settings we want to be used to create that HTML file:\nformat:\n  html:\n    embed-resources: true\nThere are lots of other ways we can use the format section of the header in a Quarto document to fine-tune how the resulting document works. You can find out more in the HTML section of the Quarto website.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Writing reports in R</span>"
    ]
  },
  {
    "objectID": "14_writing_reports/index.html#in-summary",
    "href": "14_writing_reports/index.html#in-summary",
    "title": "14  Writing reports in R",
    "section": "14.5 In summary",
    "text": "14.5 In summary\n\nIn this tutorial we have learned how to use Markdown to create structured text using simple formatting codes and how to integrate that text with our code to produce integrated reports that include the results of our analysis. This means we can make our work reproducible so that we can avoid all the mistakes that can happen if we have to copy and paste statistics and graphics into reports written in software like Word.\nIt also means we can produce periodic reports with updated data very easily, since we can just choose the data we want the report to be based on using filter() at the start of our file and render the document. This can save a huge amount of time in producing reports such as performance bulletins or monthly summaries of crime in an area.\n\nTo get used to working with Quarto files, download this example report to your computer and open it in RStudio. Now make the following changes:\n\nChange the global chunk options so that the code used in each code chunk is not shown in the final document.\nRender the file into documents of at least two different formats (e.g. HTML and Word). If you have installed tinytex on your computer so that you can create PDF documents, look at how different the document appears in PDF format compared to HTML format because of the different templates used.\n\nThere is a lot more you can do with Quarto. To find out more, refer to these resources.\n\nR for Data Science chapter on Quarto.\nA gallery of different documents, websites and presentations written with Quarto.\n\n\nArtwork by @allison_horst",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Writing reports in R</span>"
    ]
  },
  {
    "objectID": "15_no_maps/index.html",
    "href": "15_no_maps/index.html",
    "title": "15  Presenting spatial data without maps",
    "section": "",
    "text": "15.1 Introduction\nLearn how to use tables and charts to effectively communicate information about spatial data, and when these techniques might be more effective than using maps.\nMaking maps is the core of analysing spatial data. But just because a particular dataset has a spatial element to it does not mean that a map is the best way to present that data. In this tutorial we will learn some other techniques for presenting data that can be more effective than maps for answering certain questions about spatial data.\nAs with so much in spatial analysis, whether it is best to make a map or use some other technique to convey information will depend on the circumstances. When you decide how to communicate information about the data you are analysing, you will need to consider the questions you are trying to answer, the audience that you are communicating to, what they will be using the information for and in what circumstances they will be using it.\nWhile the best choice of how to communicate spatial information will depend on the circumstances, there are a few instances in which maps are typically not the best way to communicate your data. These are:",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Presenting spatial data without maps</span>"
    ]
  },
  {
    "objectID": "15_no_maps/index.html#introduction",
    "href": "15_no_maps/index.html#introduction",
    "title": "15  Presenting spatial data without maps",
    "section": "",
    "text": "15.1.0.1 When you only need to convey a handful of pieces of information\nMaps are very effective for communicating detailed information, such as the density of crime across thousands of cells in a KDE grid. But to do this, maps typically encode information into aesthetics such as colour, size and so on. This is necessary for communicating large amounts of information, but it makes the connection between the data and the visual representation of the data less direct. If you only need to communicate a small amount of information, there is less justification for forcing your audience to mentally translate the aesthetic into whatever it represents.\nFor example, if you wanted to show the number of violent and sexual offences in each of the seven districts in Northamptonshire in England, a choropleth map is less clear than a bar chart (for example, in being able to decipher if there were more offences in Kettering or in Wellingborough).\n\n\n\nA map might be a useful addition to the bar chart in this case if you are trying to communicate information to people who are not familiar with the locations of the districts. In that case, we might want to add a small reference map to help people understand which area is which:\n\n\n\nBut in most circumstances in which you create crime maps, you will be creating them for an audience (such as local police officers) that already has sufficient knowledge of the area and so an inset map such as this would not be needed. In that case, a bar chart will probably be more effective at showing this information than a map would be.\n\n\n15.1.0.2 When you need to convey several different things about one place\nMaps are generally most effective when they show a single piece of data about each place. For example, a choropleth map shows a single shade of colour for each area on the map to represent a single value, such as the frequency of crime in that area. If you wanted to show the frequency of burglary and the frequency of robbery in the same area on a map, this would be quite hard. So if you need to convey multiple different things about each place, it is generally best to do this in a table or chart, rather than a map.\nOne exception to this is when you present multiple maps side by side, each showing a single thing about an area. These are called small multiple maps and we will learn about them in a future tutorial.\n\n\n15.1.0.3 When the geographic relationship between places on the map is not the most important thing about them\nMaps emphasise the spatial relationship between different places, but they do this at the expense of making non-spatial relationships between those places less obvious. If the spatial relationships are the most important thing that you want to convey, a map makes sense. For example, a hotspot map is often a very good way to communicate where crime is most concentrated. But in other cases the geographic relationships between variables will be much less important. For example, if you wanted to show the relationship between the amount of crime in an area and the level of poverty there, a scatter plot would probably be a more-effective way to do this than a map would be.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Presenting spatial data without maps</span>"
    ]
  },
  {
    "objectID": "15_no_maps/index.html#tables",
    "href": "15_no_maps/index.html#tables",
    "title": "15  Presenting spatial data without maps",
    "section": "15.2 Tables",
    "text": "15.2 Tables\nWell-designed tables can be a very effective way of communicating information, whereas badly-design tables can be confusing and even lead your audience to give up trying to engage with the information you’re trying to communicate.\nTables used to present information almost always show only a summary of the available data, so the first step in preparing a table is to wrangle the data into the right format. In a previous tutorial we learned about the summarise() function from the dplyr package that we can use to produce summaries of rows of data.\nTo learn about creating a good table for displaying summary data in a report, we will use the example of the frequency of different types of violence in the different states of Malaysia in 2017. This data is already loaded in an object called violence. We can get a feel for the data by looking at a random sample of rows using the slice_sample() function function from the dplyr package (remember dplyr is loaded automatically by tidyverse).\n\nlibrary(tidyverse)\n\nslice_sample(violence, n = 10)\n\n# A tibble: 10 × 5\n   region        state            year crime_type         count\n   &lt;chr&gt;         &lt;chr&gt;           &lt;dbl&gt; &lt;chr&gt;              &lt;dbl&gt;\n 1 West Malaysia Johor            2017 murder                66\n 2 West Malaysia Selangor         2017 murder                83\n 3 West Malaysia Melaka           2017 aggravated assault   176\n 4 West Malaysia Kelantan         2017 unarmed robbery      219\n 5 West Malaysia Pahang           2017 murder                16\n 6 West Malaysia Terengganu       2017 armed robbery          0\n 7 West Malaysia Kuala Lumpur     2017 unarmed robbery     3175\n 8 West Malaysia Kedah            2017 armed robbery          2\n 9 East Malaysia Sabah            2017 armed robbery          0\n10 West Malaysia Negeri Sembilan  2017 murder                14\n\n\nThe output of slice_sample() looks acceptable as a table, especially if it is included in a Quarto document, but readers of our reports probably don’t want to know the type of each variable (underneath the variable names) and won’t want to page through the table if there are more rows or columns than can fit in the available space.\n\n15.2.1 Making data wider for presentation\nOne issue with printing the violence object as a table is that it has 70 rows, so it will take up a lot of space on a page or screen. We can make the data more compact by converting it from long format to wide format. In the previous tutorial on tidying messy data we learned that data are often easier to analyse in long format. But it is often better to present data in a table in wide format.\nTo convert the table to a wider format we can use the pivot_wider() function from the tidyr package, just was we used the corresponding pivot_longer() function to tidy data in the messy-data tutorial. To make data wider, we specify a single column in the data to use as the names of multiple new columns using the names_from argument and a column to use as the values for the new columns using the values_from argument.\n\npivot_wider(violence, names_from = crime_type, values_from = count)\n\n# A tibble: 14 × 8\n   region        state    year `aggravated assault` `armed robbery` murder  rape\n   &lt;chr&gt;         &lt;chr&gt;   &lt;dbl&gt;                &lt;dbl&gt;           &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;\n 1 East Malaysia Sabah    2017                  230               0     36   211\n 2 East Malaysia Sarawak  2017                  368               3     27   150\n 3 West Malaysia Johor    2017                  614               1     66   196\n 4 West Malaysia Kedah    2017                  364               2     21   119\n 5 West Malaysia Kelant…  2017                  252               2     13   114\n 6 West Malaysia Kuala …  2017                  651               4     37   132\n 7 West Malaysia Melaka   2017                  176               1      7    69\n 8 West Malaysia Negeri…  2017                  241               2     14    91\n 9 West Malaysia Pahang   2017                  188               1     16   163\n10 West Malaysia Perak    2017                  380               4     35    95\n11 West Malaysia Perlis   2017                   47               0      2    30\n12 West Malaysia Pulau …  2017                  275               0     17    80\n13 West Malaysia Selang…  2017                 1108              14     83   321\n14 West Malaysia Tereng…  2017                  130               0      5    64\n# ℹ 1 more variable: `unarmed robbery` &lt;dbl&gt;\n\n\nNow the table has only 14 rows, which makes it much easier to present both on screen and in print. We can also see that the year column is constant (all the values are the same), so we can remove this using the select() function from dplyr. We can also use select() to change the order of the columns from left to right so that the two types of robbery appear next to each other.\n\nviolence |&gt; \n  pivot_wider(names_from = crime_type, values_from = count) |&gt; \n  select(\n    region, state, murder, rape, `aggravated assault`, `armed robbery`, \n    `unarmed robbery`\n  )\n\n# A tibble: 14 × 7\n   region        state         murder  rape `aggravated assault` `armed robbery`\n   &lt;chr&gt;         &lt;chr&gt;          &lt;dbl&gt; &lt;dbl&gt;                &lt;dbl&gt;           &lt;dbl&gt;\n 1 East Malaysia Sabah             36   211                  230               0\n 2 East Malaysia Sarawak           27   150                  368               3\n 3 West Malaysia Johor             66   196                  614               1\n 4 West Malaysia Kedah             21   119                  364               2\n 5 West Malaysia Kelantan          13   114                  252               2\n 6 West Malaysia Kuala Lumpur      37   132                  651               4\n 7 West Malaysia Melaka             7    69                  176               1\n 8 West Malaysia Negeri Sembi…     14    91                  241               2\n 9 West Malaysia Pahang            16   163                  188               1\n10 West Malaysia Perak             35    95                  380               4\n11 West Malaysia Perlis             2    30                   47               0\n12 West Malaysia Pulau Pinang      17    80                  275               0\n13 West Malaysia Selangor          83   321                 1108              14\n14 West Malaysia Terengganu         5    64                  130               0\n# ℹ 1 more variable: `unarmed robbery` &lt;dbl&gt;\n\n\n\n\n15.2.2 Using the gt package to make better tables\n\nWe can create much better display tables with the gt package, which is designed to format data for display. The gt package works in a similar way to the ggplot2 package, in that tables are made up of stacks of functions that contribute to the appearance of the final table. One difference is that the layers in a gt stack are joined using the pipe operator (|&gt;) rather than the plus operator (+).\nWe can create a very basic gt table by just passing a data frame or tibble to the gt() function. So we can add gt() to the end of the pipeline of functions we have already started to build to create a good display table.\n\nviolence |&gt; \n  pivot_wider(names_from = crime_type, values_from = count) |&gt; \n  select(\n    region, state, murder, rape, `aggravated assault`, `armed robbery`, \n    `unarmed robbery`\n  ) |&gt; \n  gt()\n\n\n\n\n\n\n\nregion\nstate\nmurder\nrape\naggravated assault\narmed robbery\nunarmed robbery\n\n\n\n\nEast Malaysia\nSabah\n36\n211\n230\n0\n284\n\n\nEast Malaysia\nSarawak\n27\n150\n368\n3\n328\n\n\nWest Malaysia\nJohor\n66\n196\n614\n1\n1701\n\n\nWest Malaysia\nKedah\n21\n119\n364\n2\n490\n\n\nWest Malaysia\nKelantan\n13\n114\n252\n2\n219\n\n\nWest Malaysia\nKuala Lumpur\n37\n132\n651\n4\n3175\n\n\nWest Malaysia\nMelaka\n7\n69\n176\n1\n589\n\n\nWest Malaysia\nNegeri Sembilan\n14\n91\n241\n2\n536\n\n\nWest Malaysia\nPahang\n16\n163\n188\n1\n288\n\n\nWest Malaysia\nPerak\n35\n95\n380\n4\n626\n\n\nWest Malaysia\nPerlis\n2\n30\n47\n0\n53\n\n\nWest Malaysia\nPulau Pinang\n17\n80\n275\n0\n706\n\n\nWest Malaysia\nSelangor\n83\n321\n1108\n14\n4944\n\n\nWest Malaysia\nTerengganu\n5\n64\n130\n0\n155\n\n\n\n\n\n\n\nThis table is already better than the default table produced by Quarto if we just print a data frame or tibble. The gt table does not take up the whole width of the page unnecessarily (which can make it harder to read across rows) and has hidden the type of each column.\nNote that in the code above, some of the variable names are surrounded by back ticks (`). There are spaces in these variable names, so we use back ticks to stop R from interpreting the first space as the end of the variable name.\nWe can add more functions to the gt() stack to adjust the appearance of the table. For example, we can format the numeric columns as numbers using the fmt_number() function. This adds thousand separators (in British English, commas) to make it easier to read the large numeric values and can make various other changes such as adding a prefix or suffix to numbers (useful for showing units), scaling numbers (useful for very large numbers) or automatically formatting numbers according to the conventions of the language your computer is set to use (referred to in R help pages as the locale of your computer).\nWe choose which columns fmt_number() should format using the columns argument. In this case, we want to format all the numeric columns in the data, so we will set columns = where(is.numeric).\nWe don’t want the numbers in the table to have any decimal places (since the crime counts are all whole numbers), so we also set decimals = 0. We can use the default values of all the other arguments to fmt_number() – type ?gt::fmt_number in the R console to find out more about the different options available on the help page for the fmt_number() function.\n\nviolence |&gt; \n  pivot_wider(names_from = crime_type, values_from = count) |&gt; \n  select(\n    region, state, murder, rape, `aggravated assault`, `armed robbery`, \n    `unarmed robbery`\n  ) |&gt; \n  gt() |&gt; \n  fmt_number(columns = where(is.numeric), decimals = 0)\n\n\n\n\n\n\n\nregion\nstate\nmurder\nrape\naggravated assault\narmed robbery\nunarmed robbery\n\n\n\n\nEast Malaysia\nSabah\n36\n211\n230\n0\n284\n\n\nEast Malaysia\nSarawak\n27\n150\n368\n3\n328\n\n\nWest Malaysia\nJohor\n66\n196\n614\n1\n1,701\n\n\nWest Malaysia\nKedah\n21\n119\n364\n2\n490\n\n\nWest Malaysia\nKelantan\n13\n114\n252\n2\n219\n\n\nWest Malaysia\nKuala Lumpur\n37\n132\n651\n4\n3,175\n\n\nWest Malaysia\nMelaka\n7\n69\n176\n1\n589\n\n\nWest Malaysia\nNegeri Sembilan\n14\n91\n241\n2\n536\n\n\nWest Malaysia\nPahang\n16\n163\n188\n1\n288\n\n\nWest Malaysia\nPerak\n35\n95\n380\n4\n626\n\n\nWest Malaysia\nPerlis\n2\n30\n47\n0\n53\n\n\nWest Malaysia\nPulau Pinang\n17\n80\n275\n0\n706\n\n\nWest Malaysia\nSelangor\n83\n321\n1,108\n14\n4,944\n\n\nWest Malaysia\nTerengganu\n5\n64\n130\n0\n155\n\n\n\n\n\n\n\nfmt_number() is one of several formatting functions available in gt. For example, we could use fmt_currency() to format columns according to the conventions for currency values, fmt_date() for dates or fmt_percent() for percentages.\nThe region column only has two values: West Malaysia for states and territories in Peninsular Malaysia and East Malaysia for states on the island of Borneo. Rather than repeat these two values on every row of the table – which is a waste of space and makes the table more cluttered than necessary – we can instead group the rows according to these two regions and then only show the region names once at the top of each group.\ngt() will automatically create group headings in a table if the data frame or tibble passed to gt() contains groups created by the group_by() function from the dplyr package. All we have to do is use group_by() to specify which column (in this case, region) contains the values that we should use to determine which group each row is in.\n\nviolence |&gt; \n  pivot_wider(names_from = crime_type, values_from = count) |&gt; \n  select(\n    region, state, murder, rape, `aggravated assault`, `armed robbery`, \n    `unarmed robbery`\n  ) |&gt; \n  group_by(region) |&gt; \n  # Functions from `dplyr`, `tidyr` etc above and functions from `gt` below\n  gt() |&gt; \n  fmt_number(columns = where(is.numeric), decimals = 0)\n\n\n\n\n\n\n\nstate\nmurder\nrape\naggravated assault\narmed robbery\nunarmed robbery\n\n\n\n\nEast Malaysia\n\n\nSabah\n36\n211\n230\n0\n284\n\n\nSarawak\n27\n150\n368\n3\n328\n\n\nWest Malaysia\n\n\nJohor\n66\n196\n614\n1\n1,701\n\n\nKedah\n21\n119\n364\n2\n490\n\n\nKelantan\n13\n114\n252\n2\n219\n\n\nKuala Lumpur\n37\n132\n651\n4\n3,175\n\n\nMelaka\n7\n69\n176\n1\n589\n\n\nNegeri Sembilan\n14\n91\n241\n2\n536\n\n\nPahang\n16\n163\n188\n1\n288\n\n\nPerak\n35\n95\n380\n4\n626\n\n\nPerlis\n2\n30\n47\n0\n53\n\n\nPulau Pinang\n17\n80\n275\n0\n706\n\n\nSelangor\n83\n321\n1,108\n14\n4,944\n\n\nTerengganu\n5\n64\n130\n0\n155\n\n\n\n\n\n\n\nIn tables containing lots of numbers it can be difficult to see patterns. One way to help readers to understand patterns is to map the numbers to an aesthetic property such as colour that people can easily see patterns in. To do this, we can colour the cells in a column according to the value of each cell using the data_color() function (note the spelling of ‘color’ in this function). To use data_color(), we specify the columns we want to shade using the columns argument and the colour palette we want to use using the palette argument.\nIn this example, we will only colour the values in two columns, so we will pass the column names (enclosed in `back ticks` if they contain a space) to the columns argument.\nThe easiest way to specify a colour palette is to use one of the built-in colour palettes that the gt package understands automatically. These use the same colour palette names we have used in previous tutorials when making use of functions such as scale_fill_distiller().\n\nlibrary(scales)\n\nviolence |&gt; \n  pivot_wider(names_from = crime_type, values_from = count) |&gt; \n  select(\n    region, state, murder, rape, `aggravated assault`, `armed robbery`, \n    `unarmed robbery`\n  ) |&gt; \n  group_by(region) |&gt; \n  # Functions from `dplyr`, `tidyr` etc above and functions from `gt` below\n  gt() |&gt; \n  fmt_number(columns = where(is.numeric), decimals = 0) |&gt; \n  data_color(columns = `unarmed robbery`, palette = \"Oranges\") |&gt; \n  data_color(columns = rape, palette = \"Blues\")\n\n\n\n\n\n\n\nstate\nmurder\nrape\naggravated assault\narmed robbery\nunarmed robbery\n\n\n\n\nEast Malaysia\n\n\nSabah\n36\n211\n230\n0\n284\n\n\nSarawak\n27\n150\n368\n3\n328\n\n\nWest Malaysia\n\n\nJohor\n66\n196\n614\n1\n1,701\n\n\nKedah\n21\n119\n364\n2\n490\n\n\nKelantan\n13\n114\n252\n2\n219\n\n\nKuala Lumpur\n37\n132\n651\n4\n3,175\n\n\nMelaka\n7\n69\n176\n1\n589\n\n\nNegeri Sembilan\n14\n91\n241\n2\n536\n\n\nPahang\n16\n163\n188\n1\n288\n\n\nPerak\n35\n95\n380\n4\n626\n\n\nPerlis\n2\n30\n47\n0\n53\n\n\nPulau Pinang\n17\n80\n275\n0\n706\n\n\nSelangor\n83\n321\n1,108\n14\n4,944\n\n\nTerengganu\n5\n64\n130\n0\n155\n\n\n\n\n\n\n\n\nIn this table we use two different colours to show the patterns in the frequency of murder and unarmed robbery. This is because we want readers to remember that different types of crime are different and so comparisons that treat crimes as being equivalent to one another are likely to be flawed. If we used the same colour across columns, readers might end up seeing that the shade used for unarmed robberies in Kuala Lumpur was darker than the shade showing the number of murders and conclude that unarmed robberies were a bigger problem than murders. This would be a potentially false conclusion because a single murder and a single unarmed robbery are not the same in terms of their seriousness.\nFor the same reason the table does not include a column showing the total number of crimes in each state – when we total all types of crime together, we are implicitly assuming that all types of crime are the same when that is obviously untrue.\n\n\n\n15.2.3 Adding summary rows\nThe final thing we will add to this table is a summary row containing the total number of each type of crime across all the states and territories. We do this using the summary_rows() function from gt. We specify the columns we want to summarise using the columns argument as we did for fmt_number().\nSummary rows can be produced using lots of different R functions. For example, we could use the mean() function to produce a summary row showing the mean (average) number of crimes of each time across the states. In this case, we want to know the total number of each type of crime across all states, so we will use the sum() function. To specify this, we pass the fns argument to summary_rows(). For reasons we don’t need to go into, the funs argument must be a list of function names, even if only one function name is needed. So to tell summary_rows() to produce summaries based on totals, we need to specify fns = list(\"sum\").\nBy default, this will produce a row at the bottom of the table containing the totals, but that row will have a label on the left that is based on the name of the function – sum() used to produce the summary values. Since more people will know the word ‘total’ than the word ‘sum’, we will instead specify that this summary row should be labelled as ‘total’. To do this, we have to name the entry in the list passed to fns that specifies the function to use, i.e. we need to specify fns = list(\"total = \"sum\") rather than just fns = list(\"sum\").\nBy default, summary_rows() formats the calculated summary values as numbers using fmt_numeric(), we also need to specify decimals = 0 as before.\n\nviolence |&gt; \n  pivot_wider(names_from = crime_type, values_from = count) |&gt; \n  select(\n    region, state, murder, rape, `aggravated assault`, `armed robbery`, \n    `unarmed robbery`\n  ) |&gt; \n  group_by(region) |&gt; \n  # Functions from `dplyr`, `tidyr` etc above and functions from `gt` below\n  gt() |&gt; \n  fmt_number(columns = where(is.numeric), decimals = 0) |&gt; \n  data_color(columns = `unarmed robbery`, palette = \"Oranges\") |&gt; \n  data_color(columns = rape, palette = \"Blues\") |&gt; \n  summary_rows(\n    columns = where(is.numeric),\n    fns = list(\"total\" = \"sum\"),\n    decimals = 0\n  )\n\n\n\n\n\n\n\n\nstate\nmurder\nrape\naggravated assault\narmed robbery\nunarmed robbery\n\n\n\n\nEast Malaysia\n\n\n\nSabah\n36\n211\n230\n0\n284\n\n\n\nSarawak\n27\n150\n368\n3\n328\n\n\nsum\n—\n63\n361\n598\n3\n612\n\n\nWest Malaysia\n\n\n\nJohor\n66\n196\n614\n1\n1,701\n\n\n\nKedah\n21\n119\n364\n2\n490\n\n\n\nKelantan\n13\n114\n252\n2\n219\n\n\n\nKuala Lumpur\n37\n132\n651\n4\n3,175\n\n\n\nMelaka\n7\n69\n176\n1\n589\n\n\n\nNegeri Sembilan\n14\n91\n241\n2\n536\n\n\n\nPahang\n16\n163\n188\n1\n288\n\n\n\nPerak\n35\n95\n380\n4\n626\n\n\n\nPerlis\n2\n30\n47\n0\n53\n\n\n\nPulau Pinang\n17\n80\n275\n0\n706\n\n\n\nSelangor\n83\n321\n1,108\n14\n4,944\n\n\n\nTerengganu\n5\n64\n130\n0\n155\n\n\nsum\n—\n316\n1474\n4426\n31\n13482\n\n\n\n\n\n\n\n\nTables are good for showing detailed information, particularly when we want to present multiple pieces of information about a single place. But it can be hard to spot patterns in tables even with coloured cells. For this reason, do not use tables when you are primarily trying to show the relationship between two or more variables. In the next section, we will learn to create bar chart in R to show patterns more effectively.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Presenting spatial data without maps</span>"
    ]
  },
  {
    "objectID": "15_no_maps/index.html#bar-charts",
    "href": "15_no_maps/index.html#bar-charts",
    "title": "15  Presenting spatial data without maps",
    "section": "15.3 Bar charts",
    "text": "15.3 Bar charts\nBar charts are useful for showing values of one continuous variable (e.g. a count of crimes) for each value of one categorical variable (e.g. states of a country). Bar charts are very common, but there are several things we can do to make them more useful. In this section we will learn how to construct a good bar chart.\nYou’re already an expert at making maps using functions from the ggplot2 package. We can use these same functions to create many other types of graphics. For example, we can use geom_bar() to create bar charts just as we use geom_sf() to create a map using data stored in an SF object.\ngeom_bar() calculates the length of each bar on a chart by counting the number of rows of data in each category. This isn’t what we want to do to visualise the violence object, since the data provided by the Royal Malaysian Police are already in the form of counts of crimes. Instead, we will use the geom_col() function, which creates bar charts from this type of summary data.\nTo create a simple bar chart, we will work with the original (long-format) data and filter it to show only the number of murders in each state.\n\nviolence |&gt; \n  filter(crime_type == \"murder\") |&gt; \n  ggplot() +\n  geom_col(aes(x = state, y = count))\n\n\n\n\n\n\n\n\nWe can improve this chart in several ways:\n\nWe can switch the order of the variables used for the x and y aesthetics so that the bars are horizontal rather than vertical, which will stop the state names from overlapping.\nWe can change the axis labels, add a title and so on using labs().\nWe can reduce the visual clutter in the chart using theme_minimal().\n\n\nviolence |&gt; \n  filter(crime_type == \"murder\") |&gt; \n  ggplot() +\n  geom_col(aes(x = count, y = state)) +\n  labs(\n    title = \"Murders in Malaysian states, 2017\",\n    caption = \"Data from the Royal Malaysian Police\",\n    x = \"number of murders\",\n    y = NULL\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nThis chart is better, but we can improve it further. For example, we can reduce the space between the state names and the bars by setting the expand argument to the scale_x_continuous() function. scale_x_continuous() works in a similar way to the other scale functions we have used already, such as scale_fill_brewer() to control the colour of areas in a choropleth map.\nAlthough we are trying to reduce the gap between the bars and labels on the y axis, we use a function that change the x axis. This is because the space we are reducing is created by R by-default adding some space to each end of any continuous axis, such as the count of murders.\nWe can specify the space at the end of each axis using the helper function expansion(), but in this case we just want to remove the space completely so we can just set expand = c(0, 0).\nAt the same time, we can also remove the grid lines on the y axis (i.e. those running along the length of the bars) since they don’t really make it any easier to understand the chart. As a general principle, we want to remove anything on a chart that does not contribute to communicating information, since unnecessary chart elements can distract readers from understanding the data.\nWe can remove the grid lines by setting the panel.grid.major.y and panel.grid.minor.y arguments to the theme() function. The value we want to use is the helper function element_blank(), which sets the grid lines to be blank.\n\nviolence |&gt; \n  filter(crime_type == \"murder\") |&gt; \n  ggplot() +\n  geom_col(aes(x = count, y = state)) +\n  scale_x_continuous(expand = c(0, 0)) +\n  labs(\n    title = \"Murders in Malaysian states, 2017\",\n    caption = \"Data from the Royal Malaysian Police\",\n    x = \"number of murders\",\n    y = NULL\n  ) +\n  theme_minimal() +\n  theme(\n    panel.grid.major.y = element_blank(),\n    panel.grid.minor.y = element_blank()\n  )\n\n\n\n\n\n\n\n\n\n15.3.1 Ordering bar charts by value\nIf you were trying to find the three Malaysian states or territories with the most murders from this chart, it would be pretty easy to see that Selangor had the most murders, followed by Johor. But at a glance, it’s not so easy to see which state or territory comes third. We can make this easier to see by changing the order of the bars from the default alphabetical order to an order based on how many murders there were.\nTo do this, we need to convert the state column in the data to a new type of variable: a factor. Factors are what R calls categorical variables that have a defined set of possible values. For example, a factor recording if a person was under or over 18 might have two possible values: adult and child.\nOne of the benefits of storing a variable as a factor is that we can specify an order for the categories. This is useful for categories that have a meaningful order, such as bad, acceptable, good, excellent. But we can also use this feature of factors to specify that values should appear in a particular order in any charts produced from the data, whatever the order of the values in the data itself.\n\nTo work with factors in R we can use the forcats package, so-called because it’s for working with categories. forcats is loaded as part of tidyverse, so we don’t need to load it separately.\nAll the functions in the forcats package start with the letters fct_, just as all the functions in the SF package start st_. For our bar chart, we will use the fct_reorder() function. This takes a factor or character variable (such as the names of the Malaysian states and territories) and sets the order of the categories according to the values of a numeric variable (such as the number of murders in a state). So to re-order the state variable according to the count of murders, we can use fct_reorder(state, count). Since we’re changing an existing variable, we will do this inside a call to the mutate() function.\n\nviolence |&gt; \n  filter(crime_type == \"murder\") |&gt; \n  mutate(state = fct_reorder(state, count)) |&gt; \n  ggplot() +\n  geom_col(aes(x = count, y = state)) +\n  scale_x_continuous(expand = c(0, 0)) +\n  labs(\n    title = \"Murders in Malaysian states, 2017\",\n    caption = \"Data from the Royal Malaysian Police\",\n    x = \"number of murders\",\n    y = NULL\n  ) +\n  theme_minimal() +\n  theme(\n    panel.grid.major.y = element_blank(),\n    panel.grid.minor.y = element_blank()\n  )\n\n\n\n\n\n\n\n\nFinally, we can use colour to indicate which states are in which of the two regions of Malaysia. To do this, we will:\n\nspecify in the call to aes() that the fill colour of the bars should be controlled by the region column in the data,\nspecify in the call to labs() that we don’t want the legend to have a title, since the meaning is obvious from the values ‘East Malaysia’ and ‘West Malaysia’, and\nspecify in the call to theme() that we would like the legend use up some of the empty space in the bottom-right corner of the chart, rather than making the chart smaller to give space for the legend on the right-hand side.\n\nTo move the legend, we need to specify three different arguments in the theme() function. legend.position determines where in the plot the legend should be placed, based on how far along each axis (as a proportion of the axis length, on a scale from zero to one) the legend should be placed.\n\ntibble(\n  x = c(0, 0, 0, 0.5, 0.5, 0.5, 1, 1, 1), \n  y = c(0, 0.5, 1, 0, 0.5, 1, 0, 0.5, 1)\n) |&gt; \n  mutate(\n    label = str_glue(\"{number(x, accuracy = 0.1)}, {number(y, accuracy = 0.1)}\")\n  ) |&gt; \n  ggplot(aes(x = x, y = y, label = label)) +\n  geom_point() +\n  geom_label() +\n  scale_x_continuous(\n    expand = expansion(0.1), \n    position = \"top\", \n    sec.axis = dup_axis(name = NULL)\n  ) +\n  scale_y_continuous(\n    expand = expansion(0.1), \n    sec.axis = dup_axis(name = NULL)\n  ) +\n  coord_fixed() +\n  labs(\n    x = \"the first number gives the value on horizontal (x) axis\",\n    y = \"the second number gives the value on vertical (y) axis\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nUsing this specification, we can place the legend in the right-most point on the horizontal axis and the bottom-most point on the vertical axis by specifying legend.position = c(1, 0).\nlegend.position sets the anchor point from which the legend is created, with the actual size of the legend depending on how much space is required by its contents. By default, a legend will spread out in all directions from the anchor point, i.e. the legend will be horizontally and vertically centred on the anchor point. As we have positioned the legend in a corner of the plot, this is probably not what we want since some of the legend will be hidden outside the plot area. Instead, we can set the legend.justifcation argument of theme() using a similar specification to that for legend.position based on which way we want the legend to grow.\n\ntibble(\n  x = c(1.1, 1.1, 0.9, 0.9, 1.1, 0.9, 1, 1),\n  xend = c(1.25, 1.25, 0.75, 0.75, 1.25, 0.75, 1, 1),\n  y = c(1.1, 0.9, 1.1, 0.9, 1, 1, 0.9, 1.1),\n  yend = c(1.25, 0.75, 1.25, 0.75, 1, 1, 0.75, 1.25),\n  h = c(0, 0, 1, 1, 0, 1, 0.5, 0.5),\n  v = c(0, 1, 0, 1, 0.5, 0.5, 1, 0)\n)  |&gt;  \n  mutate(\n    label = str_glue(\"{number(h, accuracy = 0.1)}, {number(v, accuracy = 0.1)}\")\n  ) |&gt; \n  ggplot() +\n  geom_segment(\n    aes(x = x, y = y, xend = xend, yend = yend), \n    linewidth = 1, \n    arrow = arrow(length = unit(3, \"mm\"), type = \"closed\")\n  ) +\n  geom_label(aes(x = xend, y = yend, hjust = h, vjust = v, label = label)) +\n  geom_point(aes(x = x, y = y), size = 4) +\n  scale_x_continuous(expand = expansion(3)) +\n  scale_y_continuous(expand = expansion(0.5)) +\n  coord_fixed() +\n  theme_void()\n\n\n\n\n\n\n\n\nIf you want the legend to grow ‘inwards’ from a corner, just set legend.justification to the same value as you used for legend.position. In this case, we want the legend to be anchored in the bottom-right corner and to grow inwards from it, so we will set both arguments to c(1, 0).\n\nviolence |&gt; \n  filter(crime_type == \"murder\") |&gt; \n  mutate(state = fct_reorder(state, count)) |&gt; \n  ggplot() +\n  geom_col(aes(x = count, y = state, fill = region)) +\n  scale_x_continuous(expand = c(0, 0)) +\n  labs(\n    title = \"Murders in Malaysian states, 2017\",\n    caption = \"Data from the Royal Malaysian Police\",\n    x = \"number of murders\",\n    y = NULL,\n    fill = NULL\n  ) +\n  theme_minimal() +\n  theme(\n    legend.background = element_rect(colour = NA, fill = \"white\"),\n    legend.justification = c(1, 0),\n    legend.position = c(1, 0),\n    panel.grid.major.y = element_blank(),\n    panel.grid.minor.y = element_blank()\n  )\n\nWarning: A numeric `legend.position` argument in `theme()` was deprecated in ggplot2\n3.5.0.\nℹ Please use the `legend.position.inside` argument of `theme()` instead.\n\n\n\n\n\n\n\n\n\nBar charts are a very common way of presenting a numeric variable for each value of a categorical variable. Bar charts are easy to interpret, even for people who are not used to interpreting charts or who only have time to look at the chart for a few seconds.\n\nOne of the reasons why bar charts are easy to interpret is that the length of each bar directly corresponds to the relative size of that particular value. But this direct relationship between bar length and value only applies if the bars start at zero. If you create a bar chart in which the bars don’t start at zero, readers are likely to be mislead, so remember bar charts should always start at zero. But don’t worry – ggplot() will handle this for you automatically.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Presenting spatial data without maps</span>"
    ]
  },
  {
    "objectID": "15_no_maps/index.html#showing-a-distribution",
    "href": "15_no_maps/index.html#showing-a-distribution",
    "title": "15  Presenting spatial data without maps",
    "section": "15.4 Showing a distribution",
    "text": "15.4 Showing a distribution\nBar charts show a single piece of information about each category present in a dataset. So we might use a bar chart to show, for example, the average number of burglaries in neighbourhoods in different districts. But what if the average values masked substantial differences in the number of burglaries within each district? Averages often mask variation, and can sometimes be misleading as a result. In those circumstances it would be better to show more detail rather than a misleading average.\nLet’s start with the simple example of showing the distribution of burglary counts within a single district. The burglary object contains counts of burglaries in each lower-layer super output area (LSOA) in Northamptonshire in England in 2020. To show the distribution of burglary counts within one district (Kettering), we can create a histogram using geom_histogram().\nA histogram divides the range of values present in the data into a number of equally sized bins, then shows bars representing the number of observations (rows) in the data that have values fitting into each bin. We can either allow geom_histogram() to set the number of bins automatically, or set it ourself with the binwidth argument.\nWe will set the binwidth argument of the geom_histogram() function to binwidth = 1 so that each bar on the chart will show now many LSOAs have each individual value. We will also add some labels to help readers interpret the chart.\n\nburglary |&gt; \n  filter(district == \"Kettering\") |&gt; \n  ggplot() +\n  geom_histogram(aes(x = count), binwidth = 1) +\n  scale_x_continuous(expand = c(0, 0)) +\n  scale_y_continuous(expand = c(0, 0)) +\n  labs(\n    title = \"Number of burglaries in Kettering neighbourhoods\",\n    x = \"count of burglaries, 2020\",\n    y = \"number of LSOAs\"\n  ) +\n  theme_minimal() +\n  theme(\n    panel.grid.major.x = element_blank(),\n    panel.grid.minor.x = element_blank()\n  )\n\n\n\n\n\n\n\n\nWe can see on this chart that most LSOAs had only a few burglaries in 2020 (i.e. the largest bars are to the left of the chart), while a few LSOAs had a larger number (the bars to the right of the chart). This is what we would expect, since we know the crimes are generally concentrated in a few places.\n\n15.4.1 Plotting density curves\nDot plots can be effective at showing the distribution of a variable, but are only really useful for a single group since they become cluttered if you try to show several types of dot (e.g. of different colours). One way to show the general distribution of a variable is to summarise the locations of the points on the horizontal axis by adding a density curve with geom_density(). A density curve is a smoothed version of a histogram, which is useful to show the general distribution of a variable (in this case, the number of LSOAs with different numbers of burglaries) at the cost of not showing the exact data. The mathematical procedure used by geom_density() to calculate a density curve is the same as the kernel-density estimation process we have already learned to use to show concentrations of crime on a map.\n\nburglary |&gt; \n  filter(district == \"Kettering\") |&gt; \n  ggplot() +\n  geom_density(aes(x = count), colour = \"red\", linewidth = 1) +\n  scale_x_continuous(expand = c(0, 0)) +\n  scale_y_continuous(labels = scales::label_percent()) +\n  labs(\n    title = \"Number of burglaries in Kettering neighbourhoods\",\n    x = \"count of burglaries, 2020\",\n    y = \"percentage of LSOAs\"\n  ) +\n  theme_minimal() +\n  theme(\n    panel.grid.major.x = element_blank(),\n    panel.grid.minor.x = element_blank()\n  )\n\n\n\n\n\n\n\n\nWe can use density curves to show the distribution of a variable across multiple categories at once. For example, we could show the distribution of burglary counts at the neighbourhood level for all the districts in Northamptonshire. To do this we use the geom_density_ridges() function from the ggridges package to create a ridge plot. Although this function does not come from the ggplot2 package, it is designed to be used inside a ggplot() stack.\n\nlibrary(ggridges)\n\nburglary |&gt; \n  # Wrap the district names by replacing any space in a name with a new-line\n  mutate(district = str_replace_all(district, \"\\\\s\", \"\\n\")) |&gt; \n  ggplot(aes(x = count, y = district)) +\n  geom_density_ridges() +\n  scale_x_continuous(expand = c(0, 0)) +\n  labs(\n    title = \"Number of burglaries in Kettering neighbourhoods\",\n    x = \"count of burglaries, 2020\",\n    y = NULL\n  ) +\n  theme_minimal()\n\nPicking joint bandwidth of 1.98\n\n\n\n\n\n\n\n\n\nThe ridge plot shows the distribution of burglary counts in LSOAs within each district, with the distributions overlapping slightly to save space. From this we can see that across all districts most LSOAs have few burglaries, with a small number of LSOAs having more. We can also see there are a small number of LSOAs (probably, in fact, just one LSOA) in Wellingborough district with a much higher number of burglaries than anywhere else in Northamptonshire.\n\n\n15.4.2 Small-multiple charts\nDensity plots can be helpful to summarise a lot of information, but they have some disadvantages. In particular, they don’t show that the number of LSOAs in each district is quite different: there are 131 LSOAs in Northampton but only 41 LSOAs in Corby. To make this clearer we can instead produce several dot plots, one for each district – what are called small-multiple charts.\nWe could create small-multiple charts by producing a separate histogram for each district and then combine them with the patchwork package, but that would involve a lot of repeated code. Fortunately, we can use a feature of the ggplot2 package called faceting to split our single dot plot into multiple plots based on a column in the data (in this case, the district name).\nAdding facet_wrap() to our ggplot() stack will cause R to create multiple plots and wrap them across multiple rows and columns so that they approximately fit into the available space. If we only want the small multiples to appear on top of each other (i.e. in multiple rows) or next to each other (i.e. in multiple columns), we can use the facet_grid() function. In this case we want the small multiples to appear on top of each other, so we will use facet_grid() and say that the small multiples (which ggplot2 calls facets) should be based on the district column in the data by specifying rows = vars(district) (it is necessary to wrap the name of the column that you want to use as the basis of the small multiples in the vars() function, but we do not need to go into why).\n\nburglary |&gt; \n  ggplot(aes(x = count, fill = district)) +\n  geom_hline(yintercept = 0, colour = \"grey80\") +\n  geom_histogram(binwidth = 1) +\n  facet_grid(rows = vars(district), labeller = label_wrap_gen(width = 10)) +\n  labs(\n    title = \"Number of burglaries in Northamptonshire neighbourhoods\",\n    x = \"count of burglaries, 2020\",\n    y = \"number of LSOAs\"\n  ) +\n  theme_minimal() +\n  theme(\n    legend.position = \"none\",\n    panel.grid.major.x = element_blank(),\n    panel.grid.minor.x = element_blank(),\n    strip.text.y = element_text(angle = 0, hjust = 0)\n  )\n\n\n\n\n\n\n\n\nYou might have noticed we made some other changes to our code for this chart to make it clearer:\n\nSet the dot fill to be based on the district column in the data, so the bars in each small multiple will be a different colour. We then remove the legend that is automatically generated by adding the fill aesthetic using the legend.position = \"none\" attribute to theme(), since the legend is redundant (the same information is shown in the facet labels).\nRemoved the values and the title on the y axis, since it is clear from the rest of the chart that the vertical axis shows the count of dots in each stack.\nAdded a horizontal line at zero on the y axis using geom_hline() to make it easier to compare the height of each stack of dots to zero.\nWrapped the facet labels using the label_wrap_gen() helper function so that some of the longer district names don’t take up too much space horizontally.\nMade the facet labels easier to read by making the text horizontal (rather than the default vertical text) using the strip.text.y attribute to theme() and the element_text() helper function. angle sets the rotation of the text (or in this case, specifies that there should be no rotation) and hjust = 0 specifies that the text should be left aligned.\n\n\nIf you want to play around with each of these parts of the code to understand what they do in more detail, remove one part at a time and re-run the code to see how the appearance of the plot changes.\n\nThere are many more-technical ways to show distributions, such as box plots or violin plots. However, these can be difficult to interpret for people who are not used to looking at those particular types of chart, so they should probably be avoided for communicating with general audiences.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Presenting spatial data without maps</span>"
    ]
  },
  {
    "objectID": "15_no_maps/index.html#comparing-continuous-variables",
    "href": "15_no_maps/index.html#comparing-continuous-variables",
    "title": "15  Presenting spatial data without maps",
    "section": "15.5 Comparing continuous variables",
    "text": "15.5 Comparing continuous variables\nSo far we have used bar charts to communicate a single number (in our example, a number of murders) for each value of a categorical variable (the name of each Malaysian state or territory), and histograms to show multiple numbers (burglary counts for each neighbourhood) for each value of a categorical variable (districts in Northamptonshire).\nBoth these types of chart compare a numeric variable to a categorical one. But sometimes we may want to compare two categorical variables. We can do this with a scatter plot. For example, the object vehicle_theft contains rates of thefts of and from motor vehicles per 1,000 households saying they own a vehicle for a selection of 189 municipalities in South Africa. Since thefts of vehicle and thefts from vehicles are different but related crimes, we might want to see if there is a relationship between counts of each type.\n\nvehicle_theft |&gt; \n  head() |&gt; \n  gt() |&gt; \n  tab_header(\n    title = \"Vehicle thefts in South African municipalities\",\n    subtitle = \"a sample of the data\"\n  )  |&gt;  \n  tab_options(\n    heading.align = \"left\"\n  )\n\n\n\n\n\n\n\nVehicle thefts in South African municipalities\n\n\na sample of the data\n\n\nprovince\nmunicipality\ncrime_category\ntheft_rate\n\n\n\n\nEastern Cape\nAmahlathi\nTheft of motor vehicle\n2.492026\n\n\nEastern Cape\nAmahlathi\nTheft out of or from motor vehicle\n26.415470\n\n\nEastern Cape\nBlue Crane Route\nTheft of motor vehicle\n6.054396\n\n\nEastern Cape\nBlue Crane Route\nTheft out of or from motor vehicle\n19.094635\n\n\nEastern Cape\nBuffalo City\nTheft of motor vehicle\n7.584509\n\n\nEastern Cape\nBuffalo City\nTheft out of or from motor vehicle\n49.028434\n\n\n\n\n\n\n\nTo create a ggplot() scatter plot we use geom_point(), the same function we previously used to create point maps. This makes sense, since point maps are a specialised type of scatter plot in which the x and y axes of the chart show the latitude and longitude or easting and northing of each crime location.\nThe data in the vehicle_theft object are in long format, with each row representing a count of crime in a particular category for a particular municipality. To make a scatter plot where each point represents a municipality, we need to have all the data for a municipality in a single row of data, so we will need to transform the data with pivot_wider() (as we did for some of the tables at the start of this tutorial). Since this will turn the crime categories into column names, we will first change the column names to clean names (i.e.  containing only lower-case letters, numbers and underscores) using recode() to make the column names easier to work with.\n\nvehicle_theft_wider &lt;- vehicle_theft |&gt; \n  mutate(\n    crime_category = recode(\n      crime_category,\n      \"Theft of motor vehicle\" = \"theft_of\",\n      \"Theft out of or from motor vehicle\" = \"theft_from\"\n    )\n  ) |&gt; \n  pivot_wider(names_from = crime_category, values_from = theft_rate)\n\nWe can now make a basic scatter plot.\n\nggplot(vehicle_theft_wider, aes(x = theft_of, y = theft_from)) +\n  geom_point() +\n  scale_x_continuous(labels = scales::comma_format()) +\n  scale_y_continuous(labels = scales::comma_format()) +\n  labs(\n    title = \"Vehicle thefts in South African municipalities\",\n    subtitle = \"each dot represents one municipality, 2018-19\",\n    x = \"rate of thefts of motor vehicles per 1,000 vehicle-owning households\",\n    y = \"rate of thefts from motor vehicles per 1,000 vehicle-owning households\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nFrom this plot we can see that most areas have low rates of both theft of and theft from motor vehicles, with a few areas having very-high rates of one type or the other (but none have high rates of both).\nLooking at the bottom-left corner of the chart we can see that we have again encountered the problem of overlapping points making patterns less clear. We can try to deal with this by making the points semi-transparent using the alpha argument to geom_point().\nScatter plots can be hard for people to interpret, especially if they are not used to interpreting charts. To help readers, we can annotate the plot to show how to interpret each region of the chart. We will add two types of annotation: lines to show the median value on each axis, and labels to help interpretation.\nWe can add median lines using the geom_hline() and geom_vline() functions, which add horizontal and vertical lines to plots. We will add these to the ggplot() stack before geom_point() so that the lines appear behind the points.\nTo add text annotations we use the annotate() function from ggplot2, which allows us to add data to a chart by specifying the aesthetics (x and y position, etc.) directly rather than by referencing columns in the data. To add a text annotation, we set the geom argument of annotate() to \"text\".\n\nggplot(vehicle_theft_wider, aes(x = theft_of, y = theft_from)) +\n  geom_vline(\n    xintercept = median(pull(vehicle_theft_wider, \"theft_of\")),\n    linetype = \"22\"\n  ) +\n  geom_hline(\n    yintercept = median(pull(vehicle_theft_wider, \"theft_from\")),\n    linetype = \"22\"\n  ) +\n  geom_point(alpha = 0.5) +\n  annotate(\n    geom = \"text\", \n    x = 20, \n    y = 0, \n    label = \"high rate of thefts of vehicles\\nlow rate of thefts from vehicles\", \n    hjust = 1,\n    lineheight = 1\n  ) +\n  annotate(\n    geom = \"text\", \n    x = 1, \n    y = 75, \n    label = \"low rate of thefts of vehicles\\nhigh rate of thefts from vehicles\", \n    hjust = 0,\n    lineheight = 1\n  ) +\n  scale_x_continuous(labels = scales::comma_format()) +\n  scale_y_continuous(labels = scales::comma_format()) +\n  labs(\n    title = \"Vehicle thefts in South African municipalities\",\n    subtitle = str_glue(\n      \"each dot represents one municipality, 2018-19, dashed lines show \",\n      \"median values\"\n    ),\n    x = \"rate of thefts of motor vehicles per 1,000 vehicle-owning households\",\n    y = \"rate of thefts from motor vehicles per 1,000 vehicle-owning households\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nFrom this plot we can now see that half of municipalities have very low rates of both types of theft (shown by the dots below and to the left of the median lines).\nWe can make some further changes to this chart. For example, instead of labelling areas on the plot we could instead label the municipalities with high rates of vehicle theft (we cannot include both types of label because they would overlap). To do that, we will create a new column in the data containing either the municipality name (for high-rate municipalities) or NA (meaning ggplot() will not create a label for that row if we set na.rm = TRUE). We can then use geom_label_repel() to add the labels to the chart, remembering to add label = label to the aes() function so ggplot() knows which column in the data to use for the labels.\n\nlibrary(ggrepel)\n\nvehicle_theft_wider |&gt; \n  mutate(\n    label = if_else(theft_of &gt; 17 | theft_from &gt; 65, municipality, NA_character_)\n  ) |&gt; \n  ggplot(aes(x = theft_of, y = theft_from, label = label)) +\n  geom_vline(\n    xintercept = median(pull(vehicle_theft_wider, \"theft_of\")),\n    linetype = \"22\"\n  ) +\n  geom_hline(\n    yintercept = median(pull(vehicle_theft_wider, \"theft_from\")),\n    linetype = \"22\"\n  ) +\n  geom_point(alpha = 0.2) +\n  geom_label_repel(na.rm = TRUE, label.size = 0, lineheight = 1) +\n  scale_x_continuous(labels = scales::comma_format()) +\n  scale_y_continuous(labels = scales::comma_format()) +\n  labs(\n    title = \"Vehicle thefts in South African municipalities\",\n    subtitle = str_glue(\n      \"each dot represents one municipality, 2018-19, dashed lines show \",\n      \"median values\"\n    ),\n    x = \"rate of thefts of motor vehicles per 1,000 vehicle-owning households\",\n    y = \"rate of thefts from motor vehicles per 1,000 vehicle-owning households\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nFinally, we can add a trend line to the plot. We do this using the geom_smooth() function from ggplot2. geom_smooth() can add different types of trend line to a plot, but in this example we will specify a simple linear trend line by setting method = \"lm\". We will also specify formula = y ~ x (the default) to avoid geom_smooth() producing a message to tell us what formula it used to calculate the trend.\n\nvehicle_theft_wider |&gt; \n  mutate(\n    label = if_else(theft_of &gt; 17 | theft_from &gt; 65, municipality, NA_character_)\n  ) |&gt; \n  ggplot(aes(x = theft_of, y = theft_from)) +\n  geom_vline(\n    xintercept = median(pull(vehicle_theft_wider, \"theft_of\")),\n    linetype = \"22\"\n  ) +\n  geom_hline(\n    yintercept = median(pull(vehicle_theft_wider, \"theft_from\")),\n    linetype = \"22\"\n  ) +\n  geom_smooth(method = \"lm\", formula = y ~ x, colour = \"grey20\") +\n  geom_point(alpha = 0.2) +\n  geom_label_repel(\n    aes(label = label), \n    na.rm = TRUE, \n    label.size = 0, \n    lineheight = 1\n  ) +\n  scale_x_continuous(labels = scales::comma_format()) +\n  scale_y_continuous(labels = scales::comma_format()) +\n  labs(\n    title = \"Vehicle thefts in South African municipalities\",\n    subtitle = str_glue(\n      \"each dot represents one municipality, 2018-19, dashed lines show \",\n      \"median values\"\n    ),\n    x = \"rate of thefts of motor vehicles per 1,000 vehicle-owning households\",\n    y = \"rate of thefts from motor vehicles per 1,000 vehicle-owning households\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nFrom this chart, we can see which municipalities have particularly unusual vehicle theft rates. For example, we might well want to explore the rates of theft from vehicles in Beaufort West or Stellenbosch municipalities to see what makes them so different from the others, and similarly for the rate of theft of vehicles in Ethekwini.\nOne note of caution when using geom_smooth(): this function will show the direction of the relationship between two variables regardless of the strength of that relationship. In extreme cases, that could mean that a chart would show a trend line between two variables even if the variables had almost no relationship to one another.\nFor example, geom_smooth() produces a line showing the direction of the relationship between the two variables in each of these three charts, even though the relationship on the right is much stronger than the one on the left.\n\ntibble(\n  group = factor(\n    c(\"weak relationship\", \"moderate relationship\", \"strong relationship\"), \n    levels = c(\"weak relationship\", \"moderate relationship\", \"strong relationship\")\n  ),\n  x = list(1:10, 1:10, 1:10)\n) |&gt; \n  mutate(\n    sd = recode(\n      group, \n      \"weak relationship\" = 10, \n      \"moderate relationship\" = 4, \n      \"strong relationship\" = 0.5\n    )\n  ) |&gt; \n  group_by(sd) |&gt; \n  mutate(y = list(rnorm(10, sd = sd))) |&gt; \n  unnest(c(x, y)) |&gt; \n  mutate(y = scales::rescale(x + y, to = c(0, 10))) |&gt; \n  ungroup() |&gt; \n  ggplot(aes(x = x, y = y, colour = group)) + \n  geom_smooth(method = \"lm\", formula = \"y ~ x\", se = FALSE, na.rm = TRUE) +\n  geom_point() +\n  scale_x_continuous(limits = c(0, 10)) +\n  scale_y_continuous(limits = c(0, 10)) +\n  labs(x = NULL, y = NULL) +\n  coord_fixed() +\n  facet_grid(cols = vars(group)) +\n  theme_minimal() +\n  theme(\n    axis.text = element_blank(),\n    axis.ticks = element_blank(),\n    legend.position = \"none\"\n  )\n\n\n\n\n\n\n\n\n\nBe very careful about trying to interpret the strength of relationships between two variables by plotting them on a chart. It is much better to measure the strength of the relationship using a statistical test such as a correlation test, but statistical tests are outside the scope of this course.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Presenting spatial data without maps</span>"
    ]
  },
  {
    "objectID": "15_no_maps/index.html#in-summary",
    "href": "15_no_maps/index.html#in-summary",
    "title": "15  Presenting spatial data without maps",
    "section": "15.6 In summary",
    "text": "15.6 In summary\n\nIn this tutorial we have learned how to present data about crime at places without using maps. These techniques give us more flexibility about how to best present data to communicate the main points that we want to get across.\nWhether to use a map or a chart, and which type of map or chart to use, are design decisions for you to make. When you make these decisions, always remember that what is most important is that your audience understands your message. This makes it very important that you understand your audience.\n\nVisualising data with charts is a very large topic and there are lots of resources available to help you learn more. To get started, you might want to look at:\n\nAn Introduction to ggplot2 from the University of Cincinnati Business Analytics team.\nThe ggplot2 cheat sheet by the team that develops the `ggplot2 package.\nThe R Graph Gallery for examples of many other types of chart that you can produce in R.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Presenting spatial data without maps</span>"
    ]
  },
  {
    "objectID": "16_mapping_time/index.html",
    "href": "16_mapping_time/index.html",
    "title": "16  Mapping crime over time",
    "section": "",
    "text": "16.1 Introduction\nLearn to show change over time on crime maps and charts\nUnderstanding how crime varies over time is just as important as understanding how it varies between places. Very few places are hotspots of crime all the time – business districts might be hotspots of pickpocketing in the daytime but deserted at night, while a nearby entertainment district may be quiet in the daytime but a violence hotspot at night.\nCrime varies over time in lots of ways. For example, there was a long-term drop in many types of crime in many countries starting in the mid 1990s, e.g. residential burglary in England and Wales dropped by 67% between 1993 and 2008 while the number of homicides in New York City dropped almost 90% from 2,245 in 1990 to 289 in 2018.\nThere are also short-term variations in crime. Many types of crime are more common at some types of year than others (known as seasonal variation). In Chicago, for example, assaults, residential burglaries and personal robberies all vary throughout the year, with assaults in particular being consistently higher in summer than winter.\nIt is also important to understand short-term variation in crime. For example, both property damage and sexual violence in Chicago peaks at weekends, while there are fewer shoplifting offences on Sundays when some shops are closed or have shorter opening hours.\nUnderstanding variation in crime over time is important because we can use the temporal concentration of crime to focus efforts to respond to crime most effectively. For example, imagine we wanted to deploy police patrols to a hotspot to respond to a particular crime problem. Such patrols could be very effective if they were targeted at the times when crimes were most likely to happen or completely useless if the officers were deployed at the wrong day or the wrong time.\nIn this tutorial we will learn how to incorporate variation over time into our analysis of where crime happens, including making animated maps like this one:",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Mapping crime over time</span>"
    ]
  },
  {
    "objectID": "16_mapping_time/index.html#handling-dates-in-r",
    "href": "16_mapping_time/index.html#handling-dates-in-r",
    "title": "16  Mapping crime over time",
    "section": "16.2 Handling dates in R",
    "text": "16.2 Handling dates in R\n\n\n\nAt a very basic level, computers can store data in two days: they can store numbers and they can store text. This makes storing dates slightly complicated, because dates aren’t completely like numbers and they aren’t completely like text either. Dates aren’t like numbers because you can’t do normal maths with dates (e.g. what date is the 29th of January plus one month?) and aren’t like text because you can do some maths with them (e.g. it is easy to calculate three days from today). Dates are especially complicated because they can be written as text in so many different ways. For example 17 January can be represented in all of these ways, all of them equally valid (although some are specific to particular countries):\n\n17/01/2025\n17.01.25\n1/17/2025\n2025-01-17\n17 Jan 25\n17 January 2025\nJanuary 17th 2025\n\n\nR deals with this problem by storing dates internally as if they were numbers and displaying them (e.g. in the console or in a Quarto document) as if they were text, by default in the format 2025-01-07. Fortunately, we don’t have to worry about how dates and times are stored internally in R because we can use the lubridate package to work with them. lubridate contains functions for working with dates, including extracting parts of a date with functions such as month() and converting text to date values with functions like ymd().\nBecause of the special nature of dates, if we want to work with a date variable (for example to create a chart of crime over time) it is important that it is stored as a date, not as text or as a number. Many R functions for reading data, including read_csv(), read_tsv() and read_excel(), will try to recognise columns of data that contain dates and times stored in common formats. These will automatically be stored as date variables when the data is loaded.\nIf R does not recognise automatically that a value contains a date, we can convert it to a date by using the date-parsing functions from lubridate. Which function to use depends on the order in which the components of the date (e.g.  day, month and year) appear in the variable. For example, to convert the text “Saturday 17 January 1981” to a date format we can use the dmy() function because the day of the month comes first, the month next and then the year. Similarly, converting the text “01/17/81” needs the function mdy(). Note that the lubridate date-parsing functions are able convert both numeric and text-based months, and to ignore elements that aren’t needed, such as weekday names.\nIf a date is stored in multiple columns in a dataset, e.g. one column for the year, one column for the month and one column for the day, we can create a single date column using the make_date() function to combine them. Similarly, we can create a date-time column using the make_datetime() function. For example, if we have a dataset of crimes called thefts:\n\nlibrary(lubridate)\n\nthefts |&gt;  \n  mutate(\n    date = make_date(year = year, month = month_of_year, day = day_of_month)\n  ) |&gt;  \n  select(day_of_month, month_of_year, year, date)\n\n# A tibble: 1,840 × 4\n   day_of_month month_of_year  year date      \n          &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt; &lt;date&gt;    \n 1            1             9  2020 2020-09-01\n 2            1             9  2020 2020-09-01\n 3            1             9  2020 2020-09-01\n 4            1             9  2020 2020-09-01\n 5            1             9  2020 2020-09-01\n 6            1             9  2020 2020-09-01\n 7            1             9  2020 2020-09-01\n 8            1             9  2020 2020-09-01\n 9            1             9  2020 2020-09-01\n10            1             9  2020 2020-09-01\n# ℹ 1,830 more rows\n\n\nNote that in this dataset, the date variable we have just created has the type &lt;date&gt;.\nOnce we have converted dates stored as text to dates stored as dates, R understands that they are dates and we can do things like compare them. So while running \"Sat 17 January 1981\" == \"01/17/81\" to test if two dates are the same would give the answer FALSE (because the two pieces of text are different), once we’ve converted the text to date values R can tell that the two dates are the same:\n\n# This code returns TRUE only because the two pieces of text are identical\n\"Sat 17 January 1981\" == \"Sat 17 January 1981\"\n\n[1] TRUE\n\n# This code returns FALSE because the two pieces of text are different, even\n# though the dates they represent are the same\n\"Sat 17 January 1981\" == \"01/17/81\"\n\n[1] FALSE\n\n# This code returns TRUE because R knows they are dates and so compares them as\n# dates, finding that the two dates are the same\ndmy(\"Sat 17 January 1981\") == mdy(\"01/17/81\")\n\n[1] TRUE\n\n\n\n16.2.1 Working with dates\nWhen analysing dates and times, it is often useful to be able to extract date or time portions. We can do this with the lubridate functions year(), month(), wday() (for day of the week), mday() (for day of the month), yday() (for day of the year, counting from 1 January), hour(), minute() and second(), each of which extracts the relevant portion of a date or time as a number. The month() and wday() functions are slightly different, because they can also return the day or month name as text by specifying the argument label = TRUE. We can see this by extracting the different portions of the current date and time, which we can retrieve with the now() function from lubridate.\n\nmessage(\"Current year: \", year(now()))\n\nCurrent year: 2025\n\nmessage(\"Current month (as a number): \", month(now()))\n\nCurrent month (as a number): 1\n\nmessage(\"Current month (as text, abbreviated): \", month(now(), label = TRUE))\n\nCurrent month (as text, abbreviated): Jan\n\nmessage(\"Current month (as text): \", month(now(), label = TRUE, abbr = FALSE))\n\nCurrent month (as text): January\n\nmessage(\"Current day of the year (days since 1 Jan): \", yday(now()))\n\nCurrent day of the year (days since 1 Jan): 7\n\nmessage(\"Current day of the month: \", mday(now()))\n\nCurrent day of the month: 7\n\nmessage(\"Current day of the week (as a number): \", wday(now()))\n\nCurrent day of the week (as a number): 3\n\nmessage(\"Current day of the week (as text): \", wday(now(), label = TRUE))\n\nCurrent day of the week (as text): Tue\n\nmessage(\"Current hour of the day: \", hour(now()))\n\nCurrent hour of the day: 14\n\nmessage(\"Current minute: \", minute(now()))\n\nCurrent minute: 10\n\nmessage(\"Current second: \", second(now()))\n\nCurrent second: 40.7392361164093\n\n\nIt is sometimes useful to be able to add to or subtract from dates. For example, if you wanted to filter a dataset so that only records from the past 28 days were included, you would need to work out the date 28 days ago. We can do this with a group of functions from lubridate that store a period of time that we can then add to or subtract from an existing date. These functions are years(), months(), weeks(), days(), hours(), minutes(), and seconds().\n\nIn the lubridate package, functions that are used to extract parts of a date are singular, e.g. day(), month(), year(). Functions that are used to manipulate dates by adding or subtracting from them are plural, e.g.  days(), months(), years(). So, for example, you would use the code month(now()) to extract the month (as a number between 1 and 12) from the current date but the code now() + months(1) to find out what the date and time will be one month from now.\n\nTo subtract 28 days from today’s date (which we can retrieve with the today() function), we would use today() - days(28).\n\nmessage(str_glue(\"Today is {today()} and 28 days ago was {today() - days(28)}\"))\n\nToday is 2025-01-07 and 28 days ago was 2024-12-10\n\n\nAdding or subtracting periods from dates can be very useful when combined with the filter() function from the dplyr() package. For example, if we had a dataset of crimes stored in an object called crimes and wanted to extract only those that occurred in the most-recent seven days, we could do this:\n\nfilter(crimes, occur_date &gt;= today() - days(7))\n\n# A tibble: 52 × 5\n   incident_key occur_date murder longitude latitude\n          &lt;dbl&gt; &lt;date&gt;     &lt;lgl&gt;      &lt;dbl&gt;    &lt;dbl&gt;\n 1    192545751 2025-01-04 FALSE      -73.9     40.8\n 2    193418291 2025-01-02 FALSE      -73.9     40.8\n 3    193694861 2024-12-31 FALSE      -73.9     40.9\n 4    194817576 2025-01-04 FALSE      -73.8     40.9\n 5    195013387 2025-01-02 FALSE      -73.9     40.8\n 6    196000890 2025-01-01 FALSE      -73.9     40.9\n 7    196315233 2025-01-06 FALSE      -73.9     40.8\n 8    196414298 2025-01-03 FALSE      -73.9     40.8\n 9    196525187 2025-01-04 TRUE       -73.9     40.8\n10    196582020 2025-01-02 FALSE      -73.9     40.8\n# ℹ 42 more rows\n\n\nIf we wanted to extract crimes that occurred between two dates, we can use the between() function from dplyr, which returns either TRUE or FALSE depending on whether each item in the first argument is between the values given in the second and third arguments (inclusive).\n\nFor example, complete the following code by replacing the text ____ and ____ with ymd() functions to extract only crimes occurring between 3 and 31 December 2024 inclusive.\n\nfilter(crimes, between(occur_date, ____, ____))\n\nError in parse(text = input): &lt;text&gt;:1:37: unexpected input\n1: filter(crimes, between(occur_date, __\n                                        ^\n\n\n\n# Add the dates to the following code in YYYY-MM-DD format (because you are \n# using the function `ymd()`)\nfilter(crimes, between(occur_date, ymd(\"\"), ymd(\"\")))\n\n# A tibble: 0 × 5\n# ℹ 5 variables: incident_key &lt;dbl&gt;, occur_date &lt;date&gt;, murder &lt;lgl&gt;,\n#   longitude &lt;dbl&gt;, latitude &lt;dbl&gt;\n\n\n\nWhen filtering based on dates or times, it is important to understand that R can store dates in two ways: as a date object or as a date-time object (shown as &lt;dttm&gt; when we print a tibble). Date variables store only a date with no time, while date-time variables always include a time component, even if the data doesn’t contain any information about time. If we store a variable that only has information about the date of an event as a date-time variable, R will silently add the time midnight to each date. This is important because if we compare a date variable to a date-time variable, R will silently convert the dates to date-times with the times set to midnight. If we are trying to filter crimes between two times, this might not be what we want. For example, if we used the code:\nbetween(offence_date, ymd(\"2021-01-01\"), ymd(\"2021-01-31\"))\nto extract all the crimes that occurred in January 2021, that would work as we expected only if offence_date was a date variable. If the offence_date column instead held dates and times, R would filter the data as if we had specified:\nbetween(offence_date, ymd_hm(\"2021-01-01 00:00\"), ymd_hm(\"2021-01-31 00:00\"))\nwhich would exclude any crimes that occurred on 31 January (except those occurring at exactly midnight). To deal with this problem, we can either check to make sure the variables we are filtering on are date variables, convert them to date variables using the as_date() function, or assume that they might be date-time variables and specify the exact time that we want as the end of our range. For example, specifying:\nbetween(offence_date, ymd_hm(\"2021-01-01 00:00\"), ymd_hm(\"2021-01-31 23:59\"))\nor\nbetween(as_date(offence_date), ymd(\"2021-01-01\"), ymd(\"2021-01-31\"))\nwould allow us to select all the crimes occurring in January 2021.",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Mapping crime over time</span>"
    ]
  },
  {
    "objectID": "16_mapping_time/index.html#showing-change-over-time",
    "href": "16_mapping_time/index.html#showing-change-over-time",
    "title": "16  Mapping crime over time",
    "section": "16.3 Showing change over time",
    "text": "16.3 Showing change over time\nOne common task in crime analysis is to show how crime changes over time. The simplest way to do this is to produce a time-series chart. For example, we can see how the frequency of aggravated assaults recorded by police in Chicago has changed over time:\n\n\n\nIn this section we will learn how to construct a time-series chart like this. To illustrate this process, we will use an object called assaults that contains records of aggravated assaults in Chicago from 2010 to 2019.\n\n\n\n\n\n\n\n\ndate\nloc_cat\nlongitude\nlatitude\ndistrict\n\n\n\n\n2010-01-01 00:05:00\nresidence\n-87.6277\n41.7922\n2\n\n\n2010-01-01 00:12:00\nstreet\n-87.6683\n41.7513\n6\n\n\n2010-01-01 00:30:00\nhotel\n-87.6242\n41.8727\n1\n\n\n2010-01-01 00:30:00\nstreet\n-87.6478\n41.7536\n6\n\n\n2010-01-01 00:54:00\nstreet\n-87.6446\n41.7720\n7\n\n\n2010-01-01 01:15:00\nstreet\n-87.7311\n41.8984\n11\n\n\n\n\n\n\n\nThe first task in charting the frequency of crime is to choose a temporal unit of analysis. For example, the chart above counts the number of crimes each week. Weeks are often a good choice as units for counting crimes, since all weeks are the same length and because many human activities have a weekly cycle (e.g. people do different things at weekends than on weekdays, even though which days count as weekdays differs across cultures).\n\nMonths are much less useful than weeks as a temporal unit of analysis because months differ in length, so monthly counts of crime will look like they show some variation even if the amount of crime occurring each day remains constant. For example, if exactly 10 crimes occur every day throughout February and March, there will be 280 or 290 crimes in February (depending on whether it is a leap year) but 310 in March. In these circumstances, it will look like the volume of crime increased by 11% between February and March, not because the rate at which crimes occurred increased but because March is 11% longer than February.\nMonths are also less useful because they contain different numbers of each day of the week (e.g. one month might have four Fridays while the next has five) and crimes are typically concentrated on particular days of the week (more on that later). Avoid using monthly counts of crime unless you have no choice because the only data you have available is monthly counts.\n\nTo count the number of crimes occurring each week we can use the count() function from the dplyr package. But before doing that, we have to allocate each crime to a week so that we can then count those weeks rather than counting days. To do this we use the floor_date() function from the lubridate package. This function rounds dates down to the start of a specified unit of time, in this case a week. By default, floor_date() treats Sunday as the start of the week and so if the specified unit of time is a week, all dates will be rounded down to the date of the previous Sunday.\nfloor_date() works on date variables, so if we want to use it on a date-time variable we should first convert it to a date variable using the as_date() function from lubridate. So to convert a date-time stored in a variable called date into the date of the first day of that week, we would use the code floor_date(as_date(date), unit = \"week\").\nOne complication of counting incidents by week is that our data might not fit neatly into calendar weeks. For example, if we have data for a particular year and that year started on a Tuesday, the first weekly count will only have data for five days and it will look like there were fewer crimes that week in comparison to other weeks. This could be misleading, since this week only looks like it has less crime because we don’t have data for the whole week. The same problem can happen with the last week of data, too. To deal with this, after counting the crimes by week we will remove the first and last row of the data using the slice() function from the dplyr package.\n\nassault_weekly_counts &lt;- assaults |&gt; \n  mutate(week_date = floor_date(as_date(date), unit = \"week\")) |&gt; \n  count(week_date, name = \"count\") |&gt; \n  # The code `(n() - 1)` gives us the row number of the second-to-last row in \n  # the data because `n()` returns the number of rows in the data. Note the\n  # parentheses!\n  slice(2:(n() - 1))\n\nhead(assault_weekly_counts)\n\n# A tibble: 6 × 2\n  week_date  count\n  &lt;date&gt;     &lt;int&gt;\n1 2010-01-03   234\n2 2010-01-10   300\n3 2010-01-17   303\n4 2010-01-24   257\n5 2010-01-31   276\n6 2010-02-07   264\n\n\nNow we have weekly counts of aggravated assaults, we can plot them on a chart. The simplest way to do this would be to create a line chart using ggplot() with geom_line().\n\nggplot(assault_weekly_counts, aes(x = week_date, y = count)) +\n  geom_line() +\n  theme_minimal()\n\n\n\n\n\n\n\n\nThis plot is fine, but there are several things that we can do to make it better. Most importantly, there seems to be lots of short-term fluctuation in the frequency of crime (e.g. from one week to another). While this variation is real, we often refer to it as noise because it can obscure the signal of longer-term trends (this terminology originally came to statistics – and therefore crime analysis – from radio engineering and has become common over time).\nWe can reduce the visual impact of this short-term variation on our plot by adding a line showing a moving average (also called a rolling average or rolling mean) of the count of crime over time. A moving average is the average (or mean) of the count of crimes in the current week and a given number of adjacent (in this case, previous), weeks.\n\n\n\nSince moving averages show the average count of crime over several weeks, they are less influenced by week-to-week variation. To calculate a moving average we have to choose how many weeks to include (known as the window of the moving average). The more weeks we include in the window, the smoother the values will appear from week to week and the more short-term variation will be obscured. There is a balance to be struck between making longer-term trends clear and obscuring genuine short-term variation, so you should experiment with different window lengths to ensure you are not over-smoothing.\nWe can calculate moving averages in R with the slide_dbl() function from the slider package (so called because its functions slide along a series of values). slide_dbl() can calculate several types of moving averages, so we specify that it should use the mean() function to calculate the average by specifying .f = mean (note the lack of parentheses after mean). We use the .before argument (note the .) to specify how many weeks before the current week to include in our moving average. So if we wanted to calculate a four-week moving average (i.e. the current week and the previous three weeks), we would specify .before = 3. We also specify .complete = TRUE to stop slide_dbl() from trying to calculate averages for the first few weeks in our data, because we don’t have the necessary data from previous weeks (i.e. before the start of our data) that we would need to make these averages accurate. slide_dbl() will use NA as the moving average value for those weeks, so we later need to specify na.rm = TRUE to tell ggplot() to ignore these when we plot the data.\nOnce we’ve calculated a moving average, we can show this using the line on our chart and show the individual weekly counts as small light-grey dots to show how much short-term variation there is in the data.\n\nlibrary(slider)\n\nassault_weekly_counts |&gt; \n  mutate(moving_avg = slide_dbl(count, mean, .before = 3, .complete = TRUE)) |&gt; \n  ggplot() +\n  geom_point(aes(x = week_date, y = count), colour = \"grey75\", size = 0.75) +\n  geom_line(\n    aes(x = week_date, y = moving_avg, colour = \"black\"), \n    na.rm = TRUE, \n    key_glyph = \"timeseries\"\n  ) +\n  scale_x_date(date_breaks = \"1 year\", date_labels = \"%b\\n%Y\") +\n  scale_y_continuous(limits = c(0, NA)) +\n  scale_colour_identity(\n    labels = c(\"black\" = \"four-week moving average\"),\n    guide = guide_legend()\n  ) +\n  labs(\n    x = NULL,\n    y = \"weekly count of aggravated assaults\",\n    colour = NULL\n  ) +\n  theme_minimal() +\n  theme(\n    legend.position = \"bottom\",\n    panel.grid.minor.x = element_blank()\n  )\n\n\n\n\n\n\n\n\nExperiment with the effect of setting a longer or shorter window for the moving average by specifying larger or smaller values of the .before argument to slide_dbl(). For example, create an eight-week moving average by specifying .before = 7. What happens to the apparent seasonal variation in the number of assaults if you create an annual moving average by specifying .before = 52?\nYou may have noticed that in this code we have also made some changes to the appearance of the plot to make it easier to read. Specifically, we have:\n\nAdded a title for the y axis and removed the x axis and legend titles using the labs() function.\nChanged the labels on the x axis using scale_x_date(). In this case, we have used the argument date_breaks = \"1 year\" to specify that we want a label at the start of each year and the argument date_labels = \"%b\\n%Y\" to specify that we want each label to consist of an abbreviated month name (using the code %b), a new line (the code \\n as in a previous tutorial) and a four-digit year (using the code %Y). You can find a full list of codes used that can be used to specify different parts of a date and time by typing ?strptime in the R console.\nRemoved some of the vertical grid lines by setting the panel.grid.minor.x argument to theme() to equal element_blank().\nForced the y axis to begin at zero by specifying limits = c(0, NA), remembering that NA in this context means to use the default value.\nSpecified that we want the black line to be represented in the legend by a line that looks like a time series by specifying key_glyph = \"timeseries\".\n\nWe have also added a legend to explain what the black line shows. The code need to do this is slightly awkward, since ggplot() would not normally produce a legend for aesthetics (in this case, the colour of the line) that have only one value. To force ggplot() to add a legend, we:\n\nSpecify the colour of the line (i.e. colour = \"black\") not as an argument to the function geom_line() as we normally would but as one of the aesthetics specified using aes().\nSpecify that ggplot() should treat the value colour = \"black\" as a literal colour name rather than as a reference to a column in the data (which is how the arguments to aes() are normally interpreted). To do this we add scale_colour_identity() to the ggplot() stack, because identity is the jargon used in R to say ‘keep this value exactly as it is’.\nWithin scale_colour_identity(), specify a label to correspond to the black line by setting the labels argument.\nSpecify guide = guide_legend() to tell ggplot() to add a legend corresponding to the black line because aesthetics styled using functions in the scale_*_identity() family do not produce a legend entry by default.\n\nDoing all this is obviously tedious, but makes for a better chart.\n\n16.3.1 Showing repeating patterns over time\nWe have already seen that there is seasonal variation in the number of aggravated assaults in Chicago. As is very common for assaults, there are more offences in the summer and fewer in the winter. We can see this in the time-series chart we have already produced, but it’s hard to see the detail. For example, we might want to know how consistent this seasonal variation is across different years. Is it, for example, consistent enough that the local police might want to restrict the number of officers who can take holidays in certain weeks of the year to maximise the number of officers available when crime is likely to be highest?\nTo do this we can create a seasonal chart. This can be used to show any type of repeating variation, but is often used to show patterns across a year (hence the name). To create a seasonal plot we need to add a variable to our data specifying which year each weekly count belongs to, which we can do by using the year() function to extract the year from the offence dates. We can do this at the same time as we calculate the moving averages. Once we’ve done that, we can specify that we want our chart to have a separate line for each year by setting group = year inside the aes() function that controls how the data are shown on the chart, making each year a different colour using colour = year.\n\nassault_weekly_counts |&gt; \n  mutate(\n    moving_avg = slide_dbl(count, mean, .before = 3, .complete = TRUE),\n    year = year(week_date)\n  ) |&gt; \n  ggplot(aes(x = week_date, y = moving_avg, colour = year, group = year)) +\n  geom_line(na.rm = TRUE, key_glyph = \"timeseries\") +\n  scale_x_date(date_breaks = \"1 year\", date_labels = \"%b\\n%Y\") +\n  scale_y_continuous(limits = c(0, NA)) +\n  scale_colour_continuous(breaks = c(2010, 2019)) +\n  labs(\n    x = NULL,\n    y = \"weekly count of aggravated assaults\",\n    colour = NULL\n  ) +\n  theme_minimal() +\n  theme(\n    panel.grid.minor.x = element_blank()\n  )\n\n\n\n\n\n\n\n\nThe result might not be what you expected. Although the grouping of the lines by year has worked (there is a break between the lines at the start of each year), it’s no easier to compare the seasonal patterns across years. Comparing years would be much easier if we superimpose the weekly counts for each year on top of one another.\nTo do this, we need to trick ggplot() into plotting all the weekly counts as if they occurred in a single year, so the counts appear in the same locations on the horizontal axis of the chart, whichever year they occurred in. We can do this by creating a pseudo-date value for each weekly count which has the same month and day of the month as the original date, but a single year across all the rows in the data. We will create this pseudo date by extracting the month and day of the month using month() and mday(), then creating a new date with make_date(). We will also change the labels on the horizontal axis using the date-formatting code %e\\n%b – %e means the day of the month and %b means the abbreviated name of the month.\n\nassault_weekly_counts |&gt; \n  mutate(\n    moving_avg = slide_dbl(count, mean, .before = 3, .complete = TRUE),\n    year = year(week_date),\n    # By only specifying the `month` and `day` arguments to `make_date()` we\n    # will create a date in 1970 (the year that R uses by default), but that \n    # doesn't matter because we are not going to show the year on the chart\n    pseudo_date = make_date(month = month(week_date), day = mday(week_date))\n  ) |&gt; \n  ggplot(aes(x = pseudo_date, y = moving_avg, colour = year, group = year)) +\n  geom_line(na.rm = TRUE, key_glyph = \"timeseries\") +\n  scale_x_date(date_breaks = \"1 month\", date_labels = \"%e\\n%b\") +\n  scale_y_continuous(limits = c(0, NA)) +\n  scale_colour_continuous(breaks = c(2010, 2019)) +\n  labs(\n    x = NULL,\n    y = \"weekly count of aggravated assaults\",\n    colour = NULL\n  ) +\n  theme_minimal() +\n  theme(\n    panel.grid.minor.x = element_blank()\n  )\n\n\n\n\n\n\n\n\nFrom this chart we can see that assaults consistently peak in July, although in one year they peaked slightly earlier in late June and in one year slightly later in August. At the other end of the year, weekly counts of assaults are almost always least frequent in late January and throughout February before starting to increase quite rapidly in March.\nAs well as showing seasonal variation, we can use the same technique to understand variation over other periods of time. For example, since we know a lot of human activities follow weekly patterns, we might want to produce a chart showing the number of crimes in each hour on each day of the week.\nTo do this, we:\n\nExtract the weekday and hour components of each date using wday() and hour().\nCount the total number of crimes occurring in each hour of each day across all ten years of the data using count().\nCreate a pseudo-date-time using make_datetime().\nCreate a chart with appropriate labels on the horizontal axis and a suitable qualitative colour scheme to show the days of the week using scale_colour_brewer().\n\nWe can do this in a single piece of code.\n\nassault_hourly_counts &lt;- assaults |&gt; \n  mutate(wday = wday(date, label = TRUE), hour = hour(date)) |&gt; \n  count(wday, hour, name = \"count\") |&gt; \n  # By only setting the `hour` argument to `make_datetime()` we will create a\n  # date-time on 1 January 1970, but that doesn't matter because we will not \n  # show the date on the chart\n  mutate(pseudo_date = make_datetime(hour = hour))\n\nggplot(\n  assault_hourly_counts, \n  aes(x = pseudo_date, y = count, colour = wday, group = wday)\n) +\n  geom_line(key_glyph = \"timeseries\") +\n  scale_x_datetime(date_breaks = \"2 hours\", date_labels = \"%H:%M\") +\n  scale_y_continuous(limits = c(0, NA), labels = scales::comma_format()) +\n  scale_colour_brewer(type = \"qual\") +\n  labs(\n    x = NULL,\n    y = \"hourly total of aggravated assaults, 2010-2019\",\n    colour = NULL\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nOn this chart you can see that there are two distinct temporal patterns of assaults on different days of the week. Between Mondays and Thursdays, assaults peak between about 14:00 and 21:00 before reducing to a very-low level at about 05:00. At the weekend, the picture is different: assaults peak between midnight and 02:00 on both Saturdays and Sundays (i.e. very late on Friday and Saturday evenings).\nThis chart probably uses the maximum number of different colours for days of the week that we could use before some of the colours became too similar to one another to be distinguishable. But even with this many colours, it might not be easy for a colour-blind person to translate between the colours of the lines and the colours in the legend. When you find that there are too many colours on a chart, this is a good sign that you should consider using small-multiple charts instead. Fortunately, we can do this by adding a column to the data specifying whether each day is a weekday or on a weekend, then adding facet_grid() to our ggplot() stack.\n\nassault_hourly_counts |&gt; \n  mutate(weekend = if_else(wday %in% c(\"Sat\", \"Sun\"), \"Sat–Sun\", \"Mon–Fri\")) |&gt; \n  ggplot(aes(x = pseudo_date, y = count, colour = wday)) +\n  geom_line(linewidth = 1) +\n  # Assign the facets to rows so that we can compare the same time on different\n  # days more easily (change `rows` to `cols` to see the alternative)\n  facet_grid(rows = vars(weekend)) +\n  scale_x_datetime(date_breaks = \"2 hours\", date_labels = \"%H:%M\") +\n  scale_y_continuous(limits = c(0, NA), labels = scales::comma_format()) +\n  scale_fill_brewer(type = \"qual\") +\n  labs(\n    x = NULL,\n    y = \"hourly total of aggravated assaults, 2010-2019\",\n    fill = NULL\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nThis shows the two distinct patterns (weekdays and weekends) more clearly, while also letting us see that Friday is not like other weekdays since the peak in assaults continues later in the evening. Now that we’ve created this chart using facet_grid(), we could also add columns to show the same patterns in different areas, such as police districts or local neighbourhoods, or during different seasons.",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Mapping crime over time</span>"
    ]
  },
  {
    "objectID": "16_mapping_time/index.html#how-to-map-change-over-time",
    "href": "16_mapping_time/index.html#how-to-map-change-over-time",
    "title": "16  Mapping crime over time",
    "section": "16.4 How to map change over time",
    "text": "16.4 How to map change over time\nTime-series or seasonal charts are often the best way to show change in the frequency of crime over time. But it can also be useful to show maps of the patterns of crimes at different points in time. We might, for example, want to show the density of crime in an area for different periods in time.\nChoosing how to divide up time into periods is an important step in this process, because in doing so we are converting a continuous variable (time) into a number of categories (periods of time). Whenever we convert an continuous variable to a categorical one we inevitably lose information. For example, if we decided to categorise the maximum temperature every day as being either ‘hot’ or ‘cold’, we would lose a lot of information about whether a particular day was moderately hot, very hot, etc. The same is true of time, since by splitting time into periods (hours, days, weeks, etc.) we lose information about variations within each period. This is inevitable, since we can’t produce infinite maps showing all the infinite moments in time, but it’s the reason why choosing periods carefully is important.\nWhen choosing a period over which to count crime, it is important not to just use default periods like the day from 00:00 to 23:59 just because that is the standard definition of a day. As we saw in the previous section, the peaks of many types of crime like assaults cross over the boundaries between days because the peak frequency is late in the evening. For this reason it may be better to, for example, define a day as a period from 05:00 to 04:59 and count the number of crimes within each day according to that definition. This takes advantage of the fact that very few crimes concentrate in the early hours of the morning.\nSometimes, it will be easy to choose periods because they will be dictated by the purpose for which you’re creating the map. In this section we will create separate maps showing the density of aggravated assaults in a part of Chicago for each of the standard Chicago Police shifts of 06:00 to 13:59, 14:00 to 21:59 and 22:00 to 05:59 (bearing in mind that the actual hours some officers work may differ slightly).\nTo do this, we will estimate the density of assaults separately for each shift period, the combine the three density layers and plot them on small-multiple maps. First, we create a new object containing data for the Chicago Police districts we are interested in with a column showing which police shift each assault occurred in.\nWe can construct this column using the case_when() function from dplyr. case_when() allows us to specify any number of tests that we can apply to our data – when a test is passed the function assigns the corresponding label to that row (the label is separated from the test by a tilde character ~). case_when() is like recode(), but for when we need to test for more-complicated things than just whether a variable has a particular value.\n\nassaults_by_shift &lt;- assaults |&gt; \n  filter(district %in% c(1, 12, 18)) |&gt; \n  mutate(\n    shift = case_when(\n      between(hour(date), 6, 13) ~ \"06:00 to 13:59\",\n      between(hour(date), 14, 21) ~ \"14:00 to 21:59\",\n      hour(date) &gt;= 22 | hour(date) &lt; 6 ~ \"22:00 to 05:59\",\n      TRUE ~ NA_character_\n    )\n  )|&gt; \n  # Convert the data to an SF object\n  st_as_sf(coords = c(\"longitude\", \"latitude\"), crs = \"EPSG:4326\") |&gt; \n  # Transform it to a co-ordinate reference system based on metres\n  st_transform(\"EPSG:26916\")\n\nhead(assaults_by_shift)\n\nSimple feature collection with 6 features and 4 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 444951.9 ymin: 4635831 xmax: 449271.4 ymax: 4641406\nProjected CRS: NAD83 / UTM zone 16N\n# A tibble: 6 × 5\n  date                loc_cat    district shift                  geometry\n  &lt;dttm&gt;              &lt;chr&gt;         &lt;dbl&gt; &lt;chr&gt;               &lt;POINT [m]&gt;\n1 2010-01-01 00:30:00 hotel             1 22:00 to 05… (448202.2 4635831)\n2 2010-01-01 01:45:00 street           18 22:00 to 05…   (446518 4641406)\n3 2010-01-01 01:48:00 hotel             1 22:00 to 05… (448202.2 4635831)\n4 2010-01-01 02:00:00 government       18 22:00 to 05… (449271.4 4637966)\n5 2010-01-01 02:59:00 leisure          12 22:00 to 05… (444951.9 4637265)\n6 2010-01-01 05:00:00 residence        18 22:00 to 05… (447278.5 4639968)\n\n\n\n\nWhy did we include TRUE ~ NA_character_ in case_when()?\n\n\ncase_when() uses a series of true/false tests to create a variable, usually based on the values of other columns in the data. To make sure that every row in the dataset is matched by at least one of the tests, it is common practice to include a final test that is always true. The easiest way to do this is to simply create a test with the fixed value TRUE, since TRUE is always true! This catch-all test must be the last test within the case_when() function, because case_when() runs the tests in the order in which they are given and stops testing a given row in the data as soon as a test produces a true value.\nIn this case, we will set the label for this last test to be NA (in fact, the special value NA_character_ because case_when() only accepts labels that are characters) to catch any rows that have missing values of date or aren’t matched by any of our tests. Since our tests between them cover all the hours of the day, there shouldn’t be any rows that are not matched by at-least one test, but including a catch-all test makes it easier to catch any problems with our code.\nThere’s lot’s more to learn about the case_when() function: you can find out more by looking at the case_when() page on the package website.\n\n\n\nThe next step is to produce a kernel density (KDE) layer for assaults occurring in each shift. In a previous tutorial we learned how to do this using the sfhotspot package. We could run the hotspot_kde() function from that package three times to create the KDE layers for each shift, but there is a simpler way to apply the same function to different parts of a dataset using the group_modify() function from dplyr.\nNormally, when we use a function to modify a dataset, that function is applied to the dataset as a whole. With group_modify(), we can apply a function to different parts of a dataset separately but still get the result as a single tibble with a column showing which group each row relates to (which is what we need to produce a map).\ngroup_modify() needs two inputs. The first is a grouped dataset. We already know how to specify which column in a dataset represents which group each row is in using the group_by() function. The second input to group_modify() is the name of the function we want to use to modify each group in the data. The only complication is that we have to provide the details of the function that group_modify() should use in a slightly unusual format called an anonymous function. You can see this in the block of code below. You do not need to understand the details of how an anonymous function works, but you should note two things:\n\nAnonymous functions start with the ~ character.\nIn an anonymous function used within group_modify(), you can use the special value .x (note the dot) to represent the data in each group.\n\nSo this code:\nsome_data |&gt; \n  group_by(some_variable) |&gt; \n  group_modify(~ hotspot_kde(.x))\nMeans ‘take the dataset some_data, group it according to the values in the some_variable column and apply the hotspot_kde() function separately to each group’. The only thing left for us to do then is to use ungroup() to remove the grouping from the result produced by group_modify() and convert that result back to an SF object using st_as_sf(). Don’t worry about specifying any arguments to st_as_sf() – R will extract information such as the co-ordinate reference system from the geometry column of the data automatically.\n\nlibrary(sfhotspot)\n\nkde_by_shift &lt;- assaults_by_shift |&gt; \n  group_by(shift) |&gt; \n  group_modify(\n    ~ hotspot_kde(.x, cell_size = 200, bandwidth_adjust = 0.75, quiet = TRUE)\n  ) |&gt; \n  ungroup() |&gt; \n  st_as_sf() |&gt; \n  # Clip the result to the boundary of Chicago, which is already stored in the \n  # `cpd_central` object\n  st_intersection(cpd_central)\n\nWarning: attribute variables are assumed to be spatially constant throughout\nall geometries\n\nhead(kde_by_shift)\n\nSimple feature collection with 6 features and 4 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 446980 ymin: 4637342 xmax: 447685.6 ymax: 4637558\nProjected CRS: NAD83 / UTM zone 16N\n# A tibble: 6 × 5\n  shift              n   kde district_no                                geometry\n  &lt;chr&gt;          &lt;dbl&gt; &lt;dbl&gt;       &lt;dbl&gt;                           &lt;POLYGON [m]&gt;\n1 06:00 to 13:59     0  161.          18 ((447085.6 4637358, 447085.6 4637355, …\n2 06:00 to 13:59     4  204.          18 ((447085.6 4637358, 447112.6 4637358, …\n3 06:00 to 13:59     0  147.          18 ((447085.6 4637558, 447085.6 4637358, …\n4 06:00 to 13:59     0  186.          18 ((447085.6 4637558, 447285.6 4637558, …\n5 06:00 to 13:59     2  225.          18 ((447285.6 4637558, 447485.6 4637558, …\n6 06:00 to 13:59     4  257.          18 ((447485.6 4637558, 447685.6 4637558, …\n\n\n\n\nHow to anonymous functions work?\n\n\nAnonymous functions in R (and in many other programming languages) are a compact way of creating a new function that we can then immediately use to modify a dataset.\nWe can create our own functions in R by using the function() function. For example, if we wanted to create our own version of the head() function that printed the first 10 rows of a dataset – rather than the default six rows printed by head() – we could create a function called head10():\nhead10 &lt;- function(x) {\n  head(x, n = 10)\n}\nWhen we call the new head10() function on a dataset, R will take whatever data we provide to that function and run all the code inside the braces {} above using that data. We can use this new function anywhere in our code after we’ve created it. For example, if we had a dataset called some_data we could view the first 10 rows of it by creating an then using our new function:\nhead10 &lt;- function(x) {\n  head(x, n = 10)\n}\n\nhead10(some_data)\nNote that inside our new function, the dataset is referred to by the name that we gave it in the parentheses () after the word function, not by the name of the dataset itself. This is important because it means our custom function will work regardless of what a particular dataset is called.\nWe could include lots of lines of code inside our new function, but in this case there is just one – head(x, n = 10). When we want to create a function that consists of a single line of code, we can dispense with the braces and put the whole function on one line:\nhead10 &lt;- function(x) head(x, n = 10)\n\nhead10(some_data)\nCreating a new function and giving it a name is useful if we want to use the function several times in our code. But in the case of the function we need to provide to group_modify() in our code, we only need to use the new function once. In this case, we can dispense with the need to give our new function a name, and instead create an anonymous function. To do this, we replace function(x) with the ~ character.\nThis makes the definition of our custom function even faster, but means we can only use it inside a function such as group_modify() or map() that needs a function definition to work. Using an anonymous function also means we need some way of referring to our dataset inside the new function, since we don’t have the opportunity to define it inside parentheses after the word function. Fortunately, group_modify() understands that if we refer to .x (note the dot) inside our anonymous function, that should be interpreted as referring to our dataset.\nThere is a lot more you could learn about creating your own functions in R. If you are interested, you might want to work through this lesson on Creating Functions.\n\n\n\nNow we have a KDE layer for each shift, we can create three maps for the three shifts by adding facet_wrap() to a ggplot() stack.\n\nlibrary(ggspatial)\n\nggplot() +\n  annotation_map_tile(type = \"cartolight\", zoomin = 0, progress = \"none\") +\n  geom_sf(\n    aes(fill = kde), \n    data = kde_by_shift, \n    alpha = 0.75, \n    colour = NA\n  ) +\n  geom_sf(\n    data = filter(cpd_districts, district_no %in% c(1, 12, 18)), \n    colour = \"grey33\", \n    fill = NA\n  ) +\n  facet_wrap(vars(shift)) +\n  scale_fill_distiller(\n    breaks = range(pull(kde_by_shift, kde)),\n    labels = c(\"lower\", \"higher\"),\n    direction = 1\n  ) +\n  labs(\n    fill = \"density of aggravated assaults\"\n  ) +\n  theme_void() +\n  theme(\n    legend.position = \"bottom\",\n    legend.title.align = 1\n  )\n\nWarning: The `legend.title.align` argument of `theme()` is deprecated as of ggplot2\n3.5.0.\nℹ Please use theme(legend.title = element_text(hjust)) instead.\n\n\n\n\n\n\n\n\n\nOn this map we can see that some places have a relatively high density of assaults throughout all three shifts, but others only have a high density at certain times. We can perhaps make this clearer by only showing the grid cells with the highest estimated density of assaults during each shift. We can do this using the slice_max() function from dplyr, which allows us to extract the rows in a dataset with the highest values of a particular variable. In this case we will use slice_max() together with group_by() to get the rows with the highest values separately for each shift rather than those with the highest values across all three shifts combined.\n\nkde_shift_highest &lt;- kde_by_shift |&gt; \n  group_by(shift) |&gt; \n  slice_max(order_by = kde, n = 10)\n\nggplot() +\n  annotation_map_tile(type = \"cartolight\", zoomin = 0, progress = \"none\") +\n  geom_sf(\n    aes(fill = kde), \n    data = kde_shift_highest, \n    alpha = 0.75, \n    colour = NA,\n    fill = \"red2\"\n  ) +\n  geom_sf(\n    data = filter(cpd_districts, district_no %in% c(1, 12, 18)), \n    colour = \"grey33\", \n    fill = NA\n  ) +\n  facet_wrap(vars(shift)) +\n  theme_void()\n\n\n\n\n\n\n\n\nThis map makes it very clear that the grid cells with the highest densities of aggravated assaults are very similar in the daytime and evening shifts, in both places being concentrated in the downtown area known as The Loop. For the overnight shift, however, the cells with the highest densities are on the other side of the Chicago River in the River North neighbourhood. A map like this might be particularly useful if the resources available to respond to a crime problem were very limited and so could only be deployed in the places where the problem was worst – this is often the case because crime-prevention resources are often very limited.",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Mapping crime over time</span>"
    ]
  },
  {
    "objectID": "16_mapping_time/index.html#making-animated-maps",
    "href": "16_mapping_time/index.html#making-animated-maps",
    "title": "16  Mapping crime over time",
    "section": "16.5 Making animated maps",
    "text": "16.5 Making animated maps\nThe small-multiple map we have produced of aggravated-assault hotspots in Chicago is useful, especially for policing because it uses periods based on police shifts. But aggregating crimes into only three temporal periods inevitably throws away a lot of information about when crime happens. For example, at what time of night does the area of highest assault density move across the river from The Loop to River North?\nWe could produce a series of small-multiple maps showing shorter periods (meaning more small multiples). For example, we could show one small-multiple map for each hour of the day. However, this would make each map very small and it would be hard to see the details of locations on each map.\nOne alternative is to produce an animated map with each frame in the animation representing the map for each hour. We can do this using the gganimate package.\nThe first step in producing an animated map is to create a KDE layer for each hour of the day. The code for this is the same as for the code we have already used to produce the KDE layers for each shift, except that we create a variable for hour of the day rather than police shift. Because an animated map of hours of the day needs 24 KDE layers, in this case it is particularly useful to use group_modify() to avoid having to create 24 different objects and then binding them together.\n\nlibrary(gganimate)\n\nassaults_by_hour &lt;- assaults |&gt; \n  filter(district %in% c(1, 12, 18)) |&gt; \n  mutate(\n    # Create nicely formatted labels for each hour\n    hour_name = str_pad(hour(date), width = 2, pad = \"0\"),\n    hour_name = str_glue(\"{hour_name}:00 to {hour_name}:59\")\n  ) |&gt; \n  st_as_sf(coords = c(\"longitude\", \"latitude\"), crs = \"EPSG:4326\") |&gt; \n  # Convert the data to a suitable co-ordinate system for Chicago\n  st_transform(\"EPSG:26916\")\n\nhour_layers &lt;- assaults_by_hour |&gt; \n  group_by(hour_name) |&gt; \n  group_modify(\n    ~ hotspot_kde(.x, cell_size = 200, bandwidth_adjust = 0.75, quiet = TRUE)\n  ) |&gt; \n  ungroup() |&gt; \n  st_as_sf() |&gt; \n  st_intersection(cpd_central)\n\nWarning: attribute variables are assumed to be spatially constant throughout\nall geometries\n\n# Extract only the 10 cells with the highest density in each hour\nhour_highest &lt;- hour_layers |&gt; \n  group_by(hour_name) |&gt; \n  slice_max(order_by = kde, n = 10)\n\n\nIt is possible that this code will time out and give an error saying Your code ran longer than the permitted timelimit for this exercise. – if that happens then just continue with the rest of the tutorial as normal.\n\n\nThis code is likely to take a while to run, because it calculates KDE values separately for every grid cell for each of 24 hours.\n\nWe can now use the hour_highest object as the basis for a new base map that only includes the areas of the highest density (so they appear larger on the animated map).\nTo create an animated map we use the transition_states() function from the gganimate package. transition_states() works in a similar way to facet_wrap(), in that when added to a ggplot() stack it splits the chart or map up into a separate map for each value of one of the variables in the data (in this case, the hour of the day). The only difference is that while facet_wrap() arranges those separate maps next to one another, transition_states() arranges them into an animation.\n\nggplot() +\n  annotation_map_tile(type = \"cartolight\", zoomin = 0, progress = \"none\") +\n  geom_sf(\n    aes(fill = kde),\n    data = hour_highest, \n    alpha = 0.75, \n    colour = NA,\n    fill = \"red2\"\n  ) +\n  geom_sf(data = cpd_central, colour = \"grey33\", fill = NA) +\n  transition_states(states = hour_name) +\n  labs(\n    title = \"Aggravated assaults in downtown Chicago, 2010-2019\",\n    subtitle = \"Areas with most aggravated assaults:\\n{closest_state}\",\n    caption = \"Data from Chicago Police\"\n  ) +\n  theme_void()\n\n\n\n\n\n\n\n\n\n\nI got an error starting Your code ran longer than\n\n\nThis code may take longer to run than is allowed for code inside an interactive tutorial. In which case, you may see the error Your code ran longer than the permitted timelimit for this exercise. If that happens, you can download the Chicago assaults data and create an animated map in an R script of your own using the code in the chunks above. For reference, this code should produce a map looking like this:\n\n\n\n\n\n\nThere is one other function of gganimate we have used in the code used to make this map. You might have noticed that in the map subtitle is the code {closest_state}. This is a special code that gganimate will replace with the current value of the variable in the data that is used to control which facet appears in each frame of the animation. So for this map, {closest_state} will be replaced in the animation with the value of the hour_name variable in the data for each frame in the animation.\nThis animated map is very sensitive to the number of grid cells we choose to extract (in the map above, 10 cells for each hour) – it might look quite different if we had chosen a different number. To show the density of crime in all of downtown Chicago, we can combine the base map and KDE layers we have already created to produce another animated map.\n\nggplot() +\n  annotation_map_tile(type = \"cartolight\", zoomin = 0, progress = \"none\") +\n  geom_sf(aes(fill = kde), data = hour_layers, alpha = 0.75, colour = NA) +\n  geom_sf(data = cpd_central, colour = \"grey33\", fill = NA) +\n  transition_states(states = hour_name) +\n  scale_fill_distiller(\n    breaks = range(pull(hour_layers, kde)),\n    labels = c(\"lower\", \"higher\"),\n    direction = 1\n  ) +\n  labs(\n    title = \"Aggravated assaults in downtown Chicago, 2010-2019\",\n    subtitle = \"Density of aggravated assaults: {closest_state}\",\n    caption = \"Data from Chicago Police\",\n    fill = \"density of\\naggravated\\nassaults\"\n  ) +\n  theme_void()\n\n\n\n\n\n\n\n\n\nThis code typically takes a minute or so to finish running because it has to generate 24 maps and then stitch them together. It is unlikely that the code will finish running before the maximum time that a chunk of code in an R tutorial is allowed to run for before automatically stopping. If you were to run this code in RStudio, the map it would produce would look like this:\n\n\n\n\nWe can save an animated map to a file using the animate() and anim_save() functions. animate() controls the type of file the animation will be saved in (by default, an animated GIF), the height and width of the plot and so on. anim_save() then saves the animation in a file. For example, if we stored the map created above in an object called chicago_downtown_kde_map, we could save it to an animated GIF file.\nanim_save(\n  filename = \"chicago_downtown_agg_assaults.gif\", \n  animation = animate(\n    plot = chicago_downtown_kde_map,\n    height = 800, \n    width = 800, \n    units = \"px\"\n  )\n)",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Mapping crime over time</span>"
    ]
  },
  {
    "objectID": "16_mapping_time/index.html#in-summary",
    "href": "16_mapping_time/index.html#in-summary",
    "title": "16  Mapping crime over time",
    "section": "16.6 In summary",
    "text": "16.6 In summary\n\nIn this tutorial we have learned how to incorporate change over time into our analysis of where crime happens. This is important because the distribution of crime across different places often varies at different times. Being aware of the importance of time when we make maps means we can do things like create small-multiple or animated maps for different time periods, which we could use to make sure that scarce crime-prevention resources are used at the right time as well as in the right place.\n\nThis is the complete code we need to create an animated map of aggravated assaults in Chicago. Think about how you could change it, or what extra information you could add, to make this map as useful as possible to different groups of practitioners and policy makers.\n\n# Load packages\nlibrary(gganimate)\nlibrary(ggspatial)\nlibrary(lubridate)\nlibrary(sf)\nlibrary(sfhotspot)\nlibrary(tidyverse)\n\n\n# Load data --------------------------------------------------------------------\n\n# Aggravated assaults, Chicago, 2010 to 2019\nassaults &lt;- read_csv(\"https://mpjashby.github.io/crimemappingdata/chicago_aggravated_assaults.csv.gz\")\n\nRows: 148636 Columns: 5\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (1): loc_cat\ndbl  (3): longitude, latitude, district\ndttm (1): date\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n# Chicago Police districts 1, 12 and 18\ncpd_central &lt;- read_sf(\"https://mpjashby.github.io/crimemappingdata/chicago_police_districts.kml\") |&gt; \n  st_transform(\"EPSG:26916\") |&gt; \n  mutate(district_no = as.numeric(Name)) |&gt; \n  filter(district_no %in% c(1, 12, 18))\n\n\n# Wrangle data -----------------------------------------------------------------\n\n# Calculate number of assaults each hour\nhour_layers &lt;- assaults |&gt; \n  filter(district %in% c(1, 12, 18)) |&gt; \n  mutate(\n    hour_name = str_pad(hour(date), width = 2, pad = \"0\"),\n    hour_name = str_glue(\"{hour_name}:00 to {hour_name}:59\")\n  ) |&gt; \n  st_as_sf(coords = c(\"longitude\", \"latitude\"), crs = \"EPSG:4326\") |&gt; \n  st_transform(\"EPSG:26916\") |&gt; \n  group_by(hour_name) |&gt; \n  group_modify(\n    ~ hotspot_kde(.x, cell_size = 200, bandwidth_adjust = 0.75, quiet = TRUE)\n  ) |&gt; \n  ungroup() |&gt; \n  st_as_sf() |&gt; \n  st_intersection(cpd_central)\n\nWarning: attribute variables are assumed to be spatially constant throughout\nall geometries\n\n# Create map ---------------------------------------------------------------------\nchicago_downtown_kde_map &lt;- ggplot() +\n  annotation_map_tile(type = \"cartolight\", zoomin = 0, progress = \"none\") +\n  geom_sf(aes(fill = kde), data = hour_layers, alpha = 0.75, colour = NA) +\n  geom_sf(data = cpd_central, colour = \"grey33\", fill = NA) +\n  transition_states(states = hour_name) +\n  scale_fill_distiller(\n    breaks = range(pull(hour_layers, kde)),\n    labels = c(\"lower\", \"higher\"),\n    direction = 1\n  ) +\n  labs(\n    title = \"Aggravated assaults in downtown Chicago, 2010-2019\",\n    subtitle = \"Density of aggravated assaults: {closest_state}\",\n    caption = \"Data from Chicago Police\",\n    fill = \"density of\\naggravated\\nassaults\"\n  ) +\n  theme_void()\n\n\n# Save map ---------------------------------------------------------------------\nanim_save(\n  filename = \"chicago_downtown_agg_assaults.gif\", \n  animation = animate(\n    plot = chicago_downtown_kde_map,\n    height = 800, \n    width = 800, \n    units = \"px\"\n  )\n)\n\n\nYou can learn more about:\n\nusing the different functions in the lubridate package to Do more with dates and times in R and\nanimating different types of map and chart in different ways in Getting Started with gganimate.\n\n\n\nArtwork by @allison_horst",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Mapping crime over time</span>"
    ]
  },
  {
    "objectID": "appendices/read_functions.html",
    "href": "appendices/read_functions.html",
    "title": "Appendix A — Functions for reading data into R",
    "section": "",
    "text": "R can read data in many different formats. Different functions (often from different packages) are needed to read files of different formats. These are some of the functions needed to read common types of file used in data analysis, including spatial data formats.\n\n\n\n\n\n\nFunctions for reading data into R\n\n\nData file type\nPackage\nFunction\nCan load compressed files?\nCan load files from URL?\n\n\n\n\nComma-separated values (.csv)\nreadr\nread_csv() (data from English-speaking countries) or read_csv2() (data from elsewhere)\nyes\nyes\n\n\nFixed-width files (usually .txt)\nreadr\nread_fwf()\nyes\nyes\n\n\nGeoJSON (.geojson)\nsf\nread_sf()\nno\nyes\n\n\nGeoPackage (.gpkg)\nsf\nread_sf()\nno\nyes\n\n\nGoogle Sheets\ngooglesheets4\nread_sheet()\nn/a\nyes\n\n\nHTML (.htm or .html)\nxml2\nread_html() (probably used with functions from the rvest package)\nn/a\nyes\n\n\nJSON (.json)\njsonlite\nread_json()\nyes\nyes\n\n\nMicrosoft Excel (.xlsx or .xls)\nreadxl\nread_excel()\nno\nno\n\n\nOpenDocument Spreadsheet (.ods)\nreadODS\nread_ods()\nno\nyes\n\n\nR Data (.rds)\nreadr\nread_rds()\nn/a\nyes\n\n\nSAS (.sas7bdat)\nhaven\nread_sas()\nyes\nyes\n\n\nShapefile (.shp)\nsf\nread_sf()\nno\nno\n\n\nSPSS Statistics (.sav)\nhaven\nread_sav()\nyes\nyes\n\n\nStata (.dta)\nhaven\nread_dta()\nyes\nyes\n\n\nTab-separated values (.tsv or .tab)\nreadr\nread_tsv()\nyes\nyes\n\n\nXML (usually .xml)\nxml2\nread_xml()\nyes\nyes",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Functions for reading data into R</span>"
    ]
  },
  {
    "objectID": "appendices/map_checklist.html",
    "href": "appendices/map_checklist.html",
    "title": "Appendix B — Checklist: Making a good crime map",
    "section": "",
    "text": "Whenever you make a crime map, use this checklist to make sure your map is likely to be useful to the people you are making it for.\n\n\n1. Have you chosen the best type of map for this particular purpose?\nDifferent maps are used for different purposes:\n\nTo understand patterns or concentrations of crime, use a density map (Chapter 6) or hotspot map (Chapter 11).\nTo understand crime rates, use a choropleth map (Chapter 9) or a dual kernel density map (Chapter 11).\nTo show the exact locations of a small number of incidents, use a point map (Chapter 4).\nTo show events in a crime series, use a crime-series map (Chapter 13).\nTo show how patterns of crime have changed over time, use small-multiple or animated maps (Chapter 16).\n\n\n\n2. Does the map have an appropriate visual hierarchy?\nThe crime events or patterns you are trying to communicate should always be the most visually prominent element on a map, to help readers focus on what is most important. The rest of the elements on the map should follow the visual hierarchy set out in Section 7.3.\n\n\n3. Is everything on the map clearly legible?\nMake sure that all the elements of your map are clearly legible, and that there is enough contrast between text and background to make the text easy to read.\n\n\n4. Is the language used in the map text appropriate for the audience?\nAll the text on a map should use language that can be easily understood by the audience that specific map is being made for. Many map users are busy people who need to make decisions quickly, so it’s important that they can easily grasp what information a map is communicating. In particular, make sure you avoid using cartographic jargon such as ‘kernel density’ or ‘choropleth’ – most people don’t know these terms.\n\n\n5. Does the map include layers of of supporting information?\nMaps will generally communicate more effectively if the audience understands where in a city or neighbourhood crime points or hotspots are. Effective maps almost always have an appropriate base map (Section 6.4) underneath the data layer(s). Make sure the base map helps the audience focus on the points or patterns the map is about by not being so colourful or complicated that it is distracting. Remember that any filled layers on top of a base map might need to be semi-transparent so that the audience can see both the data and the base map underneath.\nMany maps also benefit from including extra layers of data such as relevant boundaries (Section 6.5) for administrative or other areas. It may also be useful to highlight of facilities that are relevant to a particular crime or other issue (Chapter 10).\n\n\n6. Does the map have a declarative title?\nMaps should have a title that summarises in a single sentence the point you think it is most important that the audience understands. A declarative title like this is almost always better than a descriptive title (Section 7.7).\n\n\n7. Does the legend explain how to interpret data-driven elements of the map?\nMost maps need a legend (or even more than one), unless each point on the map is labelled directly. Make sure the title of each legend explains how to interpret each of the data-driven elements of the map (Section 7.8). For example, if the map uses colour to show concentrations of a particular type of crime, make sure the legend title explains this.\n\n\n8. Does the map comply with any legal requirements?\nMany sources of data that you might use to make maps impose a legal requirement that any maps made using that data must include an acknowledgement of the data source (Section 7.7.1). Usually the best place to add data attribution statements is the map caption, since while this information is legally required it does not need to be high on the visual hierarchy.\n\n\n9. Does the map convey information using the least ink possible?\nMaps often contain a lot of information, so it is easy for a map to become cluttered and that make the main message harder to understand. Convey each piece of information using the minimum ‘ink’ possible, e.g. by removing unnecessary borders around boxes. Make sure every piece of information you add to a map is going to help the audience understand the main message – if it doesn’t help, don’t put it on the map.\n\n\n10. Does the map say who made it?\nUnderstanding who made a document is an important part of working out whether to trust the contents. For example if you read a news story or an academic journal article, you might look at the name of the author as part of deciding how trustworthy the contents is likely to be. The same is true for a map, so a map should generally include the name of either the individual or organisation that made the map (Section 7.7.1). This is generally not necessary, though, if the map is going to be included in a report or larger document by the same authors.\n\n\n11. Does the map state when it was made?\nData changes over time, so it is often important to make it clear when the map was made and/or what period of time the map data relates to. This might be included in the sub-title of the map, or in the caption.\n\n\n12. Does the map need a scale bar?\nNot all maps need a scale bar (Section 7.9), but you should include a scale bar if the audience is likely to need to understand distances between places on the map. This might be important if, for example, the audience might need to understand how large a crime hotspot is. If you do include a scale bar, make sure it is low on the visual hierarchy so that it does not distract from the main message of the map.\n\n\n13. Does the map need a north arrow?\nMost crime maps don’t need a north arrow (Section 7.9.2), since it’s unlikely that the audience will be using a crime map for navigation. Crime maps do need a north arrow, however, if navigation is important to the purpose of the map or if the map does not follow the convention of having north at the top of the map. If a north arrow is included, it should be low on the visual hierarchy.\n\n\n14. Have you checked your spelling?\nCheck all the text on the map for spelling and grammar mistakes.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Checklist: Making a good crime map</span>"
    ]
  },
  {
    "objectID": "appendices/common_errors.html",
    "href": "appendices/common_errors.html",
    "title": "Appendix C — Common R errors and how to fix them",
    "section": "",
    "text": "In these error messages, the code blah represents the function, object or value that the error relates to.\n\nthere is no package called 'blah'\n\nYou have either mis-typed the package name or the package is not installed. Check the spelling or use install.packages() to install the package.\n\ncould not find function \"blah\"\n\nYou have either mis-typed the function name or the package containing that function is not loaded. Check the spelling or use library() to load the package.\n\nobject 'blah' not found\n\nYou have either mis-typed the name of the object or the object does not exist. Check the spelling and make sure you have run the code that creates the object. This error can also occur when you have forgotten to put quotes around a character value in an argument, since R treats words without quotes around them as the names of objects. In this case, check that you have used quote marks around any character values in your code.\n\n'blah' does not exist in current working directory\n\nYou have either mis-typed the name of a file that R is trying to access or the file does not exist in the location you have specified. Check the spelling and make sure the file exists.\n\nnon-numeric argument to binary operator\n\nYou have tried to use a mathematical operator such as + or - with a non-numeric value. For example, you might have written the code 1 + blah thinking that blah holds a numeric value, but if blah actually holds a character value then trying to add it to 1 makes no sense. Check that any objects in your code have the values you expect them to.\n\nobject of type 'closure' is not subsettable\n\nYou have tried to use a function as if it is an object, which can happen when you store data in an object that has the same name as an R function (most commonly, when you store some data in an object called data, since there is a function called data()). The circumstances that produce this error are often not simple to understand, so the best way to handle this error is to avoid it by not naming objects using the names of functions.\n\nno applicable method for 'blah' applied to an object of class \\\"blah\\\"\n\nSome R functions (called generic functions) work in different ways depending on what type of object you use them on. But if you use a generic function on an object that it does not know how to handle, you will see this error. Check the object(s) that you provided to the function causing the error to make sure it is the type of object you are expecting it to be.\n\nunexpected numeric constant in \"blah\", unexpected string constant in \"blah\" or unexpected symbol in \"blah\"\n\nYou have a typo somewhere in your code. Check the line of code producing the error to make sure it is formatted correctly. The most common typos that cause this error are a missing comma or closing parenthesis, but there are several other typos that can cause similar errors.\n\nunused argument (var = \"blah\")\n\nYou have used an argument name in a function that does not understand it. Check the manual page for that function.\n\nargument \"blah\" is missing, with no default\n\nYou have used a function without providing all the necessary arguments. Check the manual page for that function.\n\nThe pipe operator requires a function call as RHS\n\nThere is a pipe operator at the end of the final line of a code pipeline (or what R thinks should be the final line), or you have omitted the parentheses at the end of a function name.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Common R errors and how to fix them</span>"
    ]
  },
  {
    "objectID": "appendices/functions_to_avoid.html",
    "href": "appendices/functions_to_avoid.html",
    "title": "Appendix D — Some R functions you should avoid",
    "section": "",
    "text": "R is very useful, but there are some R functions that are no longer the best way to get particular things done. Some of these have negative side-effects or can cause other problems. Fortunately, in every case there is a newer function that does the same thing better. Use this guide to help you avoid getting trapped by an obsolete R function.\n\n\n\n\n\n\n\nWhy haven’t these functions been removed from R?\n\n\n\n\n\nThe R language is 31 years old and has developed substantially over that time. Lots of people need to run R code that was written some time ago, and so may use functions that are now obsolete. To ensure backwards compatibility with old code, new versions of R typically do not remove obsolete functions, but just provide new functions as alternatives. This means old R code will still run, but also means people writing new R code need to know which obsolete R functions to avoid.\n\n\n\n\nAvoid attach()\n\nWhat it does\n\nAllows you to refer to each column in a dataset as if it were a separate object.\n\nWhy you should avoid it\n\nOnce your code includes more than one object containing data (which code almost always does), it becomes very difficult to keep track of which dataset a particular function is using. The version of each column loaded by attach() also doesn’t update when you make changes to the dataset as you wrangle your data. Find out more about why using attach() is usually a bad idea.\n\nWhat to do instead\n\nExplicitly refer to the dataset you want to use when you call each function (this is the way most code is written in any case). Modelling functions such as lm() and glm() have a data = argument you can use for this purpose. If you need to extract a column from a dataset, use pull() from the dplyr package.\n\n\n\n\nAvoid setwd()\n\nWhat it does\n\nSets the working directory for your R session, which tells R where to look for any data files you try to load, and where to save any output files to.\n\nWhy you should avoid it\n\nThe way setwd() works means that your code will break as soon as you share the file with someone else or move to a new computer. It also makes it easy to end up with files in the wrong place if you cut-and-paste code from one script to another. Find out more about why using setwd() is generally a bad idea.\n\nWhat to do instead\n\nCreate an RStudio Project for each analytical project you work on. RStudio will manage where files should go relative to the directory you save the project in. You can learn more about RStudio Projects in Section 5.2. You can also use the here package to specify where you want files to be stored.\n\n\n\n\nAvoid rm(list = ls())\n\nWhat it does\n\nDeletes all the R objects you have created during your R session (also called clearing the environment).\n\nWhy you should avoid it\n\nSometimes it’s useful to clear your environment, for example when you want to start working on a new piece of analysis. However, if you include rm(list = ls()) in your R scripts then you might accidentally delete everything when you don’t want to. Worse, if you share your code with someone else and they run it, this line of code will delete everything they were already working on. Find out more about why rm(list = ls()) is always a bad idea, and sometimes a dangerous one.\n\nWhat to do instead\n\nWhen you want to clear the R environment, do it by restarting R: click the Session menu in RStudio then click Restart R.\n\n\n\n\nAvoid subset()\n\nWhat it does\n\nExtracts some rows and/or columns from a dataset.\n\nWhy you should avoid it\n\nsubset() isn’t bad, but filter() and select() from the dplyr package perform the same tasks better. In particular, subset() doesn’t understand grouped datasets (filter() does), and subset() sometimes changes the types of columns in datasets. Find out more about subset() vs filter() and select().\n\nWhat to do instead\n\nUse the filter() and select() functions from the dplyr package (Chapter 3).\n\n\n\n\nAvoid %&gt;%\n\nWhat it does\n\nPipes the result of one function to another, e.g. runif(100) %&gt;% mean().\n\nWhy you should avoid it\n\n%&gt;% (from the magrittr package) is very useful, but it has been replaced by the |&gt; that does the same thing in almost all circumstances, is easier to type, and doesn’t require you to load the magrittr package.\n\nWhat to do instead\n\nIn most cases it’s better to use the |&gt; pipe, although the differences between them are not large. Find out more about the differences between %&gt;% and |&gt;.\n\n\n\n\nAvoid as.numeric()\n\nWhat it does\n\nas.numeric(), as.double() and as.integer() convert non-numeric values to numbers.\n\nWhy you should avoid it\n\nThese functions are commonly used either to convert numbers that have been stored as text to numeric values, or to convert factors to numeric values. The problem is that converting factors to numeric values is (to quote the R documentation for as.numeric()) “often meaningless as it may not correspond to the factor levels”. Meanwhile as.numeric() fails to convert numbers stored as text to numeric values if the text also includes non-numbers. For example, as.numeric(\"$1\") returns a NA.\n\nWhat to do instead\n\nTo extract numbers from text and store them as a numeric value, use the parse_number() function from the readr package. This is able to handle text that includes non-numeric characters, e.g. parse_number(\"$1\") returns the value 1 rather than a missing value.\n\n\n\n\nAvoid ifelse()\n\nWhat it does\n\nifelse() returns one of two specified values depending on whether a particular criterion is true or false.\n\nWhy you should avoid it\n\nThe job ifelse() does is important, but ifelse() doesn’t handle missing values very well and sometimes silently converts variables to a different type of value (which can cause hard-to-detect errors later in your code).\n\nWhat to do instead\n\nThe if_else() (note the underscore) function from the dplyr package does the same job as ifelse() but is better at handling missing values and doesn’t convert variables to different types.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>Some R functions you should avoid</span>"
    ]
  },
  {
    "objectID": "appendices/handling_errors.html",
    "href": "appendices/handling_errors.html",
    "title": "Appendix E — Checklist: handling code errors",
    "section": "",
    "text": "When you run some code in R and it produces an error, follow the steps learned in this checklist to identify and fix the problem.\n\n\nRead the error message and fix any obvious problems.\nMake sure your code follows the style guide introduced in Chapter 5, which will make it easier to see where there might be problems, especially those caused by typos. In particular, make sure that each function is on a separate line so you can comment them out individually. Remember you can use the styler package (see Section 5.8 for details) to automatically style your code.\nComment out all the lines of your code. Uncomment each line in turn (starting with the first line), re-running the code each time until you see the error you’re trying to understand. Now you know which line is likely to be causing the problem.\nIf the error message is one of the common errors listed in Appendix C, follow the steps outlined in Appendix C to fix it.\nIf that does not fix the problem, read the manual page for the function that you think is causing the problem. Check that the values you have passed to each argument are what the function needs in order to work.\nIf that does not fix the problem, create a reproducible example (also called a reprex or a minimal working example) as explained in Section 8.6. Often, the act of creating a reproducible example will help you fix the problem, but if not then you can use it to get help from others.\n\nYou will be able to fix almost all the errors in your code by following the early steps in this list. Only quite rarely will you need to post a reprex online to get help.\nYou can find more details about how to deal with bugs in your code in Chapter 8.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>E</span>  <span class='chapter-title'>Checklist: handling code errors</span>"
    ]
  }
]